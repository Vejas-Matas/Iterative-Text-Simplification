{
  "iterative": [
    {
      "SARI": 42.4673498290705,
      "BLEU": 0.3185337005461182,
      "FKGL": 20.983999999999998
    },
    [
      {
        "source": "Abstract   Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. This work describes an innovative mobile application for wildfire information management that operates on Windows Phone devices and acts as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management. Several tasks can be accomplished from the AEGIS App, such as routing, spatial search for closest facilities and firefighting support infrastructures, access to weather data and visualization of fire management data (water sources, gas refill stations, evacuation sites etc.). An innovative feature of AEGIS App is the support of these tasks by a digital assistant for artificial intelligence named Cortana (developed by Microsoft for Windows Phone devices), that allows information utilization through voice commands. The application is to be used by firefighting personnel in Greece and is potentially expected to contribute towards a more sophisticated transferring of information and knowledge between wildfire confrontation operation centers and firefighting units in the field.",
        "prediction": "Abstract   Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as accessing data and information, sharing intelligence, and coordinating personnel and vehicles. This work describes an innovative mobile application for wildfire information management that operates on Windows Phone devices and acts as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management. Several tasks can be accomplished from the AEGIS App, such as routing, spatial searches for the closest facilities and firefighting support infrastructures, access to weather data, and visualization of fire management data (water sources, gas refill stations, evacuation sites, etc.). The AEGIS App includes a helpful feature that uses an artificial intelligence digital assistant called Cortana (created by Microsoft for Windows Phone devices), which lets users access information using voice commands. This application is intended for use by firefighting personnel in Greece and is expected to enhance the transfer of information and knowledge between wildfire operation centers and firefighting units in the field.",
        "reference": "Novel mobile devices and applications can be used in wildfire confrontation by helping users to access data and information and coordinate personnel and vehicles. This paper describes a new Windows Phone application for wildfire information management which is complementary to the web version of the AEGIS platform for wildfire prevention and management. The AEGIS App can be used for searching for the closest firefighting infrastructures, as well as access to weather data, and visualization of water sources, gas refill stations, evacuation sites, etc. The AEGIS App supports these tasks through voice commands with Microsoft Cortana (AI digital assistant for Windows Phone).  The application is designed for firefighters in Greece but it can help communication between wildfire coordination and local firefighting units.",
        "metrics": {
          "SARI": 47.4107304786094,
          "BLEU": 0.27937330456875137,
          "FKGL": 20.5
        }
      },
      {
        "source": "In recent years, mobile technologies have developed and applied in education fields, and some mobile emerging carriers with mobile technologies include a personal digital assistant (PDA), smart phone, and e-book. Some of the mobile carriers combines context-aware technologies or involve into wireless network environments to make effective use through the combination of learning scenarios and technologies, and then provide students new learning experiences differed from the past learning. In view of the application of mobile technology in education, some previous studies have addressed that mobile learning is a meaningful learning that can improve the interaction between students and the situations and reach the purposes of learning. In the main stream of mobile learning, using mobile carriers with suitable learning methods or strategies in mobile learning activities for different students to enhance learning have gradually become a important and concern issue. The purpose of this study is to investigate the learning achievement and learning attitude of elementary school students on a campus plant learning activity when they use mobile carriers and competitive learning strategies. The experimental results show that the competitive learning group of students have better learning performance than non-competitive learning group of students. After completing the learning activity, the two groups of students presented high positive attitudes towards learning.",
        "prediction": "In recent years, mobile technologies have developed and been applied in education fields, and some emerging mobile devices include a personal digital assistant (PDA), smartphone, and e-book. Some mobile devices combine technologies that are aware of context or connect to wireless networks to effectively blend learning scenarios with technology, providing students with new learning experiences that are different from past ones. Previous studies have shown that mobile learning is meaningful as it can improve the interaction between students and their learning environments, helping them achieve their learning goals. In the main stream of mobile learning, using mobile devices with suitable learning methods or strategies for different students has gradually become an important concern. The purpose of this study is to investigate the learning achievement and attitude of elementary school students during a campus plant learning activity when they use mobile devices and competitive learning strategies. The experimental results show that the competitive learning group of students performed better than the non-competitive learning group. After completing the learning activity, both groups of students expressed high positive attitudes toward learning.",
        "reference": "Mobile technologies, such as personal digital assistants (PDA), smartphones, and e-books, are increasingly used in the educational field. Some mobile carriers combine learning scenarios and context-aware technologies or wireless network environments providing students with new learning experiences. Mobile learning is effective as it can improve the interaction between students and situations. An important question in the mobile learning field is how to enhance learning through the use of mobile carriers with suitable learning methods in student activities. This study investigates learning achievement and attitude of elementary school students when they use mobile carriers and competitive learning strategies during campus plant learning activities. The competitive learning group of students has better learning performance than the non-competitive one. After the activity, both groups of students were positive about learning.",
        "metrics": {
          "SARI": 48.76033217669528,
          "BLEU": 0.24212592285337578,
          "FKGL": 15.6
        }
      },
      {
        "source": "This paper proposes an electrophysiological wireless patient monitoring system which integrates a Wireless ECG signal transmitter, GPS device and a mobile phone to acquire physiological signals and transmit them to a local server via Bluetooth wireless technology. Four kinds of monitor units were specially designed for a wireless communication, including a control center , a local monitor unit, mobile devices (personal digital assistant; PDA), and a Web page (for both patient and doctor). The use of various monitor units is created to fulfill different medical personnel requirements and wishes. This application was developed to promote the mobility and flexibility for the patients and also for the medical personnel, which further will improve both the quality of health care and lifestyle of the patient.",
        "prediction": "This paper presents a wireless patient monitoring system that uses a heart signal transmitter, GPS device, and a mobile phone to collect health data and send it to a local server using Bluetooth. Four types of monitor units were specifically designed for wireless communication: a control center, a local monitor unit, mobile devices (personal digital assistants; PDA), and a web page (for both patients and doctors). These various monitor units were created to meet different requirements and preferences of medical personnel. This application aims to promote mobility and flexibility for both patients and medical personnel, which will further improve the quality of health care and the lifestyle of the patient.",
        "reference": "This paper suggests a system to monitor patients wirelessly using a device that transmits ECG signals, a GPS device, and a mobile phone. The signals are sent to a local server via Bluetooth technology. Four types of monitor units were created for wireless communication: a control center, a local monitor unit, mobile devices such as personal digital assistants (PDA), and a web page accessible to both patients and doctors. Various monitor units are used to satisfy different medical personnel needs. This application was developed to boost the quality of healthcare and patient lifestyle by increasing mobility and flexibility for both patients and medical personnel.",
        "metrics": {
          "SARI": 59.34414119146402,
          "BLEU": 0.41489016707103743,
          "FKGL": 16.4
        }
      },
      {
        "source": "As various kinds of output devices emerged, such as highresolution printers or a display of PDA(Personal Digital Assistant), the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging image with high quality. One of the largest problems on image enlargement is the exaggeration of the jaggy edges. To remedy this problem, we propose a new interpolation method, which uses artificial neural network to determine the optimal values of interpolated pixels. The experimental results are shown and evaluated. The effectiveness of our methods is discussed by comparing with the conventional methods.",
        "prediction": "As various kinds of output devices emerged, such as high-resolution printers or displays of PDAs Personal Digital Assistants, the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging images while maintaining quality. One of the largest problems in image enlargement is the exaggeration of jagged edges. To address this issue, we suggest a new approach to fill in gaps in images using a computer program that learns from examples to find the best colors for the missing parts. The experimental results are presented and evaluated, and we discuss the effectiveness of our methods by comparing them with conventional techniques.",
        "reference": "The rise of output devices like high-resolution printers and PDA displays has increased the need for high-quality resolution conversion. The paper proposes a new method to make images bigger while maintaining high quality. The main issue with enlarging images is that jagged edges can become exaggerated. To solve this problem, we suggest a new method that helps us to estimate the value of the newly generated pixels using a neural network. The experiment's results are presented and analyzed. We evaluate the effectiveness of our methods by comparing them to traditional approaches.",
        "metrics": {
          "SARI": 48.77839056491034,
          "BLEU": 0.20748827114730112,
          "FKGL": 13.9
        }
      },
      {
        "source": "A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours. For them, everyday tasks like navigating through a train or metro network map becomes demanding. We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant, non-invasive sound. This technique was implemented inside a Personal Digital Assistant (PDA) portable device. In this implementation, colour information is extracted from the input image and categorised according to how human observers segment the colour space. This information is subsequently converted into sound and sent to the user via speakers or headphones. In the original implementation, it is possible for the user to send its feedback to reconfigure the system, however several features such as these were not implemented because the current technology is limited.We are confident that the full implementation will be possible in the near future as PDA technology improves.",
        "prediction": "A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours For them everyday tasks like navigating through a train or metro network map become demanding We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant non-invasive sound This technique was implemented inside a Personal Digital Assistant PDA portable device In this implementation color information is taken from the input image and arranged based on how people see different colors This information is then converted into sound and sent to the user via speakers or headphones In the original implementation users can provide feedback to reconfigure the system however several features such as these were not implemented due to current technology limitations We are confident that full implementation will be possible in the near future as PDA technology improves",
        "reference": "A significant percentage of people have trouble differentiating or seeing colors. Simple tasks like using a train or subway map can be difficult for them. We introduce a new method to help visually impaired users perceive color information using non-invasive, pleasant sounds generated from everyday stimuli. We put this technique into a portable device called a Personal Digital Assistant (PDA). In this implementation, we extract color information from the input image and organize it based on how people perceive color. Then, the information is changed into sound and sent to the user through speakers or headphones. In the first version, the user could give feedback to change the system, but we couldn't add all the features due to technology limitations. We believe that we can add all the features soon as PDA technology advances.",
        "metrics": {
          "SARI": 43.00597678276121,
          "BLEU": 0.17245362883176382,
          "FKGL": 63.8
        }
      },
      {
        "source": "Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients but also incurs a great financial cost to the society. A medicine decision support system in a Personal Digital Assistant (PDA), with a barcode reader, can provide an overview of the patientsu0027 complete medicine use, and detect unsuitable drugs and drug combinations. Focusing on the elderly, our aim was to evaluate if a mobile medicine decision support system with a barcode reader is useful and user-friendly for nurses in home care. The participants received a comprehensive overview from the patientsu0027 medicine use and noted drug-drug interactions, therapeutic duplications and warnings for drugs unsuitable for elderly people. The nurses regarded that the decision support system increased prevention and safety, was useful and user-friendly. Our findings suggest that most of the content and functions were regarded as important. Therefore, this decision support system might be a useful tool for district nurses.",
        "prediction": "Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients, but it also incurs a great financial cost to society. A medicine decision support system on a Personal Digital Assistant (PDA) with a barcode reader can show how patients use their medications and identify unsuitable drugs and drug combinations. Focusing on the elderly, our aim was to evaluate whether a mobile medicine decision support system with a barcode reader is useful and user-friendly for nurses in home care. The participants received a comprehensive overview of the patients' medication use and noted drug-drug interactions, therapeutic duplications, and warnings for drugs unsuitable for elderly people. The nurses felt that the decision support system increased prevention and safety and considered it to be useful and user-friendly. Our findings suggest that most of the content and functions were viewed as important. Therefore, this decision support system could be a valuable tool for district nurses.",
        "reference": "Misusing medicines can make older people more likely to end up in the hospital. This causes patient suffering and financial cost to society. A support system for medicine decision, with a bar code reader, in a PDA can help identify inappropriate medication use and combinations by providing a complete overview of the patient's medication The goal was to evaluate if a mobile medicine decision support system with a barcode reader is helpful and easy to use for home care nurses working specially with older patients. The participants were provided with a complete overview of the patient's medication usage, and identified any drug interactions, duplicated medications, and any warnings about drugs that are not suitable for elderly patients. The nurses found the decision support system to be helpful, and easy to use, and improving the prevention and safety measures. Our results indicate that the majority of the content and features were considered significant. So, this system could be helpful for district nurses.",
        "metrics": {
          "SARI": 42.16075288041412,
          "BLEU": 0.21227763228478777,
          "FKGL": 12.5
        }
      },
      {
        "source": " :[0],\"access to patient appointment schedules can help clinicians manage time and problems better. Many large health care organizations manage clinical appointment scheduling via enterprise resource scheduling systems which are poorly accessible by clinicians. Also, staff other than the clinicianu0027s personal assistant(s) may manage scheduling, making it difficult for clinicians to stay informed of changes. Many a clinician today uses a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician.",
        "prediction": "Access to patient appointment schedules can help clinicians manage time and problems better. Many large healthcare organizations use scheduling systems to manage appointments, but these systems are hard for clinicians to access. Additionally, staff other than the clinician's personal assistant(s) may handle scheduling, making it difficult for clinicians to stay informed of changes. Many clinicians today use a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician",
        "reference": "Knowing patients' appointment schedules can help clinicians manage time and problems more effectively. Healthcare organizations often use enterprise resource scheduling systems for clinical appointment scheduling, but these systems may not be easily accessible by clinicians. Moreover, scheduling may not only be handled by the clinician's assistant(s), creating challenges for clinicians to stay updated on any changes. Many clinicians today use a personal digital assistant (PDA) with a basic calendar. Our project called 'PalmOversite' shows the practicability of integrating enterprise appointment schedule information into a PDA calendar. This makes it easier for clinicians to access their schedule information.",
        "metrics": {
          "SARI": 52.04415531996123,
          "BLEU": 0.3770340426066808,
          "FKGL": 15.2
        }
      },
      {
        "source": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. Currently, a SES employee visits each health establishment to collect this information by hand, process it and type it into an electronic medical record system  this :[81],\"paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify and upload monthly bacteriology data into the PIH-EMR. After an initial implementation period, we performed a pilot study to test the use of this system. We completed a baseline assessment in two health districts and then implemented the electronic system in one of the districts while the control site continued to use the paper-based system during the same period. The PDA-based system had a processing time of 6.2 days, significantly lower than measurements for both the baseline [54.8] and control sites [64.4] (both pu003c0.0001). It was also able to reduce the frequency of discrepancy from 10.1% to 2.8% (pu003c0.0001) and receive positive feedback from the users. Finally, the system’s cost would be recuperated in three months from time savings due to increased work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors and costs.",
        "prediction": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. Currently, a SES employee visits each health establishment to collect this information by hand, process it, and type it into an electronic medical record system. In this paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify, and upload monthly bacteriology data into the PIH electronic medical record. After an initial implementation period, we performed a pilot study to test the use of this system. We completed a baseline assessment in two health districts and then implemented the electronic system in one of the districts while the control site continued to use the paper-based system during the same period. The PDA-based system processed data in 6.2 days, which is much faster than the 54.8 days for the baseline and 64.4 days for the control sites. It also reduced the number of errors from 10.1% to 2.8% (p<0.0001) and received positive feedback from users. Finally, the system would pay for itself in three months because it saves time and increases work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors, and costs.",
        "reference": "PIH (Partners In Health) and SES (Socios En Salud) provide treatment for most of the MDR-TB (multidrug-resistant tuberculosis) patients in Peru along with the Peruvian NTP (National TB Program). Monthly bacteriology tests are necessary for this treatment and must be collected from various health establishments located throughout the city. An employee from SES is currently going to each health establishment and collecting monthly bacteriology test results by hand, processing them, and typing them into an electronic medical record system. In this paper, we explain how we made a digital system using a personal digital assistant (PDA) to gather, check, and upload monthly bacteriology data into the PIH-EMR (Partners In Health Electronic Medical Record).  After an initial period of putting the system into action, we conducted a test study to evaluate the effectiveness of using this system. We first evaluated the current system in two health districts, then introduced the electronic system in one district while the other continued using the paper-based system during the same time. The PDA-based system took only 6.2 days to process, which was significantly lower than both the baseline (54.8) and control sites (64.4) (both less than 0.0001). The PDA-based system reduced differences from 10.1% to 2.8% and received positive feedback from users. The system would pay for itself in three months due to increased efficiency and time savings. A larger study will be conducted to assess the impact of this system on delays, errors, and costs.",
        "metrics": {
          "SARI": 49.496245175705596,
          "BLEU": 0.3282195623647738,
          "FKGL": 10.8
        }
      },
      {
        "source": "In order to facilitate knowledge transfer between specialists and generalists and between experts and novices, and to promote interdisciplinary communication, there is a need to provide methods and tools for doing so. This interdisciplinary research team developed and evaluated a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage/tele-consultation when the presenting problem was chest pain. The combined human factors methods of cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase were used to develop the DST. A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period. During this time, the DST was used by the nine nursing coordinators who provide tele-triage/tele-consultation 24/7. This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses, its potential for use by nurses less experienced in cardiac care, and for its potential use in an interdisciplinary team environment.",
        "prediction": "In order to facilitate knowledge transfer between specialists and generalists and between experts and novices and to promote interdisciplinary communication there is a need to provide methods and tools for doing so This research team created and tested a decision support tool DST for cardiac tele-triage and tele-consultation using a personal digital assistant PDA when patients had chest pain The combined human factors methods of cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase were used to develop the DST A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period During this time the DST was used by the nine nursing coordinators who provide tele-triage and tele-consultation 24/7 This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses its potential for use by nurses less experienced in cardiac care and for its potential use in an interdisciplinary team environment",
        "reference": "To improve communication between specialists, generalists, experts, and novices and encourage interdisciplinary collaboration, we need to develop methods and tools. An interdisciplinary research team created and tested a decision support tool (DST) on a personal digital assistant (PDA) to aid cardiac tele-triage/tele-consultation for patients with chest pain. To create the DST (Decision Support Tool), we used both cognitive work analysis and ecological interface design methods during the requirements-gathering and design phases. A trial was done for 3 months at a quaternary cardiac care hospital During the trial, nine nursing coordinators used DST for 24/7 tele-triage/tele-consultation. This clinical trial confirmed that the design is useful for advanced cardiac care nurses, as well as for less experienced nurses and interdisciplinary teams.",
        "metrics": {
          "SARI": 37.61093320716367,
          "BLEU": 0.21076625637857843,
          "FKGL": 66.5
        }
      },
      {
        "source": "This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Since conventional computer-assisted language learning is often boring, it exploits a cyber pet game to increase the learner’s interest. M-CALL runs on a personal digital assistant with public wireless LAN for mobile learning. It grows a cyber pet by solving problems of Korean language learning. Korean Proficiency Test (KPT), a nationally certified Korean language test, was used as problem sets. It consists of cyber pet game, mobile learning courseware, mobile learning system, and mobile tutoring. It provides various functions for Korean language learning. Currently, the prototype M-CALL was designed and partly implemented between mobile PDA and personal computer.",
        "prediction": "This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Traditional computer-assisted language learning can be dull, so M-CALL uses a cyber pet game to make learning more engaging. It runs on a personal digital assistant with public wireless LAN for mobile learning. Learners grow a cyber pet by solving problems related to Korean language learning. Problem sets are based on the Korean Proficiency Test (KPT), a nationally certified Korean language test. M-CALL includes a cyber pet game, mobile learning courseware, a mobile learning system, and mobile tutoring. It provides various functions to support Korean language learning. Currently, the prototype M-CALL has been partly designed and implemented for use on both mobile PDAs and personal computers.",
        "reference": "M-CALL is a mobile courseware for Korean language learners. It incorporates a cyber pet game to make it more engaging for learners than traditional computer-assisted language learning methods. M-CALL is a mobile learning system that uses a personal digital assistant with public wireless LAN (Computer network in small geographic areas). A cyber pet is raised while solving problems related to Korean language learning. The KPT, a nationally recognized exam for Korean language proficiency, served as the source material for the problem sets. It includes a virtual pet game, a mobile course for learning, a mobile learning platform, and mobile tutoring. It offers multiple features for learning Korean. Right now, a preliminary version of M-CALL has been developed and partially implemented for use on both mobile PDAs and personal computers.",
        "metrics": {
          "SARI": 57.67403284052276,
          "BLEU": 0.35853873648037315,
          "FKGL": 11.5
        }
      },
      {
        "source": "This study compared text entry performance of two stylus-driven soft keyboards for use in hand-held computing devices: the QWERTY and the T9. Participants transcribed text presented on a computer screen into a personal digital assistant (PDA) using a stylus and one of these two keyboards. We introduced a new psychophysical technique for measuring transcription rate that provides a composite measure of speed and accuracy. Using this technique, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparent superior physical characteristics. An ancillary experiment demonstrated that the poorer performance of the T9 layout may have resulted from an increase in visual scanning time due to perceptual grouping of the letters on the keys. Together these findings imply that the QWERTY keyboard layout remains the most effective of the currently available designs for stylus tapping on soft keyboards.",
        "prediction": "This study compared text entry performance of two stylus-driven soft keyboards for hand-held computing devices: the QWERTY and the T9. Participants transcribed text displayed on a computer screen into a personal digital assistant (PDA) using a stylus and one of these keyboards. We introduced a new method to measure how fast and accurately people can type. Using this method, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparently better physical design. An additional experiment revealed that the T9 layout's poorer performance may have come from longer visual scanning times due to the way the letters are organized on the keys. Together, these findings suggest that the QWERTY keyboard layout is the most effective of the available designs for stylus tapping on soft keyboards",
        "reference": "This study compared two onscreen keyboards, the QWERTY and the T9, for text entry performance on hand-held computing devices. Participants used a stylus to input text on a PDA using either a QWERTY or T9 keyboard in this study. We created a new way to measure transcription rate that considers both speed and accuracy together. We used this method to determine the fastest possible typing speed for each keyboard. The study found that the QWERTY keyboard had a higher transcription rate than the T9 keyboard, even though the T9 keyboard was physically better. Another test showed that the T9 keyboard's lower performance might be because the grouped letters on its keys require more visual scanning time. Overall, these results suggest that, among the current options, the QWERTY keyboard layout is still the most efficient for typing on touch screens with a stylus.",
        "metrics": {
          "SARI": 43.6217210241002,
          "BLEU": 0.16162221226523085,
          "FKGL": 12.5
        }
      },
      {
        "source": "The proliferation of mobile devices such as smart phones and Personal Digital Assistant (PDA) opens new ways for developing mobile E-commerce systems (so called Mobile-commerce or M-commerce system). In Mcommerce systems, we still see many common components that we have in standard E-commerce applications, such as web servers and database servers. However, these new applications raise some unique challenges. For example, the limitations of mobile devices (e.g., small screen size and reduced CPU performance) implies that software development is partly different from desktop applications. Questions which are posed when designing M-commerce system include: “What kind of information do the users want to access via mobile devices?”, “How can we provide useful applications with limited user input facility?”, “How can we test a distributed M-commerce system?” In this paper, we discuss about some software development issues for mobile commerce systems from our experiences while developing an M-commerce system for tourists. Our prototype, easyHotel, is an useful software that allows booking hotel rooms via mobile phones.",
        "prediction": "The proliferation of mobile devices such as smart phones and Personal Digital Assistant PDA opens new ways for developing mobile E-commerce systems also known as Mobile-commerce or M-commerce systems In M-commerce systems many common components found in standard E-commerce applications like web servers and database servers can still be seen However these new applications have unique challenges For example mobile devices have limitations such as small screens and weaker processors which means that software development is somewhat different than that for desktop applications \n\nWhen designing M-commerce systems several important questions arise What kind of information do users want to access via mobile devices How can we provide useful applications with limited user input facilities and How can we test a distributed M-commerce system \n\nIn this paper we discuss some software development issues for mobile commerce systems based on our experiences while developing an M-commerce system for tourists Our prototype easyHotel is a useful software that allows users to book hotel rooms via mobile phones",
        "reference": "The increasing use of mobile devices like smartphones and PDAs provides opportunities for the development of new mobile e-commerce systems, also known as M-commerce (Mobile-commerce) systems. M-commerce systems share several components with traditional E-commerce applications, such as web servers and database servers. However, these new applications present some distinct challenges. For example, due to mobile devices' limitations, such as smaller screens and weaker processors (also called CPUs), software development for mobile applications is partially different from desktop applications. Designing an M-commerce system involves asking questions such as: \"What information do users want on mobile devices?\", \"How can we create useful apps with limited user input?\", and \"How do we test a distributed M-commerce system?\" In this paper, we share our experiences developing an M-commerce system for tourists and discuss software development issues for such systems. Our software prototype, easyHotel, is a helpful application enabling users to book hotel rooms through their mobile phones.",
        "metrics": {
          "SARI": 45.94836892964507,
          "BLEU": 0.2102991751582626,
          "FKGL": 68.4
        }
      },
      {
        "source": "It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As emerging mobile platforms such as different kinds of Personal Digital Assistant (PDA) tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common approaches for content adaptation are automated conversion and explicit specification of adapted content, with a trade-off between quality and development/maintenance effort. As alternative avoiding this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework in the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development/maintenance effort.",
        "prediction": "It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As emerging mobile platforms, such as different kinds of Personal Digital Assistants (PDAs), tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common methods for adapting content include automated conversion and clearly defining the adapted content, but there is a trade-off between quality and the effort needed for development and maintenance. As an alternative that avoids this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework on the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation, WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development and maintenance effort.",
        "reference": "It is widely recognized that information, such as web content, should be adjusted for mobile platforms to accommodate the limitations of mobile environments. As new mobile platforms, such as various types of PDAs, differ significantly in their capabilities, we propose that adaptation should be tailored to each platform. Common methods for content adaptation involve automated conversion or explicitly specifying adapted content, balancing the trade-off between quality and the effort required for development and maintenance. Instead of making this trade-off, we suggest using a straightforward object-oriented framework for content adaptation. To make this framework more accessible for use on the web, we build our approach on the object-oriented WebComposition model and its implementation, WCML, which is an application of XML. We use our object-oriented framework in an example application to show how specifying platform-adapted content using an object-oriented approach can lower development and maintenance costs.",
        "metrics": {
          "SARI": 53.613515355639066,
          "BLEU": 0.3527290420141813,
          "FKGL": 17.2
        }
      },
      {
        "source": "In monitoring a patientu0027s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in timely-context aware manner. In this paper a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device — PSD) used in the Healthcare systems are bulky in design and are not flexible and comfortable for the elderly patients. Presented is a unique flexible, as well as detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage and processing of patient related data, which ensures periodic logging of patient data without saturating communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smart-phone and the ECG and temperature acquisition apparatus is implemented Bluetooth. The presented Data Management System — Periodic Model (DMS — PM) is designed to manage wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a Smart phone making use of the existing GSM network.",
        "prediction": "In monitoring a patient’s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in a timely, context-aware manner. In this paper, a system is designed for patients with cardiac disorders, focusing on the design of the sensing device and the chosen communication scheme. Many wearable devices for health monitoring (Patient Sensing Device — PSD) are heavy and not comfortable for elderly patients. Therefore, we present a unique, flexible, and detachable PSD to enhance patient comfort. Additionally, a model for the transmission, storage, and processing of patient-related data is discussed, which ensures periodic logging of patient information without overwhelming communication networks. A proof of concept prototype has been developed to enable the transmission of Electrocardiogram (ECG) signals and body temperature, with potential expansion to other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition apparatus is implemented through Bluetooth. The presented Data Management System — Periodic Model (DMS — PM) is designed to manage the wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or smartphone, utilizing the existing GSM network.",
        "reference": "Medical practitioners can access a wealth of data sources by using Body Area Networks (BAN) to monitor a patient's real-time vital signs. A BAN refers to a network of wearable or implantable devices worn on or attached to the human body for various purposes. Timely and context-aware delivery of data is crucial. This paper presents a system specifically designed for patients with cardiac disorders, focusing on the design of the sensing device and communication scheme selected. Many of the current wearable physiological devices used in healthcare systems, known as Patient Sensing Devices (PSD), are bulky and uncomfortable for elderly patients due to their inflexible design. We introduce a unique Patient Sensing Device (PSD) that is both flexible and detachable, designed for patient comfort. We also propose a model for transmitting, storing, and processing patient-related data that ensures regular logging of data without overwhelming communication networks. We have developed and implemented a proof-of-concept prototype that can transmit a patient's Electrocardiogram (ECG) signal and body temperature, potentially expanding to other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition apparatus is achieved via Bluetooth. The Data Management System - Periodic Model (DMS - PM) introduced here is created to facilitate the wireless connection between sensor units and the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a smartphone, utilizing the existing GSM network.",
        "metrics": {
          "SARI": 54.029138913057814,
          "BLEU": 0.360280743030063,
          "FKGL": 15.4
        }
      },
      {
        "source": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. Current trend integrates usage of mobile terminals such as smart PDA (personal digital assistant) in a learning system. Such devices can communicate with wireless mobile terminal and realize the intelligence exchange with its surroundings [1]. A LOCH (Language Learning Outside the Classroom) system named One Day Trip is developed to assist overseas students to learn Japanese in real life situations with the help of a PDA. This paper proposes the implementation with Knowledge Management (KM) concepts in the system to aid the teacher and learners in the learning environment.",
        "prediction": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space Today mobile devices like smart PDAs personal digital assistants are used in learning systems These devices can communicate with wireless mobile terminals and exchange information with their surroundings A LOCH Language Learning Outside the Classroom system named One Day Trip has been developed to help overseas students learn Japanese in real-life situations with the assistance of a PDA This paper proposes implementing Knowledge Management KM concepts in the system to support both teachers and learners in the learning environment",
        "reference": "Wireless and mobile technology progress has allowed more platforms to facilitate learning within a unified space. The current trend is to use mobile devices, such as smart PDAs, in learning systems. These devices can communicate wirelessly with mobile terminals and exchange information with the surrounding environment One Day Trip is a language learning system called LOCH (Language Learning Outside the Classroom), designed to help foreign students learn Japanese in real-life situations using a PDA. This paper suggests incorporating Knowledge Management (KM) concepts in the One Day Trip system to support both the teacher and learners in the learning environment.",
        "metrics": {
          "SARI": 48.09616111205464,
          "BLEU": 0.2757112202000753,
          "FKGL": 43.5
        }
      },
      {
        "source": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko personal digital assistant, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Instead of porting each application to individual platforms, which would prove to be extremely time consuming because of other APIu0027s and networking protocols, we created the Vassar College Messaging Layer. This layer abstracts the Cybiko networking structure and interface, allowing Cybiko applications to be ported to other platforms with minimal code alteration.",
        "prediction": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko personal digital assistant, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Rather than moving each application to different platforms, which would take a lot of time due to various application interfaces and networking rules, we created the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, allowing Cybiko applications to be moved to other platforms with minimal code changes.",
        "reference": "In recent years, we have created applications for teaching both wired and wireless networking. These applications are designed for the Cybiko personal digital assistant, which offers a cost-effective alternative to other hardware options. The Cybiko PDA was discontinued recently. Consequently, these applications needed to be adapted to other platforms to maintain their usefulness. To avoid the time-consuming task of porting each application to various platforms due to different APIs and networking protocols, we developed the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, making it easier to adapt Cybiko applications to different platforms without needing to make significant changes to the code.",
        "metrics": {
          "SARI": 52.75190689824204,
          "BLEU": 0.42845692319765744,
          "FKGL": 13.7
        }
      },
      {
        "source": "Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful (and thus more useful), machines. On the other side of this border we find the Personal Digital Assistant  :[66],\"our view, to be useful as an extension of the usersu0027 private sphere, a machine must at least have enough functionality and resources to create trustworthy digital signatures (to speak for the user, as it were). A less resourceful machine can merely act as a memory prothesis, helping the owner remembering addresses and phone  are :[121],\"designed to be tamper resistant, and as such they seem ideal as a minimal machine. However, trustworthy digital signatures can not be created by smartcards alone, simply because the user does not know what is given to the card for  :[163],\"order to be trusted--that is, being able to make trustworthy digital signatures--a smartcard must be supported by some infrastructure outside the card proper. We explore what must be included in such an infrastructure, and demonstrate that trustworthy digital signatures can in fact be made using a standard  :[211],\"argue that based on this fact, nontrivial distributed systems can be constructed by utilizing smartcards; a nontrivial system is one where holders of smartcards are \"first class citizens\". Asserting that a smartcard can act as a trusted machine gives new opportunities for designers of user-centric  that smartcards are :[257],\"here to stay, finding ways to apply them in constructive manners is prudent.",
        "prediction": "Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful and thus more useful machines. On the other side, we find the Personal Digital Assistant. In our view, to be helpful as an extension of the user's private life, a device must have enough features and capabilities to create secure digital signatures that represent the user. A less resourceful machine can merely act as a memory prosthesis, helping the owner remember addresses and phone numbers. \n\nSmartcards are designed to be tamper resistant, making them seem ideal as minimal machines. However, trustworthy digital signatures cannot be created by smartcards alone, simply because the user does not know what information is given to the card for it to be trusted. To make trustworthy digital signatures, a smartcard must be supported by some infrastructure outside the card itself. We explore what must be included in such an infrastructure and demonstrate that trustworthy digital signatures can be made using a standard model. \n\nWe argue that based on this fact, nontrivial distributed systems can be constructed by utilizing smartcards; a nontrivial system is one where holders of smartcards are first class citizens. Asserting that a smartcard can act as a trusted machine opens new opportunities for designers of user-centric applications. Given that smartcards are here to stay, finding ways to apply them constructively is prudent.",
        "reference": "Smartcards lack the capability to communicate with the user in a secure manner, unlike other hardware classes. (Smart cards are small, portable devices that have a special chip inside. They can store and process information securely.) Without a way to keep the owner updated, it becomes challenging to take advantage of the useful features of smart cards. We look at how smart cards and powerful machines can be combined to make even better devices. We believe that Personal Digital Assistants (PDAs) are more useful than smartcards because they can extend the user's private sphere. PDAs have enough functionality and resources to create trustworthy digital signatures, which can speak for the user. A less resourceful machine can serve as a memory aid, assisting the owner in remembering addresses and phone numbers, such as [121]. These machines are designed to be tamper-resistant, making them suitable for a minimalistic purpose. To create trustworthy digital signatures, a smartcard needs support from infrastructure outside the card itself. This is because the user is not aware of what information is given to the card, making it impossible for the card to be trusted solely on its own. We investigate the necessary components of this infrastructure and show that reliable digital signatures can indeed be created using a standard method [211]. We propose that smartcards can be utilized to build nontrivial distributed systems, where smartcard holders are considered important participants or \"first class citizens\". Claiming that smartcards can function as trusted machines opens up fresh possibilities for designers focused on user-centric approaches. It is evident that smartcards are here to stay, so it is wise for designers to explore and implement them in meaningful ways.",
        "metrics": {
          "SARI": 40.42986666759057,
          "BLEU": 0.1898752828831955,
          "FKGL": 12.2
        }
      },
      {
        "source": "In this paper we present the main features of a system that we have developed and that provides a new kind of tele-assistance service to elderly people.apart from the functionalities offered by the current tele-assistance services, also provides a high quality, anywhere and at any time assistance. We rate the assistance as high quality because allows the monitoring of vital signs and a local detection of anomalous situations by the device that the person carries in real time. Therefore, it sends alarms autonomously when  this paper we :[84],\"explain the three main functionalities offered by AINGERU: universal assistance anywhere and at any time, monitoring of vital signs irrespective of location or time and remote monitoring by allowing authorized external persons to consult data about monitored persons using the Internet. Moreover we show how those functionalities are achieved by making use of PDAs (Personal Digital Assistant), wireless communication, Semantic Web, Web services and agent technologies.",
        "prediction": "In this paper, we present the main features of a system we have developed that provides a new kind of tele-assistance service for elderly people. This system offers high-quality assistance anywhere and at any time, beyond what current tele-assistance services provide. We consider the assistance high quality because it can monitor vital signs and detect unusual situations in real time using the device the person carries. This allows the system to send alarms autonomously when necessary. \n\nWe explain the three main functionalities offered by AINGERU: universal assistance available anytime and anywhere, monitoring of vital signs regardless of location or time, and remote monitoring that enables authorized individuals to access data about the monitored persons via the Internet. Moreover, we demonstrate how these functionalities are achieved using PDAs (Personal Digital Assistants), wireless communication, Semantic Web, Web services, and agent technologies.",
        "reference": "This paper introduces a new tele-assistance service for elderly people that goes beyond existing services by offering high-quality assistance anywhere and anytime. Our system provides high-quality assistance because it allows real-time monitoring of vital signs and local detection of anomalous situations through the device carried by the person. We explain the three main functions of AINGERU in this paper: universal assistance anywhere and at any time, monitoring vital signs regardless of location or time, and remote monitoring that allows authorized external individuals to access data about monitored persons via the internet. We use PDAs, wireless communication, Semantic Web (aims to enhance the World Wide Web by adding meaning to the information available online), Web services, and agent technologies to achieve the aforementioned functionalities.",
        "metrics": {
          "SARI": 54.043171862604375,
          "BLEU": 0.33544409894286614,
          "FKGL": 14.7
        }
      },
      {
        "source": "Objective: To evaluate the effectiveness of a personal digital assistant (PDA)–based clinical decision support system (CDSS) on nonsteroidal anti-inflammatory drug (NSAID) prescribing safety in the outpatient  :[26],\"The design was a randomized, controlled trial conducted in a university-based resident clinic. Internal medicine residents received a PDA-based CDSS suite. For intervention residents, the CDSS included a prediction rule for NSAID-related gastrointestinal risk assessment and treatment recommendations. Unannounced standardized patients (SPs) trained to portray musculoskeletal symptoms presented to study physicians. Safety outcomes were assessed from the prescriptions given to the SPs. Each prescription was reviewed by a committee of clinicians blinded to participant, intervention group assignment, and baseline or follow-up  :[108],\"Prescriptions were judged as safe or unsafe. :[26],\"The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control  :[135],\"At baseline, the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar (0.27 vs. 0.29, p u003e 0.05). Controlling for baseline performance, intervention participants prescribed more safely than controls after receiving the CDSS (0.23 vs. 0.45 [F = 4.24, p u003c 0.05]). With the CDSS, intervention participants documented more complete assessment of patient gastrointestinal risk from  :[198],\"Participants provided with a PDA-based CDSS for NSAID prescribing made fewer unsafe treatment decisions than participants without the CDSS.",
        "prediction": "Objective To evaluate the effectiveness of a personal digital assistant PDA–based clinical decision support system CDSS on nonsteroidal anti-inflammatory drug NSAID prescribing safety in the outpatient setting The study was a randomized controlled trial conducted in a university-based resident clinic Internal medicine residents received a PDA-based CDSS suite which included a prediction rule for assessing gastrointestinal risks associated with NSAIDs and provided treatment recommendations Unannounced standardized patients SPs trained to portray musculoskeletal symptoms presented to study physicians Safety outcomes were evaluated based on the prescriptions given to the SPs and each prescription was reviewed by a committee of clinicians who were blinded to participant information intervention group assignment and baseline or follow-up data Prescriptions were classified as safe or unsafe\n\nThe main result we looked at was how unsafe NSAID prescriptions changed for the group that used the CDSS compared to the group that did not At baseline the average rate of unsafe prescriptions per physician was similar for both groups 0.27 vs 0.29 p > 0.05 After considering their starting performance the participants who used the CDSS prescribed NSAIDs more safely than those who did not 0.23 unsafe prescriptions compared to 0.45 unsafe prescriptions based on statistical analysis Additionally participants using the CDSS documented a more complete assessment of patient gastrointestinal risk Overall those using the PDA-based CDSS for NSAID prescribing made fewer unsafe treatment decisions than those who did not have access to the CDSS",
        "reference": "We did a study to test if a clinical decision support system (CDSS) on a personal digital assistant (PDA) could improve the safety of prescribing nonsteroidal anti-inflammatory drugs (NSAIDs) in outpatient clinics. We conducted a randomized, controlled trial in a university-based resident clinic. Residents in internal medicine were given a CDSS suite based on PDA. Intervention residents were provided with a CDSS that had a prediction rule for assessing gastrointestinal risk related to NSAIDs and recommendations for treatment. Trained individuals, known as standardized patients (SPs), who were equipped with musculoskeletal symptoms, interacted with the physicians participating in the study without prior notification. The study evaluated the safety outcomes of prescriptions given to standardized patients portraying musculoskeletal symptoms. A committee of clinicians who didn't know about the participant, intervention group assignment, or the baseline or follow-up data reviewed each prescription. They determined whether a prescription was safe or unsafe. The study aimed to see if the PDA-based CDSS would result in a reduction of unsafe prescribing of NSAIDs. The main outcome measured the difference in unsafe prescribing between the intervention and control groups. At the beginning of the study, both groups had a similar proportion of unsafe prescriptions. (0.27 vs 0.29) After controlling for the baseline performance, it was found that those who received the CDSS suite prescribed nonsteroidal anti-inflammatory drugs more safely than those who didn't (0.23 vs 0.45, with statistical significance). Participants who had access to the CDSS through a PDA documented a more comprehensive assessment of gastrointestinal risk in patients. Moreover, the presence of the CDSS resulted in fewer unsafe treatment decisions compared to participants who did not have access to the CDSS.",
        "metrics": {
          "SARI": 46.27384023126954,
          "BLEU": 0.15921616304438468,
          "FKGL": 21
        }
      },
      {
        "source": "Recently pedestrian navigation has been studied by many researchers, which provides pedestrians with a similar function of car navigation. On the other hand, a 3dimensional (3D) map, which is one of major themes in machine vision research, has been utilized as a simulation tool in city and landscape planning, and other engineering fields. The 3D maps will give more intuitive information compared to conventional 2-dimensional (2D) ones. In this paper we first propose pedestrian navigation based on 3D maps, and describe technologies required and its use situations. Secondly we compare effectiveness of 2D and 3D maps for navigation by object search experiments under conditions: 3D maps with and without texture; display sizes corresponding to a mobile phone and PDA (Personal Digital Assistant). The experimental results show that 3D maps with texture on mobile phone display size, and 3D maps without texture on PDA display size are superior to 2D maps in search time and error rate. From these results, we can say the effectiveness of 3D maps in pedestrian navigation.",
        "prediction": "Recently pedestrian navigation has been studied by many researchers providing pedestrians with a function similar to car navigation On the other hand a 3-dimensional 3D map which is a major theme in machine vision research has been used as a simulation tool in city and landscape planning as well as in other engineering fields The 3D maps give more intuitive information compared to conventional 2-dimensional 2D ones\n\nIn this paper we first propose pedestrian navigation based on 3D maps and describe the technologies required and their use situations We also compare how well 2D and 3D maps work for navigation using object search experiments We test 3D maps with and without texture using display sizes similar to a mobile phone and a PDA Personal Digital Assistant\n\nThe experimental results show that 3D maps with texture on mobile phone display sizes and 3D maps without texture on PDA display sizes are superior to 2D maps in both search time and error rate Therefore we can conclude that 3D maps are effective for pedestrian navigation",
        "reference": "Recently many researchers have studied pedestrian navigation, which is similar to car navigation but designed for pedestrians. A 3D map is widely used in machine vision research and various engineering fields, such as city and landscape planning, as a simulation tool. 3D maps are more intuitive than 2D maps. In this paper, we suggest using 3D maps for pedestrian navigation and explain the necessary technology and possible scenarios for its application. We will conduct object search experiments to compare the effectiveness of 2D and 3D maps for navigation. The experiments will consider the following conditions: 3D maps with and without texture, and display sizes equivalent to a mobile phone and a Personal Digital Assistant (PDA). The results of the experiment suggest that 3D maps with texture on a mobile phone display and 3D maps without texture on a PDA display are better than 2D maps in terms of search time and error rate. Based on these results, we can conclude that 3D maps are effective for pedestrian navigation.",
        "metrics": {
          "SARI": 49.50279223579223,
          "BLEU": 0.30777525499469666,
          "FKGL": 70.8
        }
      },
      {
        "source": "Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments, that are characterized by low bandwidth communication facilities and ad hoc connection/disconnection to stationary systems. They can be used for information retrieval and information filtering, in which case they evaluate replies and return only the relevant data. Mobile agents as a metaphor of active objects are created on a mobile device such as a Personal Digital Assistant (PDA), will be launched into the information galaxy and are fulfilling the mobile user ‘s task on the services available on networked stationary systems. One transmission channel for these itinerant agents is email. This paper introduces Active M 3 as an example of an active mail framework, which can be regarded as a first approach in authoring mobile agents in a graphical interactive manner. Active M 3 integrates two known concepts: active mail and multimedia mail.",
        "prediction": "Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments which are characterized by low bandwidth communication facilities and ad hoc connection/disconnection to stationary systems They can be used for information retrieval and filtering evaluating replies and returning only relevant data Mobile agents which are like active objects are created on mobile devices such as Personal Digital Assistants PDAs to help users complete tasks using services available on connected stationary systems One way these itinerant agents can communicate is through email This paper introduces Active M 3 as an example of an active mail framework which represents a first step in creating mobile agents interactively with a graphical interface Active M 3 combines two well-known concepts active mail and multimedia mail",
        "reference": "Mobile agents are a competitive concept in client-server computing, particularly well-suited for mobile environments. These environments are characterized by limited bandwidth communication and the frequent connection and disconnection to stationary systems. They can be utilized for information retrieval and filtering, where they assess responses and provide only the pertinent data. Mobile agents act like active objects that can be created on a mobile device like a PDA. They are sent out to perform tasks on the networked stationary systems and fulfill the user's needs. Email serves as a transmission channel for these mobile agents. This paper presents Active M 3, which is an active mail framework that allows users to create mobile agents using a graphical interface. Active M3 is a framework that combines two concepts: active mail and multimedia mail.",
        "metrics": {
          "SARI": 43.726603140584295,
          "BLEU": 0.1889475228262976,
          "FKGL": 55.2
        }
      },
      {
        "source": "Digital Video Broadcasting — Handheld (DVB-H) is a technology developed as an extension of Digital Video Broadcasting — Terrestrial (DVB-T) with additional features that make it suitable for delivery to mobile devices such as phones and Personal Digital Assistant (PDAs). This paper deals with the conditions of receiving digital video broadcast type services in mobile handheld devices. The DVB-H standard is introduced and the key technology elements on data link layer and physical layer are discussed in some details. Finally the DVB-H terminal and network design is described.",
        "prediction": "Digital Video Broadcasting — Handheld (DVB-H) is a technology that improves upon Digital Video Broadcasting — Terrestrial (DVB-T) and has extra features that make it suitable for mobile devices like phones and Personal Digital Assistants (PDAs) This paper deals with the conditions for receiving digital video broadcast services on mobile handheld devices The DVB-H standard is introduced and key technology elements on the data link layer and physical layer are discussed in detail Finally the design of the DVB-H terminal and network is described",
        "reference": "DVB-H is a technology based on DVB-T that is designed for delivering content to mobile devices like phones and PDAs. This paper discusses how digital video broadcast services can be received on mobile handheld devices. The introduction of the DVB-H standard includes a discussion of key technology elements on the data link layer and physical layer, providing some detailed information. This paper also covers the design of DVB-H terminals and networks.",
        "metrics": {
          "SARI": 48.66668664755816,
          "BLEU": 0.26015236508877265,
          "FKGL": 37.6
        }
      },
      {
        "source": "Summary form only given, as follows. We present a design for a low cost but powerful and high speed communications device powered by IDTu0027s RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus compatible IDT ATM NIC. The device could be used as a two way pager, a set-top box, an Internet terminal, a personal digital assistant or as a video phone. The RV4640 is a very affordable 64 bit RISC processor that executes 175 dhrystone MIPS at 133 MHz. The RV4640 can be connected to a System Interface Chip which will provide I/O and memory control. The IDT77903 ATM card is a full duplexed 25 Mbps NIC with a PCI bus interface and it costs less than $100 (US). Integrating this card into our device as a network interface will make highspeed videoconferencing and multimedia applications easily implementable.",
        "prediction": "Summary form only given, as follows. We present a design for a low-cost but powerful and high-speed communications device powered by IDT's RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus-compatible IDT ATM NIC. It can be used as a two-way pager, a set-top box, an Internet terminal, a personal digital assistant, or a video phone. The RV4640 is a low-cost 64-bit computer processor that runs at 133 MHz and performs 175 million tasks per second. It can be connected to a System Interface Chip, which provides I/O and memory control. The IDT77903 ATM card is a full duplexed 25 Mbps NIC with a PCI bus interface and costs less than $100 (US). Integrating this card into our device as a network interface will make high-speed videoconferencing and multimedia applications easily implementable.",
        "reference": "Summary provided in the following. We propose a low-cost and high-speed communication device that uses IDT's RV4640 as the processing engine. This device is designed with IDT's RV4640 as the processing engine and can achieve high speed due to the PCI bus (a type of computer bus that simplifies the communication between computer components by providing a standardized interface) compatible IDT ATM NIC. This device can serve multiple purposes such as a two-way pager, set-top box, Internet terminal, personal digital assistant, or video phone. The RV4640 is a low-cost 64-bit RISC processor that runs at 133 MHz and can execute 175 dhrystone MIPS. The RV4640 can connect to a System Interface Chip for I/O (a specialized integrated circuit designed to facilitate communication between a computer system and its external devices) and memory control. The IDT77903 ATM card, which costs less than $100 (US), is a NIC with a PCI bus interface that is capable of full duplexing at 25 Mbps. Using the IDT77903 ATM card as a network interface in our device will make it easy to implement high-speed videoconferencing and multimedia applications.",
        "metrics": {
          "SARI": 51.08105062583085,
          "BLEU": 0.3097446712764078,
          "FKGL": 10.1
        }
      },
      {
        "source": "Dr. Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework, which serves as the foundation for the NET natural user interface. The paper discusses MiPadu0027s design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates compelling user advantages for wireless personal digital assistant (PDA) devices, MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in todayu0027s PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built in roller to navigate, the tapping action narrows the number of possible instructions for spoken word understanding. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition is performed. The Dr Who CSR engine uses a unified CFG and n-gram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager.",
        "prediction": "Dr Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework which serves as the foundation for the NET natural user interface. The paper discusses MiPad's design ongoing implementation work and a preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates clear user advantages for wireless personal digital assistant PDA devices. MiPad combines continuous speech recognition CSR and spoken language understanding SLU to help users complete common tasks with a user-friendly interface and wireless technology. It addresses the challenge of using tiny styluses or typing on small keyboards in today's PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It features a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built-in roller to navigate the tapping action narrows the number of possible spoken instructions. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition occurs. The Dr Who CSR engine employs a unified context-free grammar CFG and an n-gram language model while the Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager",
        "reference": "Dr. Who is a Microsoft research project that aims to create a speech-focused system for interacting with technology. This system will serve as the foundation for a natural user interface called NET. The paper discusses MiPad's design and implementation, which are still in progress, as well as a preliminary user study comparing it to existing pen-based PDA interfaces. MiPad is a prototype application that shows how wireless personal digital assistants (PDAs) can benefit from integrating continuous speech recognition (CSR) and spoken language understanding (SLU) technologies. This allows users to perform common tasks using a multimodal interface and wireless technology. It aims to solve the issue of having to use small styluses or type on tiny keyboards on modern PDAs. MiPad does not rely solely on speech interaction, unlike a cellular phone. MiPad has a built-in microphone that turns on when a field is selected. When a user taps the screen or utilizes a built-in roller for navigation, this action reduces the range of potential instructions for spoken word comprehension. MiPad is currently operating on a Windows CE Pocket PC, with speech recognition being carried out on a Windows 2000 machine. The Windows CE Pocket PC is a PDA that runs the Windows Embedded Compact operating system. The Dr Who CSR engine combines a CFG and an n-gram language model. The Dr Who SLU engine uses a chart parser and a plan-based dialog manager.",
        "metrics": {
          "SARI": 44.35975243583518,
          "BLEU": 0.2678589279946568,
          "FKGL": 12.2
        }
      },
      {
        "source": "Little is known about how viable new mobile applications emerge in complex commercial environments. Research in the sociology of technology suggests that the evolution of new applications is a process of social interaction between multiple constituencies, aiming to create a common definition of mobile technology problems and solutions that is obvious only in retrospect. This theory is illustrated by the early evolution of the personal digital assistant (PDA). According to this view, established technological communities attempt to impose their own definitions of key problems and solutions on new mobile technology, thereby forcing newer players to provide a \"total system\" that completely redefines a new application area.",
        "prediction": "Little is known about how viable new mobile applications emerge in complex commercial environments. Research in the sociology of technology suggests that the evolution of new applications arises from different groups working together to create a shared understanding of the problems and solutions in mobile technology, which usually only becomes clear over time. This theory can be seen in the early evolution of the personal digital assistant (PDA). Established tech groups impose their own definitions of important problems and solutions on new mobile technology, putting pressure on new companies to develop a complete system that transforms the entire field of new applications",
        "reference": "We don't know much about how new mobile apps succeed in complicated business environments. Research in the sociology of technology indicates that the development of new applications involves social interaction among different groups. These groups work together to establish a shared understanding of mobile technology challenges and solutions, which often becomes clear only in hindsight. The PDA's early evolution exemplifies this theory. This idea suggests that existing technology groups try to define the problems and solutions for new mobile tech, which puts pressure on new players to create a total system that redefines the application area.",
        "metrics": {
          "SARI": 48.32612725210638,
          "BLEU": 0.19324211465064953,
          "FKGL": 15.6
        }
      },
      {
        "source": "In the ubiquitous environment, mobile terminals like personal digital assistant or cellular phone are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. Extracting images and text from Web content, u-PaV generates a Macromedia Flash file, which is a popular multimedia format. Although the presentation of Web content depends on the terminal type, the Web interface of mobile terminals is designed almost the same as that of personal computers. However, comparing mobile and desktop terminals, some difference in device characteristics, such as screen resolution or sound faithfulness, affect the presentation on the terminal. Moreover, when changing the access style between interactive and passive, the presentation should be adjusted for the terminal type because the lacking information is usually not perceived after transformation. To adjust the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content by the logotype of the keyword. Third, we segment the picture of the content into pieces and present one after another using visual effects. These procedures can be applied to any type of terminal.",
        "prediction": "In the ubiquitous environment, mobile terminals like personal digital assistants or cellular phones are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. By extracting images and text from Web content, u-PaV generates a Macromedia Flash file, a popular multimedia format. Even though how Web content is shown depends on the type of device, mobile terminals are designed very similarly to personal computers. However, when comparing mobile and desktop terminals, differences in device characteristics, such as screen resolution and sound quality, can affect how the content is presented. Moreover, when switching between interactive and passive access styles, the presentation should be adjusted for the terminal type because missing information is often not perceived after transformation. To adapt the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content through the logotype of the keyword. Third, we segment the picture of the content into pieces and present them one after another using visual effects. These procedures can be applied to any type of terminal.",
        "reference": "In the ubiquitous environment, devices like personal digital assistants or cell phones are used to access the Internet. We created a system called ubiquitous/universal passive viewer (u-PaV) that converts Web content into a format suitable for easy viewing on mobile devices. u-PaV takes text and images from the Web and creates a multimedia file in Macromedia Flash format, a widely used multimedia format. Mobile devices and personal computers have similar Web interfaces, although they may look different depending on the device. Mobile and desktop terminals have some differences in device characteristics, such as screen resolution and sound quality, which can affect how content is presented on the device. When switching between interactive and passive access styles, the presentation needs to be adjusted for the device type because some information may be lost during transformation and not noticed by the user. We developed a visual presentation method to customize how Web content is displayed based on the device type. First, we analyze keywords to capture the emotional tone of a web page and adjust the screen's background color accordingly. Then, we use the keyword's logo to highlight the main subject of the content. Finally, we divide the content's image into sections and present them one by one using visual effects. These steps can be used on any type of device.",
        "metrics": {
          "SARI": 50.1462719644017,
          "BLEU": 0.2899298128820588,
          "FKGL": 13.5
        }
      },
      {
        "source": "We have developed two-way location aware system, named as “The Beijing Explorer”, which exchanged positioning information and users’ situation to one another using a PDA (Personal Digital Assistant) with built-in wireless LAN and a GPS (Global Positioning System) receiver in real time. Users can see their position and their chats on the screen of a PDA using the system real-timely. The system was used for the guidance of the Palace Museum (Forbidden City) in Beijing, China. We carried on experiments two times using the system. The results of experiments show that the service using positioning data and sharing contents were valuable and interesting. The correct positioning information is important for the guidance system.",
        "prediction": "We created a two-way location-aware system called “The Beijing Explorer.” This system allows users to share their location and status in real time using a PDA (Personal Digital Assistant) that has wireless internet and a GPS (Global Positioning System) receiver. Users can see their position and their chats on the PDA screen in real time. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system, and the results showed that the service, which uses positioning data and content sharing, was valuable and interesting. Accurate positioning information is important for the guidance system",
        "reference": "We created a location-aware system called \"The Beijing Explorer\" that uses a PDA with wireless LAN and GPS to exchange positioning and user information in real-time. Users can view their position and chats in real-time on the PDA screen when using the system. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system. The experiment results indicate that the service was useful and engaging because it shared positioning data and content. Accurate positioning information is crucial for the guidance system.",
        "metrics": {
          "SARI": 59.35740497433674,
          "BLEU": 0.3967971150125656,
          "FKGL": 11.3
        }
      },
      {
        "source": "A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. It is personal and private in an unprecedented manner, and its modus operandi is such that network and power failures will not be rare. Designing distributed systems where PDAs are treated as \"first class citizens\" is a  private :[59],\"assets (electronic money, keys for authentication and opening doors) will be stored in PDAs. Ownership and control of these :[59],\"assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private  :[105],\"introduce the \"open-ended argument\" to describe the design strategy we used for designing a system that is designed to reveal information to the user (as opposed to hide it). We argue and show that when systems are designed this way, the user (a human) is better able to control the system and his personal data, as he can make better decisions than the system itself based on qualitative assessment of the provided information. The system we have designed and implemented under this design guidelines is presented and discussed.",
        "prediction": "A truly personal machine called a private machine and implemented as a Personal Digital Assistant PDA is fundamentally different from traditional machines It is personal and private in an unprecedented manner and its modus operandi is such that network and power failures will not be rare Designing systems where PDAs are regarded as important tools is essential because they will store private items like electronic money and keys for authentication and access Ownership and control of these assets as well as the media that store and communicate them should remain with the user This principle must be reflected in the design of systems for privacy \n\nWe introduce the open-ended argument to describe the design strategy we used for creating a system that reveals information to the user rather than hiding it We argue and show that when systems are designed this way the user a human is better able to control the system and his personal data This is because he can make better decisions than the system itself based on a qualitative assessment of the information provided The system we have designed and implemented under these guidelines is presented and discussed",
        "reference": "A Personal Digital Assistant (PDA) used as a private machine, is fundamentally distinct from traditional machines because it is a personalized device. It offers an unprecedented level of personal and private experience, and its operation is designed to handle network and power failures, which are expected to occur frequently. In designing distributed systems, PDAs are given high importance and treated as \"first class citizens\". This means that important assets like electronic money and authentication keys will be stored in PDAs. The user should retain ownership and control of these assets and the media that stores and communicates them. This principle should be incorporated into the design of private systems. We introduced the concept of the \"open-ended argument\" to describe our design strategy, which focuses on creating systems that are intended to provide information to the user rather than hiding it. We argue and demonstrate that when systems are designed in this manner, the user (a human) gains improved control over both the system and their personal data. By enabling users to make decisions based on qualitative assessment of the information provided, they are better equipped to make informed choices compared to relying solely on the system's automated decisions. We have created a system based on the design guidelines and we will explain it.",
        "metrics": {
          "SARI": 38.266738999852365,
          "BLEU": 0.17152045536981111,
          "FKGL": 79.4
        }
      },
      {
        "source": "Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the development and initial evaluation of a personal digital assistant (PDA)-based decision support system (DSS) for pediatric depression screening in ages 8 to 18 years of age by pediatric advanced practice nurse (APN) students. Three aspects are described: selection of depression screening instrument; integration of the instrument into the PDA; and quantitative (usage) and qualitative (focus group) evaluation. Only one third of eligible patients were screened. Twenty percent of those screened were identified as at risk for mood disorder. The barriers to screening identified through focus groups included a lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, as well as perceived or real cultural barriers. Suggestions for educational, research, and interventions to integrate clinical based PDA-based screening are discussed.",
        "prediction": "Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the creation and first assessment of a digital tool to help screen for depression in children and teens aged 8 to 18, developed by nursing students who specialize in pediatrics. Three aspects are discussed: selection of the depression screening instrument; integration of the instrument into the digital tool; and both quantitative (usage) and qualitative (focus group) evaluations. Only one third of eligible patients were screened, and twenty percent of those screened were identified as at risk for mood disorders. Focus groups identified several barriers to screening, including lack of time, knowledge, intervention protocols, referral resources, usability issues with the digital tool, as well as preceptor motivation and comfort, and perceived or real cultural barriers. Suggestions for educational, research, and intervention approaches to integrate clinical-based digital screening are also discussed",
        "reference": "Depression often goes unnoticed in many pediatric settings. This paper aims to explain the creation and early assessment of a decision support system (DSS) that uses personal digital assistants (PDAs) for screening pediatric depression among children aged 8 to 18 years, carried out by pediatric advanced practice nurse (APN) students. Three aspects are discussed: choosing a depression screening tool, incorporating the tool into the PDA, and evaluating it through quantitative (usage) and qualitative (focus group) assessments. Only one third of eligible patients were screened. 20% of the screened patients were found to be at risk for a mood disorder. Barriers to screening were identified through focus groups, including lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, and cultural barriers. The paper discusses suggestions for integrating PDA-based screening into clinical practice through education, research, and interventions.",
        "metrics": {
          "SARI": 41.11440750942561,
          "BLEU": 0.306100168849718,
          "FKGL": 16.4
        }
      },
      {
        "source": "This study explored the potential of the application of wireless and mobile computing technologies to be used in improving the efficiency of patient care and education and future developments in information and communication technologies to support healthcare professionals and medical students in healthcare research, medical education and training. The design used for this study was a systematic review of published materials obtained from EMBASE and MEDLINE online databases, and the Cochrane Library database, including personal observations. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistant with expected growth of more than 75% by year-end 2007. In addition, wireless and mobile computing technologies allows Personal Digital Assistant to connect directly to networks or the Internet. Studies relating to processes of patient care and should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies is only beginning to take its first step in improving patient care and education. They have shown a positive impact on patient safety, health care efficiency, and ultimately patient satisfaction.",
        "prediction": "This study looked at how wireless and mobile computing technologies can improve patient care and education and support healthcare professionals and medical students in their research, education, and training. The design of this study was a systematic review of published materials obtained from EMBASE, MEDLINE online databases, and the Cochrane Library database, including personal observations. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistants, with expected growth of more than 75% by the end of 2007. Additionally, wireless and mobile computing technologies allow Personal Digital Assistants to connect directly to networks or the Internet. Studies related to patient care processes should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies are just beginning to show improvements in patient care and education. They have a positive impact on patient safety, healthcare efficiency, and ultimately patient satisfaction.",
        "reference": "This study examined how wireless and mobile computing technologies can enhance patient care and education. It also explored how advancements in information and communication technologies can support healthcare professionals and medical students in research, education, and training. We conducted a systematic review for this study, analyzing published materials from databases such as EMBASE, MEDLINE, and the Cochrane Library, as well as personal observations. Over 50% of healthcare professionals and medical students currently use Personal Digital Assistants, and this number is expected to grow by more than 75% by the end of 2007. Wireless and mobile computing technologies enable Personal Digital Assistants to connect directly to networks or the Internet. Studies should examine whether mobile computing technologies can save time in patient care processes. Wireless and mobile computing technologies are still in the early stages of improving patient care and education. These technologies have positively impacted patient safety, healthcare efficiency, and patient satisfaction.",
        "metrics": {
          "SARI": 56.01346598577879,
          "BLEU": 0.46352902383212236,
          "FKGL": 15
        }
      },
      {
        "source": "Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore practical training is an effective learning opportunity for developing the practical nursing care ability of nursing students. Moreover, at hospitals, which are important training facilities, with regard to medical safety, the use of learning tools that produce electrical waves is not possible. So, we created a learning support environment that facilitates the imagination of nursing techniques, and enables effective preparation, review, and learning at anytime and anywhere using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named \"digital nursing dictionary\" that we developed and the evaluation of the practices using it.",
        "prediction": "Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore, practical training is an effective learning opportunity for developing the practical nursing care ability of nursing students. Moreover, at hospitals, which are important training facilities, medical safety prevents the use of learning tools that use electrical waves. To address this, we created a learning support environment that helps students think creatively about nursing techniques and enables effective preparation, review, and learning at any time and anywhere using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named \"digital nursing dictionary\" that we developed and the evaluation of the practices using it.",
        "reference": "Nursing students can learn through practical training and by interacting with patients in real medical settings. Practical training is effective for developing the practical nursing care ability of nursing students. Moreover, hospitals, which are important training facilities, do not allow the use of learning tools that produce electrical waves due to medical safety concerns. We developed a learning support environment that helps nursing students visualize and practice techniques. It allows for convenient preparation, review, and learning using a portable digital assistant (PDA) device, enabling anytime and anywhere practical training. We developed an educational tool called the 'digital nursing dictionary' and evaluated its effectiveness, as outlined in this paper.",
        "metrics": {
          "SARI": 45.15948830197571,
          "BLEU": 0.3575397581549042,
          "FKGL": 16.7
        }
      },
      {
        "source": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. Manual reconstruction of web pages for these devices would ease the problem; however, the large variation of display capabilities will greatly increase the burden of web page designers since they have to customize a web page for each possible display device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "prediction": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. While creating separate versions of web pages for these devices would help, the wide range of display options would make it much harder for web designers, as they would need to adjust a web page for every different device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "reference": "Small screens and low resolutions on mobile devices are a major obstacle for the widespread adoption of web applications in a wireless environment. PDA screens are often too small to display entire web pages. Low-resolution devices like WebTV face the same problem. Manually reconstructing web pages for these devices would help, but the wide range of display capabilities would make it challenging for web page designers as they would have to customize each web page for every possible display device, increasing their workload. This paper suggests a system for segmenting and presenting documents. The system divides a web document into logical segments automatically, using information about the screen size, document structure, and content. The system also extracts summaries and overviews to make navigation easier. The system optimizes the use of the screen for information finding by showing the segments and structure of a web document.",
        "metrics": {
          "SARI": 45.07770932838415,
          "BLEU": 0.2640744097935605,
          "FKGL": 12.4
        }
      },
      {
        "source": "Take 8-bit microcontroller (C8051F005) as the nucleus and study a kind of voltage harmonic monitor device that satisfies the long-range monitor. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). This device has two work modes: ”native” and ”remote”. It also can communicate with monitor center through telephone line, serial port, IC card, PDA (Personal Digital Assistant), etc.",
        "prediction": "Take the 8-bit microcontroller (C8051F005) as the core and study a type of voltage harmonic monitor device designed for long-range monitoring. The harmonic analysis algorithm uses Fast Fourier Transform (FFT). This device has two work modes: \"native\" and \"remote.\" It can also communicate with the monitor center through a telephone line, serial port, IC card, PDA (Personal Digital Assistant), and more.",
        "reference": "Study a voltage harmonic monitor device for long-range monitoring using the 8-bit microcontroller C8051F005 as the core. The harmonic analysis uses FFT. The device has two modes: \"native\" and \"remote\". It can communicate with a monitoring center via different methods such as telephone line, serial port, IC card, and PDA (Personal Digital Assistant).",
        "metrics": {
          "SARI": 65.03625622333597,
          "BLEU": 0.4082797971814785,
          "FKGL": 10.4
        }
      },
      {
        "source": "With network and small screen device improvements, such as wireless abilities, increased memory and CPU speeds, users are no longer limited by location when accessing on-line information. We are interested in studying the effect of users switching from a large screen device, such as a desktop or laptop to use the same web page on a small device, in this case a PDA (Personal Digital Assistant). We discuss three common transformation approaches for display of web pages on the small screen: Direct Migration, Linear and Overview. We introduce a new Overview method, called the Gateway, for use on the small screen that exploits a user’s familiarity of a web page. The users in an initial study prefer using the Gateway and Direct Migration approach for web pages previously used on the large screen, despite the common Linear approach used by many web sites.",
        "prediction": "With network and small screen device improvements such as wireless abilities increased memory and CPU speeds users are no longer limited by location when accessing online information We are interested in studying how users switch from large screen devices like desktops or laptops to the same web page on a small device such as a PDA Personal Digital Assistant We talk about three common ways to display web pages on small screens Direct Migration Linear and Overview Additionally we introduce a new Overview method called the Gateway which takes advantage of a user’s familiarity with a web page In an initial study users prefer the Gateway and Direct Migration approaches for web pages they previously accessed on large screens even though many websites commonly use the Linear approach",
        "reference": "Improvements in network and small-screen devices, such as wireless capabilities, faster CPU speeds, and increased memory, have eliminated location restrictions for accessing online information. We want to study the impact of users switching from a desktop or laptop to a PDA (Personal Digital Assistant) when accessing the same web page. We are looking at three common ways to display web pages on small screens: Direct Migration, Linear, and Overview. Gateway is our new Overview method for small screens that use the user's familiarity with a web page. Many websites use the Linear method, but an initial study showed that users prefer using the Gateway and the Direct Migration method for web pages.",
        "metrics": {
          "SARI": 42.145523211588376,
          "BLEU": 0.2284022389954469,
          "FKGL": 53.2
        }
      },
      {
        "source": "The limitations and constraints of mobile systems need to be adequately addressed in software development. We have been developing a taxonomy of risks based on SEIu0027s risk questionnaire and applied it during the development of a negotiation support system for a Personal Digital Assistant (PDA). In our planned research, we will explore how we can better integrate existing risk management strategies and Agile Methods (AM).",
        "prediction": "The limitations and constraints of mobile systems need to be adequately addressed in software development. To address this, we have created a list of potential risks using SEI's risk questionnaire and used it while developing a system to help with negotiations on a Personal Digital Assistant PDA. In our planned research, we will explore how we can better integrate existing risk management strategies and Agile Methods AM",
        "reference": "During software development, restrictions due to mobile systems need to be addressed. Based on SEI's risk questionnaire, we developed a classification of risks and applied it while developing a negotiation support system for a PDA. Risk management strategies and Agile Methods and how we can integrate them better will be studied in our future research.",
        "metrics": {
          "SARI": 42.66329495054932,
          "BLEU": 0.20299960760848068,
          "FKGL": 13.2
        }
      },
      {
        "source": "Expansions of mobile services and private data have required increased level of protection. Speaker recognition, one of the biometric technologies, arises lots of research interests for its simple, cheap and convenient characteristics. In this paper, a robust speaker recognition system which facilitates reliable authentication with multi-channel voices is presented. A large multi-channel corpus, including mobile phone, personal digital assistant (PDA), telephone and microphone, is collected to evaluate the system performance.",
        "prediction": "Expansions of mobile services and private data have required increased levels of protection. Speaker recognition, which is one of the biometric technologies, has sparked considerable research interest due to its simplicity, affordability, and convenience. In this paper, we present a strong speaker recognition system that allows for reliable verification using voices from different sources. To test how well the system works, we gathered a large collection of audio from various sources, including mobile phones, PDAs, telephones, and microphones.",
        "reference": "The growth of mobile services and private data necessitates higher levels of protection. Speaker recognition, a technology using the voice of users to identify them, leads to a lot of research interest because of its simple, cheap, and convenient characteristics. This work presents a powerful speaker recognition system that makes the authentification with multi-channel voices easier. Data are collected on mobile phones, PDAs, telephones, and microphones to evaluate the system's performance.",
        "metrics": {
          "SARI": 51.132048022741415,
          "BLEU": 0.25240074945632657,
          "FKGL": 13.3
        }
      },
      {
        "source": "This paper presents a method to generate unique and nevertheless highly random pseudonyms in a distributed environment. More precisely, each user can now generate his pseudonym locally in his personal security environment, e.g. in his smart card or his personal digital assistant. There is no need for any information interchange between issuing parties or global data (especially keys), except unique identifiers for each user and each device of the system. Additionally the holder can prove, that he generated a specific pseudonym without revealing his identity and he can reveal his identity by disclosing the pseudonym. Whereas the verifier of a disclosed pseudonym can be sure, that the presenter of the pseudonym is the holder of the pseudonym (i.e. the person which originally generated it). The identifier of the user and the identifier of the useru0027s device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form.",
        "prediction": "This paper presents a method to generate unique and highly random pseudonyms in a distributed environment. More precisely, each user can now generate their pseudonym locally in their personal security environment, such as on a smart card or personal digital assistant. There is no need for any information exchange between issuing parties or global data, especially keys, except for unique identifiers for each user and each device in the system. The holder can show that they created a specific pseudonym without giving away their identity, and they can also reveal their identity by sharing the pseudonym. The verifier of a disclosed pseudonym can be sure that the person presenting it is the original holder. The identifiers of the user and the user’s device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form",
        "reference": "This work presents a method to generate unique and random pseudonyms in a distributed system. Every user can now create his pseudonym locally in their own personal security environment, e.g. in his chip card or PDA. No information exchange is required between issuing parties or global data, except for unique identifiers for each user and device in the system. The holder can also prove that he generated a distinct pseudonym without revealing his identity; by divulging it, he can reveal his identity. In that case, the verifier of a divulgated pseudonym can be sure that the one who presents the pseudonym is holding it. (i.e. the person that originally created it) The user identifier and device's identifier will be used to create unique pseudonyms but to ensure anonymity with pseudonyms, both identifiers will be stored within the pseudonym in encrypted form.",
        "metrics": {
          "SARI": 45.88954289028798,
          "BLEU": 0.3431035619497672,
          "FKGL": 13.9
        }
      },
      {
        "source": "In todayu0027s mobile information society, location-based services play an increasingly important role. These services are to be accessed by users with a mobile end device for the use of city maps, route planning, navigation, or traffic information. Mobile end devices, however, do not have computing power or storage capacity comparable to that of a personal computer or laptop. These deficits can be bypassed by employing special methods in development of the respective applications. This paper outlines the experience gained in the development of a prototype for route computation in public transport networks to be used on a personal digital assistant. Subdivision of the route computation process into various steps for minimizing the computation expenditure on the mobile end device plays a significant role.",
        "prediction": "In today's mobile information society location-based services play an increasingly important role Users can access these services on mobile devices for city maps route planning navigation or traffic information However mobile devices do not have the same computing power or storage space as personal computers or laptops These limitations can be addressed by using special methods in the development of applications This paper outlines the experience gained in developing a prototype for route computation in public transport networks to be used on a personal digital assistant A key aspect is subdividing the route computation process into various steps to minimize computing demands on the mobile device",
        "reference": "Location-based service's role in today's mobile information society is increasing . Users can access city maps, route planning, navigation, or traffic information with their mobile devices. Compared to laptops or personal computers, mobile devices do not have the same computing power or storage capacity. This shortage can be overcome by using specific methods during the development of the respective application. This paper shows the gained experience from developing a prototype designed for a PDA that calculates routes in public transport networks. Dividing the route computation process into multiple steps to reduce the computational load on the mobile device is crucial.",
        "metrics": {
          "SARI": 48.951384348368926,
          "BLEU": 0.2755415818933948,
          "FKGL": 48.2
        }
      },
      {
        "source": "DIANE is a digital assistant system that aims to fasten the doctor access to various informations at the hospital such as health care facility, medical records, and also human resource data. The fasten access could be achieved by implementing face recognition and live streaming as part of the digital assistant system.",
        "prediction": "DIANE is a digital assistant system designed to make it easier for doctors to access important information at the hospital such as healthcare facilities medical records and human resource data This improved access can be achieved by incorporating face recognition and live streaming into the digital assistant system",
        "reference": "DIANE is a digital assistant system that allows the doctor a faster access patient and hospital information. This could be done with face recognition and live streaming.",
        "metrics": {
          "SARI": 52.26709772075871,
          "BLEU": 0.18666736823265495,
          "FKGL": 25.6
        }
      },
      {
        "source": "Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time.",
        "prediction": "Digital assistants are becoming more common in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time.",
        "reference": "Digital assistants are becoming popular in our daily lives. It will allow users to do multiple tasks in a faster way.",
        "metrics": {
          "SARI": 47.98501125205589,
          "BLEU": 0.22872196013470597,
          "FKGL": 9.7
        }
      },
      {
        "source": "As extensive experimental research has shown individuals suffer from diverse biases in decision-making. In our paper we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, managerial decisions which are based on different levels of organizational complexity and different incentive systems suffer from biases known from descriptive decision theory. The results illustrate how biases in combination with each other and in different organizational contexts affect organizational performance. We find that, contrary to intuition, some combinations of biases significantly improve organizational performance while these biases negatively affect organizational performance when they occur separately. This might evoke considerations whether decision-making should be as rational as possible.",
        "prediction": "As extensive experimental research has shown, individuals suffer from diverse biases in decision-making. In our paper, we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, we found that managers' decisions, which are based on different levels of organizational complexity and various incentive systems, are influenced by biases known from descriptive decision theory. The results show how different biases, when combined and in various organizational settings, impact organizational performance. We find that, surprisingly, some combinations of biases significantly improve organizational performance, while the same biases can harm performance when they occur separately. This might evoke considerations of whether decision-making should be as rational as possible",
        "reference": "Research showed that people suffer from diverse biases (disproportionate weight in favor of or against an idea) in decision-making. We analyzed the effects of decision-making biases of managers on organizational performance. In the simulations, managers decisions which are based on organizational skills and rules, suffer from several known biases. The results illustrate how combined biases in different contexts affect organizational performance. Contrary to expectations, some combinations of biases improve organizational performance. However, when they occur separately, they decrease performance. This raises questions about the importance of rationality in decision-making.",
        "metrics": {
          "SARI": 42.39225212498135,
          "BLEU": 0.23942222373191394,
          "FKGL": 17.8
        }
      },
      {
        "source": "Big data, the enhanced ability to collect, store and analyze previously unimaginable quantities of data in tremendous speed and with negligible costs, delivers immense benefits in marketing efficiency, healthcare, environmental protection, national security and more. The central tenets of the current privacy framework, the principles of data minimization and purpose limitation, are severely strained by the big data technological and business reality. As we increasingly interact with these artificial agents in unsupervised settings, with no human mediators, their seeming autonomy and increasingly sophisticated functionality and behavior, raises legal and philosophical questions. The focus on the machine is a distraction from the debate surrounding data driven ethical dilemmas, such as privacy, fairness and discrimination. The machine may exacerbate, enable, or simply draw attention to the ethical challenges, but it is humans who must be held accountable. Policymakers should seek to devise agreed-upon guidelines for ethical data analysis and profiling. Such guidelines would address the use of legal and technical mechanisms to obfuscate data; criteria for calling out unethical, if not illegal, behavior; categories of privacy and non-privacy harms; and strategies for empowering individuals through access to data in intelligible form.",
        "prediction": "Big data which is the ability to collect store and analyze vast amounts of information quickly and at low costs offers great benefits in areas like marketing efficiency healthcare environmental protection and national security The main ideas of today's privacy rules which focus on collecting only necessary data and using it for specific purposes are under serious pressure from the realities of big data technology and business As we increasingly interact with artificial agents in situations without human oversight their seeming autonomy and advanced capabilities raise important legal and philosophical questions Focusing too much on the machines distracts us from discussing ethical issues related to data such as privacy fairness and discrimination While machines might make these ethical challenges more visible or even exacerbate them it is ultimately humans who must take responsibility Policymakers should aim to create agreed-upon guidelines for ethical data analysis and profiling These guidelines should cover the use of legal and technical methods to protect data criteria for identifying unethical or illegal behavior categories of privacy and non-privacy harms and strategies to empower individuals by giving them access to data in understandable formats",
        "reference": "Big data (the ability to collect, store and analyze huge quantities of data in a really fast and cheap way) offers immense benefits in marketing, healthcare, environment, national security and more. The principles of privacy, limitation of data collection and use are stretched by the business and technological reality of big data. Our increasing interactions with these technologies without human supervision, and their growing autonomy, functionality and behavior raises legal and philosophical questions. The focus on the machine is a distraction from the debate on ethical dilemmas about data like privacy, fairness and discrimination. The machine may influence the ethical challenges, but the humans must remain responsible. Responsible people should find guidelines for ethical data analysis and collection. These guidelines would deal with mechanisms to protect data, criteria about unethical or illegal behavior, privacy harms, and strategies to give people access to data.",
        "metrics": {
          "SARI": 39.688171166064954,
          "BLEU": 0.12854126270378213,
          "FKGL": 80.9
        }
      },
      {
        "source": "Amazon's Echo, and Apple's Siri have drawn attention from different user groups; however, these existing commercial VUIs support limited language options for users including native English speakers and non-native English speakers. we conducted a usability study of the Google Home Smart Speaker with 20 participants including native English and non-native English speakers to understand their differences in using the Google Home Smart Speaker. The findings show that compared with their counterparts, the native English speakers had better and more positive user experiences in interacting with the device. It also shows that users' English language proficiency plays an important role in interacting with VUIs. The findings from this study can create insights for VUI designers and developers for implementing multiple language options and better voice recognition algorithms in VUIs for different user groups across the world.",
        "prediction": "Amazon's Echo and Apple's Siri have drawn attention from different user groups however these existing commercial VUIs support limited language options for users including both native and non-native English speakers We conducted a usability study of the Google Home Smart Speaker with 20 participants which included both native and non-native English speakers to understand their differences in using the device The findings show that native English speakers had a better user experience with the device than non-native speakers Additionally users' English language proficiency plays an important role in interacting with VUIs The insights from this study can help VUI designers and developers implement multiple language options and improve voice recognition algorithms for diverse user groups around the world",
        "reference": "The VUIs (voice user interface) like Amazon’s Echo or Apple’s Siri are popular nowadays. However, they have limited language options for users. We studied the usability of the Google Home Smart Speaker with native and non-native English speakers to understand their differences in using this device. The results show that the native English speakers had a better experience with the device than the non-native. It also shows that language skills play an important role. The results of this study can help VUI developers for improving language options and voice recognition in the devices.",
        "metrics": {
          "SARI": 47.566802657925805,
          "BLEU": 0.3037389827652799,
          "FKGL": 51.7
        }
      },
      {
        "source": "the smart speakers cannot distinguish human voice from machine voice. a method to identify which of human or machine is sending voice commands to a smart speaker is desired. to prevent such machine-voice based attacks to a smart speaker in absence of residents, we propose a system consisting of a speaker and a microphone array to detect the existence of a human nearby, supposing it can be incorporated in a smart speaker in the future.",
        "prediction": "The smart speakers cannot distinguish human voice from machine voice. A method to identify whether a human or a machine is sending voice commands to a smart speaker is needed. To stop attacks on smart speakers using machine voices when no one is home, we suggest a system made of a speaker and multiple microphones to check if a person is nearby, assuming it can be included in a smart speaker in the future.",
        "reference": "The smart speakers cannot differentiate between human voice and machine voice. A method to identify if the voice command is from a human or a machine is required. We propose a system composed of a speaker and microphones to detect the presence of humans. It could help to prevent attacks on a smart speaker with a machine voice in absence of residents",
        "metrics": {
          "SARI": 46.695005397703994,
          "BLEU": 0.24245726748709784,
          "FKGL": 11.7
        }
      },
      {
        "source": "As smart speakers with voice interaction capability permeate continuously in the world, more and more people will gradually get used to the new interaction medium–voice. Although speech recognition, natural language processing (NLP) have been greatly improved over the past few years, users still may encounter errors from time to time like “cannot understand”, “no requested audio resource (such as music)”, which can frustrate users. Therefore, when an error message is reported, it is vital that the smart speaker gives an effective and proper response. However, currently the response strategies adopted by leading smart speaker brands in China differed mainly on two dimensions: “apology or not” and “humor or neutral”. We explored user’s preference of response strategies under two error scenarios——“cannot understand” and “no requested audio resource”. Two dependent variables (satisfaction and perceived sincerity of response) were measured. The results showed that participants were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no significant impact on the perception of satisfaction and sincerity. But in the “cannot understand” scenario, humorous expression decreased perceived sincerity.",
        "prediction": "As smart speakers with voice interaction capability continue to spread around the world, more people are gradually getting used to interacting through voice. Although speech recognition and natural language processing (NLP) have improved greatly over the past few years, users may still encounter errors like “cannot understand” or “no requested audio resource (such as music),” which can be frustrating. Therefore, when an error message occurs, it is crucial for the smart speaker to provide an effective and appropriate response. \n\nCurrently, the leading smart speaker brands in China use different response strategies based on two main factors: whether to apologize and whether to be funny or serious. We examined what users preferred in these response strategies during two types of errors—“cannot understand” and “no requested audio resource.” We measured user satisfaction and how sincere they felt the response was. \n\nThe results revealed that users felt more satisfied and believed the smart speaker was more sincere when it apologized in both situations. In the 'no requested audio resource' situation, humor did not have a significant impact on satisfaction and sincerity. However, in the 'cannot understand' situation, using humor made people feel that the smart speaker was less sincere.",
        "reference": "Smart speakers are becoming popular around the world and people will get used to this new way of interacting with the voice. Even with the progress of speech recognition and natural language processing (interactions between computers and human language), users may still get errors like “cannot understand” or “no requested audio resource” which can frustrate users. So, it is important that the smart speaker gives an effective and proper response during an error message. The responses of the most popular brand of smart speakers are based on 2 elements, apology and humor. We studied user’s preference in two error scenarios : “cannot understand” and “no requested resource” Satisfaction of the user and perceived sincerity of the response were measured. The results showed that users were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no impact on the satisfaction or sincerity. But in the “cannot understand” scenario, humor decreased perceived sincerity.",
        "metrics": {
          "SARI": 37.39696660767167,
          "BLEU": 0.16693036756250354,
          "FKGL": 13
        }
      },
      {
        "source": "A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process (specifically, drug molecules, compounds in high throughput screening libraries, combinatorial chemistry building blocks and nondrug molecules). Significant differences were observed between the pharmacophore profiles obtained for the drug molecules and those obtained for the high-throughput screening compounds, which appear to be closely related to the nondrug pharmacophore distribution. It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the property-based optimization of compound selection and library design processes, thus improving the odds of success in lead discovery projects.",
        "prediction": "A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process specifically drug molecules compounds in high throughput screening libraries combinatorial chemistry building blocks and nondrug molecules. Clear differences were found between the pharmacophore profiles of drug molecules and those from high-throughput screening compounds which seem to be closely linked to the distribution of nondrug compounds. This analysis of pharmacophore profiles could serve as an additional tool for optimizing compound selection and library design processes thereby improving the chances of success in lead discovery projects.",
        "reference": "A pharmacophore analysis investigated and compared different compounds of the drug discovery process. Significant differences were observed between the pharmacophore profiles of the drug molecules and the high-throughput screening compounds, which appear to be related to the nondrug pharmacophore distribution. It is suggested that the analysis could be used as an additional tool for the optimization of compound selection and library design processes..",
        "metrics": {
          "SARI": 30.301904528341534,
          "BLEU": 0.20710913325211355,
          "FKGL": 17.9
        }
      },
      {
        "source": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been previously designed for this express purpose, and provides a layered hierarchy of meaningful terms which can be linked to.",
        "prediction": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. With a well-designed ontology, important information can be captured by stating facts as three-part statements called semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged, and evaluated in various ways, similar to other areas of drug discovery informatics. The BioAssay Ontology (BAO) was specifically designed for this purpose and provides a layered hierarchy of meaningful terms that can be linked to.",
        "reference": "Annotation of bioassay protocols with web vocabulary can make experiment descriptions machine-readable. Protocols use concise scientific English which prevents most of analysis by software algorithms. There is a sufficient ontology, so the pertinent information can be written using semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated. The BioAssay Ontology (BAO) has been designed for this purpose, and provides many meaningful terms.",
        "metrics": {
          "SARI": 51.09056494947597,
          "BLEU": 0.4180967193138684,
          "FKGL": 14.6
        }
      },
      {
        "source": "Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Subsequent bioinformatic analysis is typically done with the help of open source tools, where each application performs a single step towards the final result. This situation leaves the bioinformaticians with the tasks to combine the tools, manage the data files and meta-information, document the analysis, and ensure reproducibility.",
        "prediction": "Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Subsequent bioinformatic analysis is usually performed using open source tools, where each application handles one step towards the final result. This means that bioinformaticians have to combine the tools, manage the data files and additional information, document their work, and ensure that the analysis can be repeated.",
        "reference": "Next generation sequencing (NGS) produces datasets of billions of reads and thousands of samples. The analysis is typically done with open source tools, performing a single step towards the final result. The bioinformaticians have to combine the tools, manage the files and information, document the analysis, and ensure reproducibility.",
        "metrics": {
          "SARI": 46.31555595706702,
          "BLEU": 0.405915393605982,
          "FKGL": 14.1
        }
      },
      {
        "source": "SPSS Clementinel2.0 statistical software was used to mine the association rules between Etiology and traditional Chinese medicine (TCM), Syndromes and TCM, Symptoms and TCM. The classic Apriori algorithm is useful to mine cases of influenza treated by contemporary famous old Chinese medicine.",
        "prediction": "SPSS Clementine 2.0 software was used to discover patterns between the causes of diseases and traditional Chinese medicine TCM as well as between different syndromes and symptoms related to TCM Additionally the classic Apriori algorithm is useful for analyzing cases of influenza treated by well-known traditional Chinese medicine",
        "reference": "SPSS Clementinel2.0 statistical software was used to extract data and find relations between traditional Chinese medicine and other medicine. The classic Apriori algorithm is useful to find cases of influenza treated by old Chinese medicine.",
        "metrics": {
          "SARI": 50.12234456933238,
          "BLEU": 0.23853965544010916,
          "FKGL": 15
        }
      },
      {
        "source": "Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype and phenotype in humans and other species. a Bayesian framework for genotype estimation for mixtures of multiple bacteria, named as Genetic Polymorphisms Assignments (GPA) has reduced the false discovery rate (FDR) and mean absolute error (MAE) in single nucleotide variant (SNV) identification.",
        "prediction": "Identifying antimicrobial resistant AMR bacteria in metagenomics samples is essential for public health and food safety Next-generation sequencing NGS technology has provided a powerful tool to identify genetic variation and understand the relationships between genotype and phenotype in humans and other species A method called Genetic Polymorphisms Assignments GPA helps estimate the types of different bacteria This method has lowered the chances of false results and improved accuracy in identifying genetic changes known as single nucleotide variants SNVs",
        "reference": "Identifying bacterias that are resistant to antimicrobial in samples of genetic material collected from the environment is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype (genetic code of an individual) and phenotype (physical characteristics of an indvidual) in humans and other species. The Genetic Polymorphism Assignements (GPA) is a framework which determines the genetic constitution of multiple bacteria while reducing the number of false positive results and improving the accuracy of calculations when identifying the variation of a single nucleotide.",
        "metrics": {
          "SARI": 43.23241289295112,
          "BLEU": 0.1791557312610535,
          "FKGL": 37.3
        }
      },
      {
        "source": "The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (altering the genetic sequence) and gene regulation (without altering the genetic sequence). These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "prediction": "The CRISPR/Cas9 system is a powerful and versatile tool used for making specific changes to genetic material which includes editing genes and controlling gene activity without changing the genetic sequence. This technology requires the design of single guide RNAs sgRNAs that are efficient and specific. However designing these sgRNAs remains challenging because it involves considering many criteria",
        "reference": "The CRISPR/Cas9 system is a powerful technology for gene editing and gene regulation. These applications require the design of single guide RNAs (which are enzymes that can delete, insert or alter DNA) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "metrics": {
          "SARI": 43.34037596714973,
          "BLEU": 0.3205897910147988,
          "FKGL": 14.2
        }
      },
      {
        "source": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals; it might improve the health of individuals and communities, and, if successful, might be seen as a public good. However, enhancing future generations will require In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. Remarkably, the necessary involvement of women in an enhancing scenario has not been discussed by its proponents. The present discourse on moral obligations of future generations, although not referring to women, seems to imply that women might be required, morally, if not legally, to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. Despite the lack of a unified feminist conclusion in the discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.",
        "prediction": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals; it could enhance the health of individuals and communities, and, if successful, might be seen as a public good. However, improving future generations will need the use of IVF (in vitro fertilization) and genetic testing before implantation. Remarkably, the necessary involvement of women in this enhancement scenario has not been discussed by its proponents. The current discourse on the moral obligations of future generations, although not specifically mentioning women, seems to imply that women might be required, morally, if not legally, to reproduce with IVF. Enhancing future generations will be gendered unless artificial wombs are developed. These are challenging issues that require a broader perspective from both women and men. Despite the lack of a unified feminist conclusion in discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.",
        "reference": "Gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals. However, enhancing future generations will require In Vitro Fertilisation (IVF) and Pre-implantation Genetic Diagnosis. The necessary involvement of women in an enhancing scenario has not been discussed. The discourse on moral obligations of future generations seems to imply that women might be required to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. There are no unified feminist conclusion about the merits and risks of human genome modification. There is also an urgent need to clarify the role of women in this scenario.",
        "metrics": {
          "SARI": 35.11397253711299,
          "BLEU": 0.44620440124573046,
          "FKGL": 13.1
        }
      },
      {
        "source": "CRISPR-Cas is a tool that is widely used for gene editing. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins, which are powerful molecules that inhibit the CRISPR-Cas system, may have the potential to promote better utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. Additionally, more in-depth research on these proteins would help researchers to better understand the co-evolution of bacteria and phages. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "prediction": "CRISPR-Cas is a tool that is widely used for gene editing. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins are strong molecules that block the CRISPR-Cas system and could help improve how we use this system for gene editing, especially in gene therapy. Additionally, more in-depth research on these proteins would help researchers better understand the co-evolution of bacteria and phages. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "reference": "CRISPR-Cas is a tool used for gene editing. However, unintended genetic modifications may occur. Anti-CRISPR proteins (molecules that inhibit the CRISPR-Cas system) may improve utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. More research on these proteins would help to understand the co-evolution of bacteria and bacteriophages (virus that infects and replicates within bacteria). It is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "metrics": {
          "SARI": 41.72903977699381,
          "BLEU": 0.41101017780339355,
          "FKGL": 10.9
        }
      },
      {
        "source": "CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its function of gene perturbation, a variety of gene editing techniques have been developed to achieve different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen.",
        "prediction": "CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Because it can change genes, many different gene editing techniques have been created to achieve various goals. Target locations in a DNA strand can be precisely broken and repaired, allowing genes to be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagent preparation and the ease of conducting experiments make it a powerful tool for high-content screening",
        "reference": "CRISPR is a popular research area known for its efficiency and specificity in editing DNA sequences. A variety of gene editing techniques have been developed for different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it a powerful tool.",
        "metrics": {
          "SARI": 34.961643467550324,
          "BLEU": 0.4208505837397069,
          "FKGL": 11.4
        }
      },
      {
        "source": "The CRISPR-Cpf1 system has been successfully applied in genome editing. However, target efficiency of the CRISPR-Cpf1 system varies among different gRNA sequences. Using machine learning technology, a SVM model was created to predict target efficiency for any given gRNAs. the first web service application, CRISPR-DT (CRISPR DNA Targeting), to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and specificity is available.",
        "prediction": "The CRISPR-Cpf1 system has been successfully applied in genome editing. However, the effectiveness of the CRISPR-Cpf1 system varies among different gRNA sequences. Using machine learning, a model was created to predict how well different gRNAs work. The first web service application, CRISPR-DT (CRISPR DNA Targeting), is available to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both efficiency and accuracy.",
        "reference": "The CRISPR-Cpf1 system (a DNA-editing technology) has been successfully applied in gene editing. However target efficiency of the CRISPR-Cpf1 system varies among different gRNA (piece of RNA that serves as a guide to DNA-targeting enzymes) sequences. Using machine learning technology, a support-vector machine model (algorithm used for data classification) was created to predict the target efficiency for any given gRNAs. CRISPR-DT (CRISPR DNA Targeting) is available. It is the first web service application to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and target specificity (recognition mecanism).",
        "metrics": {
          "SARI": 38.65869770121295,
          "BLEU": 0.3293493333329102,
          "FKGL": 10.6
        }
      },
      {
        "source": "Emerging self-driving vehicles are vulnerable to different attacks due to the principle and the type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via vehicular ad hoc networks (VANETs). VANETs add new threats to self-driving vehicles that contribute to substantial challenges in autonomous systems. These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for self-driving and semi self-driving cars. The hybrid detection system relies on the Back Propagation neural networks (BP), to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.",
        "prediction": "Emerging self-driving vehicles are vulnerable to different attacks due to the principle and type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via vehicular ad hoc networks (VANETs). VANETs add new threats to self-driving vehicles that contribute to substantial challenges in autonomous systems. These communication systems make self-driving vehicles open to various harmful attacks, including Sybil attacks, Denial of Service (DoS), and others like black hole, grey hole, and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for both self-driving and semi self-driving cars. The hybrid detection system relies on Back Propagation neural networks (BP) to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.",
        "reference": "Self-driving vehicles are vulnerable to different attacks because of their communication systems. These vehicles use external communication via a network. This network add new threats to self-driving vehicles that contribute to the challenges in autonomous systems. With these communication systems, self-driving vehicles are vulnerable to many types of malicious attacks on the network. In this paper, we propose an intelligent security system to secure external communications for self-driving cars. The system will detect a common type of attack in the network: Denial-of-Service (attack trying to make network resource unavailable). The results show that the intrusion detection system is capable of identifying malicious vehicles in self-driving vehicles.",
        "metrics": {
          "SARI": 41.448551659846984,
          "BLEU": 0.34153712047834917,
          "FKGL": 14.8
        }
      },
      {
        "source": "Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications. When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, might become deadlocked, or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols, while ensuring safety.",
        "prediction": "Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is ensuring that multiple vehicles can work together safely using sensors and communication with each other. When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, become deadlocked, or slam on the brakes, making it uncomfortable or unsafe for passengers. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named the Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems to cooperate with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput compared to simple traffic protocols while ensuring safety",
        "reference": "Self-driving vehicle technologies are progressing and are expected to play a significant role in the future of transportation. A main challenge for self-driving vehicles on public roads is the safe cooperation, collaboration and communication with other vehicles. When self-driving vehicles are in the same area, they might collide, become blocked, or suddenly brake and make uncomfortable or unsafe the passengers. We study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other vehicles (self-driving or human-driven). Our simulation results show that our traffic protocol leads to a better traffic flow, while ensuring safety.",
        "metrics": {
          "SARI": 46.02550875409479,
          "BLEU": 0.4611776418851654,
          "FKGL": 15.7
        }
      },
      {
        "source": "This paper provides a holistic study of how stock prices vary in their response to financial disclosures across different topics. Thereby, we specifically shed light into the extensive amount of filings for which no a priori categorization of their content exists. For this purpose, we utilize an approach from data mining - namely, latent Dirichlet allocation - as a means of topic modeling. This technique facilitates our task of automatically categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings from U.S. companies. We then evaluate the subsequent stock market reaction. Our empirical evidence suggests a considerable discrepancy among various types of news stories in terms of their relevance and impact on financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit rating, but also for disclosures regarding business strategy, the health sector, as well as mergers and acquisitions. Our results yield findings that benefit managers, investors and policy-makers by indicating how regulatory filings should be structured and the topics most likely to precede changes in stock valuations.",
        "prediction": "This paper provides a holistic study of how stock prices vary in response to financial disclosures across different topics. We specifically focus on the extensive number of filings that lack a prior categorization of their content. To address this, we use a data mining method called latent Dirichlet allocation for topic modeling. This technique helps us automatically categorize the content of more than 70,000 regulatory 8-K filings from U.S. companies. We then evaluate the subsequent stock market reactions. Our empirical evidence shows a significant difference among various types of news stories in terms of their relevance and impact on financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit ratings, as well as disclosures regarding business strategy, the health sector, and mergers and acquisitions. Our findings provide valuable insights for managers, investors, and policymakers, indicating how regulatory filings should be structured and which topics are most likely to precede changes in stock valuations.",
        "reference": "This paper studies how stock prices vary with financial disclosures. We particularly study the many uncategorized filings. We use latent Dirichlet allocation for topic modeling. This helps automatically predict the categories based on the content of 70,000 regulatory 8-K filings from U.S. companies, which announce significant events relevant to shareholders. We then evaluate the stock market reaction. Evidence suggests a large difference among types of news stories in relevance and impact on financial markets. For example, we find unusual returns in response to disclosures about earnings results, credit rating, business strategy, the health sector, mergers and acquisitions. Our results are useful for managers, investors and policy-makers. They show how to structure regulatory filings and which topics influence stock valuations.",
        "metrics": {
          "SARI": 51.471290892949696,
          "BLEU": 0.31228370027583147,
          "FKGL": 13.5
        }
      },
      {
        "source": "A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.",
        "prediction": "A significant problem of using deep learning techniques is the limited amount of data available for training. While there are some datasets for popular issues like item recognition and classification for self-driving cars, the availability is very limited in the industrial robotics field. In earlier research, we trained a special type of neural network called a Convolutional Neural Network (CNN) to find the robot's body in an image and predict the 3D positions of its joints using only a 2D image. However, this approach only worked for certain robots made by Universal Robots (UR). In this work, we expand our method to include a new robot arm, the Kuka LBR iiwa, which has a noticeably different appearance and an additional joint. Instead of collecting large datasets again, we gather several smaller datasets containing a few hundred frames each, and we apply transfer learning techniques on the CNN trained on UR robots to adapt it to the new robot with different shapes and visual features. We have demonstrated that transfer learning is not only applicable in this field, but it also requires smaller, well-prepared training datasets, trains significantly faster, and achieves similar accuracy compared to the original method, even improving in some aspects.",
        "reference": "Limited data available for training affects the use of deep learning techniques. Datasets are available for problems such as item recognition and classification for self-driving cars, but very limited for industrial robotics. A multi-objective Convolutional Neural Network (CNN) was first trained to identify the robot in the image and find 3D positions of the joints using just a 2D image. But this only used robots made by Universal Robots (UR). Here we work with a new robot arm - Kuka LBR iiwa which looks very different and has an additional joint. We collect a number of smaller datasets and use transfer learning techniques on the CNN trained on UR robots to adapt it to a robot that looks different. Transfer learning requires smaller training datasets, learns faster and reaches similar or even better accuracy.",
        "metrics": {
          "SARI": 40.49866111569746,
          "BLEU": 0.24190950243435777,
          "FKGL": 14.6
        }
      },
      {
        "source": "We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a ResNet-18 variant, which is fed in images from the front of a simulated F1 car, and outputs optimal labels for steering, throttle, braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.",
        "prediction": "We introduce Ignition an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a type of ResNet-18 that takes images from the front of a simulated F1 car and provides the best recommendations for steering acceleration and braking. Importantly we never explicitly train the model to detect road features such as the outlines of a track or the distance to other cars instead we show that these hidden features can be automatically captured by the network",
        "reference": "Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments. This ResNet-18 variant model receives images from the front of a simulated car and uses them to steer, throttle, or brake appropriately. The model is not explicitly trained to detect road features such as track outline or distance to other cars, as these features can automatically be calculated by the network.",
        "metrics": {
          "SARI": 53.06358316524087,
          "BLEU": 0.3188254780084236,
          "FKGL": 16.3
        }
      },
      {
        "source": "As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) system. When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. Then, the second DNN for primary acoustic modeling employs both the bottleneck features tossed from the first DNN and the acoustics features. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "prediction": "As for the humanoid robots, internal noise from motors, fans, and moving parts reduces how accurately they can recognize speech. In this paper, we propose a novel speech recognition system that is robust to this ego-noise for humanoid robots, using the on/off state of the motor as additional information to identify relevant input features. We consider bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) systems. To learn these bottleneck features, we first use the motor on/off state data as supplementary information alongside acoustic features for the input of the first DNN, which is for preliminary acoustic modeling. Next, the second DNN for primary acoustic modeling uses both the bottleneck features from the first DNN and the acoustic features. When we evaluate the proposed method in terms of phoneme error rate (PER) on the TIMIT database, the experimental results show an obvious improvement of 11% relative to conventional systems achieved by our algorithm.",
        "reference": "Internal noise generated by motors, fans and mechanical components severely decreases speech recognition accuracy, when the humanoid robot is moving or shaking its body. This paper presents a novel speech recognition system robust to ego-noise for humanoid robots. On/off state of the motor is used to find relevant input features. Bottleneck features have been successfully applied to automatic speech recognition (ASR) systems based on deep neural networks (DNN). The motor on/off state data and acoustic features are used as the input of the first deep neural network (DNN) for preliminary acoustic modeling. The second DNN for primary acoustic modeling uses both bottleneck features from the first DNN and acoustic ones. The proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, experimental results show that improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "metrics": {
          "SARI": 44.32575457193471,
          "BLEU": 0.37518637934290483,
          "FKGL": 14.9
        }
      },
      {
        "source": "Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model is made of residual convolutional blocks with hierarchical dilated skip connections joined in steps. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.",
        "prediction": "Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening worldwide. With the arrival of self-driving cars, solving the automatic recognition of traffic and hand-held signs on major streets has become a key challenge. Different methods in machine learning, such as Random Forest, SVM, and deep learning models, have been suggested for recognizing traffic signs. Although these methods achieve state-of-the-art performance on specific datasets, they fall short of tackling multiple traffic sign recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that excels across various benchmarks, achieving a better overall score than current state-of-the-art architectures. Our model consists of residual convolutional blocks with hierarchical dilated skip connections. With this, we score 99.33% accuracy in the German sign recognition benchmark and 99.17% accuracy in the Belgian traffic sign classification benchmark. Moreover, we introduce a newly devised dilated residual learning representation technique that is very low in both memory and computational complexity.",
        "reference": "If intelligent systems recognize traffic signs, it can reduce car accidents. For self-driving cars, we need automatic recognition of traffic and hand signs. Machine learning techniques  could be used to classify traffic signs. They perform well on a particular dataset, but fail on multiple traffic sign recognition test datasets. We proposed a new and better method. This model is based on enhanced convolutional neural networks (a deep-learning approach often used for image recognition). This model was more than 99% correct on German and Belgian sign recognition datasets. This new model is very low in both memory and computational complexity.",
        "metrics": {
          "SARI": 41.02702335091333,
          "BLEU": 0.12154141232115143,
          "FKGL": 13
        }
      },
      {
        "source": "In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",
        "prediction": "In this paper, we present a transfer learning method for the end-to-end control of self-driving cars. This method allows a convolutional neural network (CNN) trained on one domain to be used for the same task in a different domain. A regular CNN for controlling self-driving cars is made to turn an image from a front-facing camera into a steering command. To enable transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module that takes the output of the last convolutional layer as input. \n\nThe CNN trained on the source domain, called the source network, is then used to train another task module called the target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from both the source and target networks are finally merged based on the LDL, and this merged command is then used to control a car in the target domain. \n\nTo demonstrate the effectiveness of the proposed method, we used two simulators, TORCS and GTAV, for the source and target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",
        "reference": "A transfer learning method to control self-driving cars is proposed, in which a convolutional neural network (CNN) is trained on one domain then used for the same task in a different domain. A conventional CNN is designed to map a single front-facing camera image to control a car. CNN produces a steering command and a lane departure level (LDL, mechanism designed to warn the driver when the vehicle begins to move out of its lane). To enable the transfer learning, a new task module takes the output of the last convolutional layer as input. The CNN trained on the source domain is then used to train the target network. It takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are merged and used for car control in the target domain. Two simulators were used to evaluate the proposed method. The proposed method is more stable and safer for car control.",
        "metrics": {
          "SARI": 44.216928336698096,
          "BLEU": 0.3996626756299061,
          "FKGL": 12.9
        }
      },
      {
        "source": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market, based on a Limit Order Book (LOB) such as is common in major stock exchanges. Construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Research aimed at understanding the dynamics of this new style of financial market is hampered by the fact that no operational real-world exchange is ever likely to allow experimental probing of that market while it is open and running live, forcing researchers to work primarily from time-series of past trading data. Similarly, university-level education of the engineers who can create next-generation automated trading systems requires that they have hands-on learning experience in a sufficiently realistic teaching environment. BSE as described here addresses both those needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHuB.",
        "prediction": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market based on a Limit Order Book (LOB), common in major stock exchanges. The construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading tasks that were once the responsibility of human traders now being performed by high-speed autonomous trading systems. \n\nHowever, research to understand this new type of financial market is challenging because real stock exchanges do not allow experiments while they are active. As a result, researchers mostly rely on data from past trading activity. Similarly, university-level education for engineers who will create next-generation automated trading systems requires hands-on learning experiences in a realistic environment. \n\nThe BSE addresses both of these needs: it has been successfully used for teaching and research at a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHub.",
        "reference": "This paper describes the Bristol Stock Exchange (BSE), a minimal simulation of a centralised financial market based on a Limit Order Book (LOB), a type of order to buy or sell a security at a specific price or better. BSE was built because most of the financial markets became automated. Research tries to understand the dynamics of this automated financial markets but Trainee engineers also need a realistic learning experience to build automated trading systems. BSE has been successfully used for both teaching and research since 2012. BSE code is freely available on GitHuB.",
        "metrics": {
          "SARI": 48.87301455744459,
          "BLEU": 0.22686649963147007,
          "FKGL": 16.4
        }
      },
      {
        "source": "Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict the probabilities of contents to be requested in specific areas. To reduce the car-DC delay, MLP outputs are logged into MEC servers attached to roadside units. Second, in order to cache entertainment contents stylized for car passengers’ features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.",
        "prediction": "Once self-driving cars become a reality and passengers are no longer worried about them, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents that need to be cached in the areas of the self-driving car is achieved at 98.04%, and our approach can minimize delay. However, retrieving entertainment contents at the Data Center can hinder content delivery service due to the high delay of car-to-Data Center communication. To address these challenges, we propose a deep learning based caching solution for self-driving cars, using approaches deployed on the Multi-access Edge Computing structure.\n\nFirst, at the Data Center, a Multi-Layer Perceptron is used to predict the probabilities of contents that will be requested in specific areas. To reduce the car-Data Center delay, the Multi-Layer Perceptron outputs are logged into MEC servers attached to roadside units. Second, to cache entertainment contents tailored for car passengers' features such as age and gender, we use a Convolutional Neural Network to predict the age and gender of passengers. This allows the self-driving car to identify the contents that need to be downloaded from the MEC server and cached. Finally, to solve the problem, we use a method called Block Successive Majorization-Minimization.\n\nThis structured approach enhances entertainment services for passengers by minimizing content downloading delays.",
        "reference": "Once self-driving cars become reality and passengers no longer worry about it, they will need to find other forms of entertainment. Prediction accuracy was 98.04% for contents to be cached for self-driving cars. Such an approach can minimize delay. However, high communication delay can slow retrieval of entertainment content from Data Centers (DC). Self-driving cars can use deep learning caching to address these challenges. It is deployed on the Multi-access Edge Computing (MEC) structure that moves services from a centralized cloud to the network edges and closer to the customer. Multi-Layer Perceptron (MLP), a type of artificial neural network, is used to predict the content requested in specific areas. To reduce delay, MLP outputs are sent to MEC servers for roadside units. A Convolutional Neural Network (CNN) is used to predict passengers’ age and gender to cache adapted entertainment content. The self-driving car can therefore identify the contents to be downloaded and cached. Deep learning caching is formulated for the self-driving car to enhance entertainment services and minimize content downloading delay. A Block Successive Majorization-Minimization (BS-MM) technique solves the problem.",
        "metrics": {
          "SARI": 38.47416804063586,
          "BLEU": 0.21100702936107207,
          "FKGL": 12.3
        }
      },
      {
        "source": "Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges.",
        "prediction": "Moral responsibility is a major concern in using technology to make decisions especially in areas like self-driving cars and kidney swapping",
        "reference": "Moral responsibility is a major concern in automated decision-making, from self-driving cars to kidney transplants.",
        "metrics": {
          "SARI": 40.4457502374169,
          "BLEU": 0.32178169209299734,
          "FKGL": 16.2
        }
      },
      {
        "source": "A key problem for autonomous car navigation is the understanding, at an object level, of the current driving situation. Addressing this issue requires the extraction of meaningful information from on-board stereo imagery by classifying the fundamental elements of urban scenes into semantic categories that can more easily be interpreted and be reflected upon (streets, buildings, pedestrians, vehicles, signs, etc.). A probabilistic method is proposed to fuse a coarse prior 3D map data with stereo imagery classification. A novel fusion architecture based on the Stixel framework is presented for combining semantic pixel-wise segmentation from a convolutional neural network (CNN) with depth information obtained from stereo imagery while integrating coarse prior depth and label information. The proposed approach was tested on a manually labeled data set in urban environments. The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to what is obtained from the semantic pixel-wise segmentation of a CNN alone.",
        "prediction": "A key problem for autonomous car navigation is the understanding at an object level of the current driving situation Addressing this issue requires the extraction of meaningful information from on-board stereo imagery by classifying the fundamental elements of urban scenes into semantic categories that can more easily be interpreted streets buildings pedestrians vehicles signs etc A method is suggested to combine a rough 3D map with the classification of stereo images A novel fusion architecture based on the Stixel framework is presented for integrating semantic pixel-wise segmentation from a convolutional neural network CNN with depth information obtained from stereo imagery while still utilizing coarse prior depth and label information The proposed approach was tested on a manually labeled data set in urban environments The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to what is obtained from the semantic pixel-wise segmentation of a CNN alone",
        "reference": "A key problem for autonomous car navigation is the understanding of the environment while driving. It needs to extract information from the sensors and cameras and classify the elements into categories so it can be easily analysed (streets, buildings, pedestrians, vehicles, signs, etc.). A method is to fuse 3D map data with classification of the sensors and cameras. This system combine image segmentation (simplify the representation of an image to facilitate its analysis) and information obtained from sensors and cameras while integrating prior information. This approach was tested on manually entered data in urban environments. The results show that the classification of elements is more accurate with this method than the image segmentation alone.",
        "metrics": {
          "SARI": 33.20162768135842,
          "BLEU": 0.16671205423790827,
          "FKGL": 68.5
        }
      },
      {
        "source": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to the development of self-driving systems is the utilization of ROS which is an open-source middleware framework used in the development of robot applications. On the other hand, the popular approach in the automotive industry is the utilization of MATLAB/Simulink which is software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables to create functionalities of ROS-based robots in MATLAB/Simulink. However, it is not been fully utilized in the development of self-driving systems yet because there are not enough samples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.",
        "prediction": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on the Robot Operating System (ROS). One approach to developing self-driving systems is utilizing ROS, which is an open-source middleware framework for robot applications. In contrast, the automotive industry commonly uses MATLAB/Simulink, software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface that allows the creation of functionalities for ROS-based robots. However, it has not been fully used in creating self-driving systems yet because there aren't enough self-driving samples, and developers find it hard to collaborate. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is a popular open-source software that offers a complete set of self-driving modules. The provided benchmark includes MATLAB/Simulink samples available in Autoware that aid in designing ROS-based self-driving systems using MATLAB/Simulink.",
        "reference": "This paper proposes MATLAB/Simulink tests for an open-source self-driving system based on Robot Operating System. One approach to the development of self-driving systems is the utilization of Robot Operating System (open-source software framework used in the development of robot applications). But the popular approach in the automotive industry is the utilization of MATLAB/Simulink (software for modeling, simulating, and analyzing). MATLAB/Simulink provides an interface that enables to create functionalities of robots (based on robot operating system). However, it is not fully utilized in the development of self-driving systems yet because there are not enough samples, and it is difficult for developers to adopt co-development. We provide MATLAB/Simulink tests for a self-driving system (based on robot operating system) called Autoware. Autoware is a popular open-source software that provides a complete set of self-driving modules. The provided tests contain MATLAB/Simulink samples available in Autoware. They help to design self-driving systems (based on robot operating system) using MATLAB/Simulink.",
        "metrics": {
          "SARI": 33.9113518962673,
          "BLEU": 0.3570077558286142,
          "FKGL": 13.4
        }
      },
      {
        "source": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. One approach to the development of self-driving systems is the use of ROS; however, the system used in the automotive industry is typically designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with ROS-based systems. To allow the two to be used in tandem, it is necessary to rewrite the C++ code and incorporate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, thereby improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "prediction": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. One way to develop self-driving systems is by using ROS. However, most systems in the automotive industry are designed with MATLAB/Simulink, which is useful for creating and testing self-driving models. Unfortunately, these models are not compatible with ROS-based systems. To use both together, it is necessary to rewrite the C++ code and integrate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "reference": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. Self-driving systems use Robot Operating System (open-source software framework used in the development of robot applications). However, in the automotive industry, the system is designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with systems based on Robot Operating System. To use both, it is necessary to incorporate new code into the system based on Robot Operating System, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a self-driving system, which improve development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "metrics": {
          "SARI": 46.054697103403605,
          "BLEU": 0.5128981302553065,
          "FKGL": 12.9
        }
      },
      {
        "source": "Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common, but costly and risky method, which has already caused fatalities. As a safer alternative, virtual tests, in which self-driving car software is tested in computer simulations, have been proposed. One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for systematically testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car software, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "prediction": "Ensuring the safety of self-driving cars is important but neither industry nor authorities have settled on a standard way to test them Deploying self-driving cars for testing in regular traffic is a common but costly and risky method which has already caused fatalities As a safer alternative virtual tests in which self-driving car software is tested in computer simulations have been proposed One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests To address this challenge we created AsFault a tool that automatically generates virtual tests to systematically check how self-driving car software works We demonstrate AsFault by testing the lane-keeping feature of an artificial intelligence-based self-driving car software for which AsFault generates scenarios that cause it to drive off the road A video illustrating AsFault in action is available at httpsyoutu belJ1sa42VLDw",
        "reference": "Ensuring the safety of self-driving cars is important, but there is no standard way to test them. Testing self-driving cars in regular traffic is costly and risky, and has already caused fatalities. As a safer alternative, virtual tests (self-driving car software tested in computer simulations) have been proposed. To cover the huge number of possible driving situations, self-driving cars must be tested in manually created tests. We developed AsFault, a tool for automatically generating virtual tests for testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of a self-driving car software with scenarios going off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "metrics": {
          "SARI": 29.798523152436452,
          "BLEU": 0.3593312775904907,
          "FKGL": 63
        }
      },
      {
        "source": "With the developing of the technology on self-driving, more and more L3 driverless vehicles are launched in market, people get opportunities to experience the self-driving cars in their daily life. As a result, there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can not only improve the driving experience, but also accelerate the process of self-driving commercialization. This paper discusses eye-movement interaction, voice interaction and gesture interaction in self-driving car, and analyzes the technology, advantages and disadvantages of the existing interaction modes, and prospect the future development trend of self-driving human-computer interaction.",
        "prediction": "With the development of self-driving technology more and more L3 driverless vehicles are being launched in the market giving people the opportunity to experience self-driving cars in their daily lives As a result there is a growing demand for the autopilot experience Good human-computer interaction can enhance the driving experience and speed up the development of self-driving cars This paper discusses eye movement interaction voice interaction and gesture interaction in self-driving cars analyzing the technology advantages and disadvantages of existing interaction modes while also exploring future trends in self-driving human-computer interaction",
        "reference": "Self-driving technology is developing, more and more driverless vehicles are launched in market and accessible for people in their daily life. So there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can improve the driving experience and accelerate the process of self-driving commercialization. This paper discusses interactions with the eye, voice and gesture in self-driving car, analyzes the technology, the interaction modes, and considers the future of self-driving human-computer interaction.",
        "metrics": {
          "SARI": 37.300710493959684,
          "BLEU": 0.22797076977187,
          "FKGL": 43.5
        }
      },
      {
        "source": "In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on highfidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The state of the art in driving scenario generation, as adopted by some of the front-runners of the self-driving car industry, still relies on human input [1]. In this paper we propose to automate the process using Bayesian optimization to generate adversarial self-driving scenarios that expose poorly-engineered or poorly-trained self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy using vision-based imitation learning we obtain safer self-driving behavior.",
        "prediction": "In recent years, self-driving vehicles have become more commonplace on public roads, promising to bring safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on high-fidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is critical to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The latest methods for creating driving scenarios used by leading self-driving car companies still depend on human involvement. In this paper, we propose automating this process using Bayesian optimization to generate challenging self-driving scenarios that reveal weaknesses in poorly-engineered or poorly-trained self-driving policies, increasing the risk of collisions with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy and fine-tuning the policy using vision-based imitation learning, we can achieve safer self-driving behavior.",
        "reference": "Self-driving vehicles have become common on public roads, with the promise of a safe and efficient mode of transport. To be reliable, these vehicles need many software tests on simulators with interactions of multiple vehicles and pedestrians. It is essential that self-driving software is evaluated with many different challenging driving scenarios. The driving scenario generation still needs human contribution. We propose to automate the process and generate conflicting self-driving that expose weaknesses of self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. By incorporating the generated scenarios into the parameters of the self-driving policy, and by adjusting the policy with the imitation learning, we obtain safer self-driving behavior.",
        "metrics": {
          "SARI": 37.70808931711863,
          "BLEU": 0.20256654046840566,
          "FKGL": 18.1
        }
      },
      {
        "source": "At the brink of the introduction of self-driving vehicles, only little is known about how potential users perceive them. This is especially true for self-driving vehicles deployed in public transport services. In this study, the relative preferences for a trip with a self-driving bus is assessed compared to a trip with a regular bus, based on a stated preference experiment. Based on the responses of 282 respondents from the Netherlands and Germany, a discrete choice model is estimated as a Mixed Logit model including attitudes towards trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. This is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. Findings from this study further suggest that the popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences towards automated public transport services are expected to evolve along with the transition from demonstration pilots to their deployment in regular operations.",
        "prediction": "At the brink of the introduction of self-driving vehicles, little is known about how potential users perceive them, especially regarding self-driving vehicles in public transport services. This study assesses the relative preferences for a trip with a self-driving bus compared to a regular bus, based on a stated preference experiment. Using answers from 282 people in the Netherlands and Germany, we created a model called a Mixed Logit model. This model examines how trust in self-driving vehicles and interest in technology affect people's choices. \n\nThe results show that public transport passengers currently prefer the self-driving bus over the regular bus, but only for short trips. This preference is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus during short commuting trips. Furthermore, findings suggest that the popularity of self-driving buses decreases when a human steward is on board or when they are operated as a demand-responsive service with fixed routes. \n\nAdditionally, people who show a strong interest in technology or trust in automated vehicle technology view the self-driving buses more positively than others. Preferences for automated public transport services are expected to evolve as the transition progresses from demonstration pilots to regular operations.",
        "reference": "Little is known about how potential users perceive self-driving vehicles. This is especially true for self-driving vehicles in public transport services. In this study, the preferences between a trip in a self-driving bus and a regular bus were compared. The results of 282 participants were studied, including trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. The preference is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. The popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences for automated public transport services are expected to evolve with their deployment in regular operations.",
        "metrics": {
          "SARI": 39.17298309306467,
          "BLEU": 0.4028649515751537,
          "FKGL": 14.7
        }
      },
      {
        "source": "In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. The fundaments of human dignity, autonomous decision-making, and democracies are shaking, but I believe that they need to be vigorously defended, as they are not only core principles of livable societies, but also the basis of greater efficiency and success.",
        "prediction": "In the wake of the ongoing digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. We need to choose between two types of societies: one where decisions are made by those in power and imposed on people using tactics like targeted ads and nudging, or another where decisions are made together by everyone in a free and cooperative way. Modern information and communication systems ICT enable both options, but the latter offers economic and strategic benefits. The foundations of human dignity, autonomous decision-making, and democracies are at risk, but I believe they need to be vigorously defended, as they are not only core principles of livable societies but also the basis for greater efficiency and success.",
        "reference": "The on-going digital revolution will dramatically transform our economy and societal institutions. But this also presents risks for our society. Automation of society is coming. We must choose either between a society in which the actions are determined top-down by coercion or manipulative technologies (such as personalized ads) or in which decisions are taken freely and cooperatively. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. Foundations of human dignity, autonomous decision-making, and democracies are weakening. These core principles of society and the basis of greater efficiency and success need to be defended.",
        "metrics": {
          "SARI": 34.49389183811718,
          "BLEU": 0.18871886392306053,
          "FKGL": 16.1
        }
      },
      {
        "source": "This paper addresses the financial markets problem space with particular emphasis on trading systems seen in banking today. With the advent of modern computing, cross-regional trading transactions can now be executed within milliseconds, a reality that is impossible without the advancements of software systems technology. We then move on and discuss the anatomy of a trading system and how it fits in with the banks ecosystem of vital inter-working components.",
        "prediction": "This paper addresses the financial markets problem space with a particular emphasis on trading systems seen in banking today. With modern computing advancements, trading transactions across different regions can now happen in just milliseconds, thanks to improvements in software technology. This ability highlights the importance of software advancements for efficient trading. We then move on to discuss the anatomy of a trading system and how it fits into the bank's ecosystem of vital inter-working components.",
        "reference": "This paper addresses problems of trading systems in banking. Modern computing allows cross-regional trading transactions within milliseconds. We then discuss how a trading system fits in with the bank ecosystem.",
        "metrics": {
          "SARI": 37.468886183703766,
          "BLEU": 0,
          "FKGL": 13
        }
      },
      {
        "source": "In this paper we present queueing-theoretical methods for the modeling, analysis, and control of autonomous mobility-on-demand MOD systems wherein robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first cast an autonomous MOD system within a closed Jackson network model with passenger loss. The theoretical insights are used to design a robust, real-time rebalancing algorithm, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study of New York shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles roughly 70% of the size of the current taxi fleet operating in Manhattan. Finally, we extend our queueing-theoretical setup to include congestion effects, and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to the problem of system-wide coordination of autonomously driving vehicles, and provides one of the first characterizations of the sustainability benefits of robotic transportation networks.",
        "prediction": "In this paper, we present queueing-theoretical methods for modeling, analyzing, and controlling autonomous mobility-on-demand (MOD) systems, where robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first describe an autonomous MOD system using a model that represents a network where customers can leave without being served. The theoretical insights from this model are used to design a robust, real-time rebalancing algorithm, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles, roughly 70% of the size of the current taxi fleet operating in the area. Finally, we extend our queueing-theoretical setup to include congestion effects and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to the problem of system-wide coordination of autonomously driving vehicles and presents one of the first characterizations of the sustainability benefits of robotic transportation networks.",
        "reference": "Queueing models are used for autonomous mobility-on-demand MOD systems. A queueing model is constructed so that queue lengths and waiting time can be predicted. In MOD systems,  robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure quality of service. We first cast an autonomous MOD system within a closed Jackson network model,a class of queueing network, with passenger loss. A real-time rebalancing algorithm is applied to a case study of New York City. It is implemented on an eight-vehicle mobile robot testbed. The current taxi demand in Manhattan can be met with about 8,000 robotic vehicles. It represents roughly 70% of the size of the current taxi fleet in Manhattan. Congestion effects are also included to study the impact of autonomously rebalancing vehicles on overall traffic jams. This paper describes the sustainability benefits of robotic transportation networks.",
        "metrics": {
          "SARI": 40.860009865153174,
          "BLEU": 0.3885438987260603,
          "FKGL": 18.5
        }
      },
      {
        "source": "Many model based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. However sensor based robot control overcomes many of the difficulties of uncertain models and unknown environments which limit the domain of application of the previous methods. Furthermore, for service and manipulation tasks, it is more suitable to study the interaction between the robot and its environment at the contact point using the sensor based control, rather than specifying the joint positions and velocities required to achieve them.",
        "prediction": "Many model-based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots such as teleoperation learning from demonstration and imitation However robot control using sensors solves many problems caused by uncertain models and unknown environments that limit the use of previous methods Furthermore for service and manipulation tasks it is more suitable to study the interaction between the robot and its environment at the contact point with sensor-based control rather than specifying the joint positions and velocities needed to achieve those tasks",
        "reference": "Model based techniques exist to apply domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. Sensor based robot control overcomes difficulties of uncertain models and unknown environments. Interaction between the robot and its environment using the sensor based control is important.",
        "metrics": {
          "SARI": 36.409923841631795,
          "BLEU": 0.21160132790981082,
          "FKGL": 39.6
        }
      },
      {
        "source": "Inaccurate information, in the field of library and information science, is often regarded as a problem that needs to be corrected or simply understood as either misinformation or disinformation without further consideration. Misinformation and disinformation, however, may cause significant problems for users in online environments, where they are constantly exposed to an abundance of inaccurate and/or misleading information. This paper aims to establish conceptual groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of cues to deception, as means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related yet distinct sub-categories of information. Misinformation is a multifaceted concept, more complex than simply being inaccurate or incomplete, and disinformation does not always entail misinformation.",
        "prediction": "Inaccurate information, in the field of library and information science, is often seen as a problem that needs correction or can be understood as either misinformation or disinformation without much consideration. Misinformation and disinformation, however, may create significant issues for users in online environments, where they are frequently exposed to a large amount of inaccurate and misleading information. This paper aims to lay the groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis includes a discussion about signs that can help identify misinformation and disinformation. We argue that misinformation and disinformation are related but distinct subcategories of information. Misinformation is a complex idea that involves more than just being wrong or missing information, and not all disinformation falls under misinformation.",
        "reference": "Inaccurate information is often regarded as a problem that needs to be corrected or simply understood as misinformation or disinformation without further consideration. Misinformation and disinformation may cause problems online because users are constantly exposed to inaccurate and/or false information. This paper aims to establish preliminary work for future research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related but distinct sub-categories of information. Misinformation is more complex than simply being inaccurate or incomplete, and disinformation does not always involve misinformation.",
        "metrics": {
          "SARI": 35.08241509227055,
          "BLEU": 0.36260329908031935,
          "FKGL": 17.5
        }
      },
      {
        "source": "The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.",
        "prediction": "The growth of social media provides a convenient communication scheme for people, but at the same time it has become a hotbed of misinformation. The widespread misinformation on social media is harmful to the public interest. We have designed a framework that combines collective intelligence and machine intelligence to help identify misinformation. The main idea is: (1) automatically gather information about what users know based on their microblog posts; and (2) connect these knowledgeable users with suspected misinformation. By sending the suspected misinformation to the appropriate experts, we can collect their assessments to judge the credibility of the information and help refute misinformation. In this paper, we focus on finding experts for misinformation identification. We propose a tag-based method to index the expertise of microblog users using social tags. Experiments on a real-world dataset demonstrate the effectiveness of our method for expert finding in relation to misinformation identification in microblogs.",
        "reference": "The growth of social media leads to better communication between people, but also propagation of misinformation. The wide spread of misinformation over social media have bad consequences for public interest. We design a framework to help identify misinformation. The idea is to index the expertise of users and to match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, they can judge the credibility of information, and help refute misinformation. In this paper, we focus on finding appropriate experts for misinformation identification. We propose a method to index the expertise of users with tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding for misinformation identification.",
        "metrics": {
          "SARI": 40.0960339731184,
          "BLEU": 0.3855651523877393,
          "FKGL": 14.2
        }
      },
      {
        "source": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation in support of such research. This paper aims to address the aforementioned challenges by conceptualizing misinformation and enabling the interoperability of misinformation. In particular, a representation and a model of misinformation are proposed through surveying, synthesizing, and explicating existing work in the field. The ontology-supported misinformation model can not only guide future misinformation research but also lay the foundation for building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a formal methodology for managing misinformation in a digital library, and suggest future research directions related to the misinformation model.",
        "prediction": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation to support such research. This paper aims to address these challenges by conceptualizing misinformation and enabling its interoperability. In particular, we propose a representation and a model of misinformation through surveying, synthesizing, and explaining existing work in the field. The misinformation model can help future research on misinformation and create a digital library for it. This model will enhance our understanding of misinformation and improve how we share, manage, and reuse it. Additionally, we present a formal methodology for managing misinformation in a digital library and suggest future research directions related to the misinformation model.",
        "reference": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the rare misinformation data. This paper aims to address these challenges by conceptualizing misinformation and being able to completely understand and work on misinformation. A representation and a model of misinformation are proposed through existing work in the field. The model can guide future misinformation research and help building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a methodology for managing misinformation in a digital library, and suggest future research directions.",
        "metrics": {
          "SARI": 43.634395511341175,
          "BLEU": 0.48700993545787374,
          "FKGL": 14.9
        }
      },
      {
        "source": "Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to the event. We define as misinformation any false or inaccurate information that is spread either intentionally or unintentionally. In this paper we study the problem of misinformation identification in OSNs, and we focus in particular on the Twitter social network. Based on user and tweets characteristics, we build a misinformation detection model that identifies suspicious behavioral patterns and exploits supervised learning techniques to detect misinformation. Our extensive experimental results on 80294 unique tweets and 59660 users illustrate that our approach effectively identifies misinformation during emergencies. Furthermore, our model manages to timely identify misinformation, a feature that can be used to limit the spread of the misinformation.",
        "prediction": "Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to these events. We define misinformation as any false or inaccurate information that is spread either intentionally or unintentionally. In this paper we study the problem of identifying misinformation in OSNs focusing specifically on the Twitter social network. We create a model to detect misinformation by examining the characteristics of users and tweets. This model finds suspicious behavior patterns and uses supervised learning methods. Our extensive experimental results on 80294 unique tweets and 59660 users show that our approach effectively identifies misinformation during emergencies. Furthermore our model can timely identify misinformation a feature that helps limit its spread.",
        "reference": "Social media have become popular means of information sharing. The spread of news regarding emergency events is common in social media but so is the spread of misinformation. Misinformation is any false or inaccurate information that is spread intentionally or not. In this paper we study the problem of misinformation identification in social media, and we focus in particular on Twitter. We build a misinformation detection model that identifies suspicious behavioral patterns and detect misinformation. We found 80294 unique tweets and 59660 users. It illustrates that our approach identifies misinformation during emergencies. Our model manages to timely identify misinformation, it can be used to limit the spread of the misinformation.",
        "metrics": {
          "SARI": 34.86799023807484,
          "BLEU": 0.3543793992604385,
          "FKGL": 13.2
        }
      },
      {
        "source": "The advent of WWW changed the way we can produce and access information. Recent studies showed that users tend to select information that is consistent with their system of beliefs, forming polarized groups of like-minded people around shared narratives where dissenting information is ignored. In this environment, users cooperate to frame and reinforce their shared narrative making any attempt at debunking inefficient. Such a configuration occurs even in the consumption of news online, and considering that 63% of users access news directly form social media, one hypothesis is that more polarization allows for further spreading of misinformation. Along this path, we focus on the polarization of users around news outlets on Facebook in different European countries (Italy, France, Spain and Germany). First, we compare the page posting behavior and the users interacting patterns across countries and observe different posting, liking and commenting rates. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities generated around specific pages.",
        "prediction": "The advent of the WWW changed the way we produce and access information. Recent studies found that users choose information that agrees with their beliefs, creating groups of similar people around shared stories while ignoring opposing information. In this environment, users work together to shape and reinforce their shared narrative, making any attempts to argue against it ineffective. This situation also occurs in how news is consumed online. Considering that 63% of users access news directly from social media, one hypothesis is that increased polarization allows for a greater spread of misinformation. Along this path, we focus on the polarization of users around news outlets on Facebook in different European countries (Italy, France, Spain, and Germany). First, we compare the page posting behavior and users' interaction patterns across countries, observing different rates of posting, liking, and commenting. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities formed around specific pages.",
        "reference": "The WWW changed the ways we can produce and access information. Users tend to select information that agrees with their beliefs. They form polarized groups of like-minded people sharing narratives that ignore contradicting information. Here, users reinforce their shared narratives and can resist debunking This is the case with online news. Because 63% of users access news from social media, polarization could increase misinfomation spread. We focus on polarisation of Facebook news consumers in different European countries. Posting, liking and commenting rates differ across countries. We look at how users interact with different pages and polarized communities form around specific pages.",
        "metrics": {
          "SARI": 41.88289145311453,
          "BLEU": 0.15530342401166622,
          "FKGL": 11.5
        }
      },
      {
        "source": "We find that Italy is the most polarized country, followed by France, Germany and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the usersu0027 engagement and trust on the news. Our findings suggest that trust in information broadcaster plays a pivotal role against polarization of users online.",
        "prediction": "We find that Italy is the most polarized country, followed by France, Germany, and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the users' engagement and trust on the news. Our findings suggest that trust in information broadcasters plays a pivotal role against polarization of users online.",
        "reference": "Italy is the most polarized country, then France, Germany and Spain. A variant of the Bounded Confidence Model simulates the emergence of these communities by considering user engagement and trust in the news. Trust in an information source plays a major role against polarization of users online.",
        "metrics": {
          "SARI": 50.11563108643106,
          "BLEU": 0.37980577279975775,
          "FKGL": 11.1
        }
      },
      {
        "source": "Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative strategies to contrast misinformation. Simultaneously, they allow the spread of misinformation by empowering individuals to self-select the narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. A precise theoretical understanding of such trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs unbiasedly based on the arrival of new information, while a fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. We show that this simple confirmation bias mechanism can generate permanent opinion polarisation. Furthermore, the model results in states where unbiased agents behave \"as if\" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. We derive analytic results for the distribution of individual agents’ beliefs, explicitly demonstrating the aforementioned trade-off between confirmation bias and social connectivity, which we further validate against US county-level data on the impact of Internet access on the formation of beliefs about global warming. Our findings indicate that confirmation bias in small doses may actually result in improved accuracy across individuals by preserving information diversity in a social network. However, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups.",
        "prediction": "Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative approaches to counter misinformation. At the same time, social media allow the spread of misinformation by empowering individuals to self-select the narratives they want to see, through both active methods (like confirmation bias) and passive methods (such as personalized news algorithms). A precise theoretical understanding of these trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs without bias based on the arrival of new information, while a fraction of participants display confirmation bias, which leads them to reject news that does not align with their existing beliefs. We demonstrate that this straightforward confirmation bias can lead to lasting differences in opinions. Furthermore, the model produces situations where unbiased agents act “as if” they were biased because their biased neighbors serve as gatekeepers, limiting their access to free and diverse information. We present analytical results on how individual agents’ beliefs are distributed, clearly illustrating the balance between confirmation bias and social connections. We further validate these results against US county-level data on the impact of internet access on the formation of beliefs about global warming. Our findings indicate that confirmation bias in small doses may actually lead to improved accuracy among individuals by preserving information diversity within a social network. However, when confirmation bias exceeds an optimal level, accuracy declines as biased agents restrict information flow to subgroups.",
        "reference": "Social media give users opportunities to interact with different opinions. We discuss the meaning of our model for policy and the downsides of myth-busting We suggest alternatives to fight misinformation. But misinformation is spread via social media because individuals can search for information that confirms their beliefs and personalized news algorithms may supply it. Theoretical understanding of the balance between opportunities and misinformation is incomplete. We introduce a stylized social learning model. In this model, most participants in a network update their beliefs based on new information, while others show confirmation bias and can reject news that differs from their beliefs. This simple confirmation bias mechanism can create lasting opinion polarisation. Besides, in the model, unbiased people behave “as if” biased because biased neighbours limit their access to diverse information. With our model, we show the trade-off between confirmation bias and social connectivity. We validated the model using the data on how internet access influences beliefs on global warming. Small confirmation bias may even improve accuracy across individuals by keeping information diversity in a social network. But large confirmation bias lowers accuracy as biased individuals limit information flow to others.",
        "metrics": {
          "SARI": 43.18036392022939,
          "BLEU": 0.16227957032158713,
          "FKGL": 16
        }
      },
      {
        "source": "Superintelligence is a potential type of future artificial intelligence (AI) that is significantly more intelligent than humans in all major respects. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. Meanwhile, the prospect of superintelligence is the subject of major ongoing debate, which includes a significant amount of misinformation. Superintelligence misinformation is potentially dangerous, ultimately leading bad decisions by the would-be developers of superintelligence and those who influence them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: strategies to prevent the spread of superintelligence misinformation and strategies to correct it after it has spread. In general, misinformation can be difficult to correct, suggesting a high value of strategies to prevent it. The strategies proposed can be applied to lay public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "prediction": "Superintelligence is a potential type of future artificial intelligence (AI) that is significantly more intelligent than humans in all major respects. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. The idea of superintelligence is being widely discussed, and there is a lot of false information about it. This misinformation can be dangerous, leading to poor decisions by those who would develop superintelligence and those who advise them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: strategies to prevent the spread of misinformation and strategies to correct it after it has spread. In general, misinformation can be difficult to correct, suggesting a high value of prevention strategies. The proposed strategies can be applied to engage the public about superintelligence, develop AI education programs, and build expert consensus.",
        "reference": "Superintelligence is a potential future artificial intelligence (AI) significantly more intelligent than humans. Superintelligence could be a major event, with potential consequences that are beneficial or catastrophic. The prospect of superintelligence is the subject of major debate, which includes a lot of misinformation. Superintelligence misinformation could be dangerous, and lead to bad decisions by the people that could work on the project. This paper searches for strategies to counter superintelligence misinformation. Two strategies are examined: to prevent the spread of misinformation and to correct it after it has spread. Misinformation can be difficult to correct, so preventing it seems to be the best strategy. The strategies proposed can be used to bring public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "metrics": {
          "SARI": 39.945907622049106,
          "BLEU": 0.3706624436801186,
          "FKGL": 13.7
        }
      },
      {
        "source": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\\\\Omega(2^{\\\\log^{1-\\\\epsilon}n^4})$ in polynomial time unless $NP \\\\subseteq DTIME(n^{\\\\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "prediction": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Ω(2^(log^(1-ε)n^4)) in polynomial time unless NP ⊆ DTIME(n^(polylog(n))). Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "reference": "The online misinformation could cause public panic and serious economic damages. The goal of misinformation containment is to limit the spread of misinformation in online social networks by launching campaigns to counter misinformation. We present the first analysis of the misinformation containment problem with a random number of cascades (people observe the actions of others and then make the same choice) allowed. First, we provide a diffusion model and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Ω(2log1−ϵn4) in polynomial time unless $NP \\subseteq DTIME(n^{\\polylog{n}})$ Third, we introduce several types of cascade priority that are frequently seen in social networks. Finally, we design algorithms for solving the misinformation containment problem. The proposed algorithm is effective and shows encouraging results.",
        "metrics": {
          "SARI": 46.65329731013217,
          "BLEU": 0.5307392233435999,
          "FKGL": 14.6
        }
      },
      {
        "source": "Yet contemporary work in Philosophy argues provisional belief in conspiracy theories is—at the very—least understandable (because conspiracies occur) and if we take an evidential approach—judging individual conspiracy theories on their particular merits—belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine the kinds of evidence typically associated with conspiracy theories, showing that the evidential problems typically associated with conspiracy theories are not unique to such theories. As such, if there is a problem with the conspiracy theorist’s use of evidence, it is one of principle: is the principle which guides their use of evidence somehow in error? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for a scepticism of conspiracy theories based purely on their use of evidence.",
        "prediction": "Yet contemporary work in Philosophy argues that provisional belief in conspiracy theories is, at the very least, understandable because conspiracies occur. If we take an evidential approach—judging individual conspiracy theories on their particular merits—then belief in such theories can be warranted in many cases. Based on this work, I look at the types of evidence commonly linked to conspiracy theories and show that the problems with evidence related to conspiracy theories are not unique to them. If there is an issue with how conspiracy theorists use evidence, it relates to a fundamental principle: is their thinking about evidence incorrect? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for skepticism of conspiracy theories based purely on their use of evidence",
        "reference": "To believe in conspiracy theories is understandable as conspiracies occur and if we take an evidential approach, belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine evidence associated with conspiracy theories, showing that the evidential problems are not unique to conspiracy theories. The problem with the use of evidence of conspiracy theorist could be the principle which guides their use of it. I argue that there is no ground to be suspicious of conspiracy theories as a whole, because those are based on evidence.",
        "metrics": {
          "SARI": 34.39824512060441,
          "BLEU": 0.16489195431770592,
          "FKGL": 16.8
        }
      },
      {
        "source": "Conspiracy theories have gained much academic and media attention recently, due to their large impact on public events. However, little is known about how conspiracy theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis--the Zika virus outbreak. Using a mixed-methods approach including content analysis and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the Zika crisis, the conditions under which Zika conspiracy theories emerge, and the particular discursive strategies through which Zika conspiracy theories developed in online forums. Our analysis shows that conspiracy talk emerged as people attempted to make sense of a public health crisis, reflecting their emergent information needs and their pervasive distrust in formal sources of Zika information. Practical implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "prediction": "Conspiracy theories have gained much academic and media attention recently due to their significant impact on public events. However, little is known about how these theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis—the Zika virus outbreak. We used a combination of methods, including content analysis and analyzing discussions, to identify different types of conspiracy theories shared on Reddit during the Zika crisis. We also explored why these conspiracy theories appeared and how they were talked about in online discussions. Our analysis shows that conspiracy talk emerged as people tried to make sense of the public health crisis, reflecting their information needs and their distrust of formal sources of Zika information. We also discuss practical implications for social computing researchers, health practitioners, and policymakers",
        "reference": "Conspiracy theories have gained much attention recently, due to their large impact on public events. Little is known about how conspiracy theories are produced and developed on social media. We present a study of conspiracy theory creation on Reddit during a public health crisis. Using content and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the crisis, the conditions of their creation, and the strategies of their development in online forums. Our analysis shows that conspiracy talk come from people trying to make sense of a public health crisis, which reflect their information needs and their lack of confidence in formal sources of information. Implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "metrics": {
          "SARI": 38.93730585618449,
          "BLEU": 0.38424440166438795,
          "FKGL": 14.5
        }
      },
      {
        "source": "Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories evolve, multiply, and interconnect, further complicating efforts to understand them and to limit their propagation. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif such as \\\"governmental agency-controls-communications\\\" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.",
        "prediction": "Conspiracy theories are omnipresent in online discussions whether to explain a late-breaking event that still lacks an official report or to give voice to political dissent. These theories change spread and connect with each other making it harder to understand them and stop them from spreading. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory the conspiratorial agents the actions they perform and their targets. For example a narrative-motif such as governmental agency-controls-communications represents the various ways in which multiple conspiratorial statements show how governmental agencies control information. Thus these narrative-motifs reveal commonalities between different conspiracy theories even when they refer to different events or circumstances. In the process these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they discuss. Our approach enables a population-scale study of conspiracy theories in alternative news and social media which has implications for understanding their adoption and combating their spread.",
        "reference": "Conspiracy theories are omnipresent in online discussions. Conspiracy theories evolve, multiply, and interconnect, complicating how to understand them and to limit their propagation. It is crucial to develop methods to examine the nature of these conspiratorial discussions. What do users talk about? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in an online community on the social media Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif (an idea) such as \"governmental agency-controls-communications\" represents the various ways in which multiple conspiratorial statements suggest how governmental agencies control information. Narrative-motifs expose similarities between multiple conspiracy theories even when they refer to different events or circumstances. These representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media and ways to understand them and limit their propagation.",
        "metrics": {
          "SARI": 42.32327294996409,
          "BLEU": 0.5732185484034809,
          "FKGL": 14.2
        }
      },
      {
        "source": "Blockchain technology is the underlying enabling technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force. Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make that transaction simpler and easier between entities. We investigate factors that influence an individualu0027s intention to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption grounded in the theory of planned behavior (TPB) to: identify the determinants for the acceptance of cryptocurrency and explore the relative importance of each construct. We offer empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context.",
        "prediction": "Blockchain technology is the underlying enabling technology developed for Bitcoin the most common cryptocurrency Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make transactions simpler and easier between entities In this context we investigate factors that influence an individual’s intention to use a blockchain cryptocurrency We create a model to understand how people decide to use cryptocurrency based on the theory of planned behavior TPB This helps us find out what factors influence acceptance and how important each factor is Our research offers empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context",
        "reference": "Blockchain technology is a technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become popular with the potential to become a powerful disruptive force. People and organizations may use it to increase secure data exchange and make transactions simpler and easier. We investigate what influence people to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption to identify how cryptocurrency is accepted. Our evidence allow a better understanding of cryptocurrency adoption.",
        "metrics": {
          "SARI": 45.1290613188165,
          "BLEU": 0.276205913008221,
          "FKGL": 56.4
        }
      },
      {
        "source": "With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. We find that cryptojacking is common, with currently 1 out of 500 sites hosting a mining script. Moreover, we perform several secondary analyses to gain insight into the cryptojacking landscape, including a measurement of code characteristics, an estimate of expected mining revenue, and an evaluation of current blacklist-based countermeasures.",
        "prediction": "With the introduction of memory-bound cryptocurrencies such as Monero the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs Based on this technology a new form of parasitic computing widely called cryptojacking or drive-by mining has gained momentum on the web A cryptojacking site takes advantage of visitors' computer power to secretly mine for cryptocurrencies In this paper we systematically explore this phenomenon We propose a 3-phase analysis approach that allows us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites Our findings indicate that cryptojacking is common with currently 1 out of 500 sites hosting a mining script Additionally we perform several secondary analyses to gain insights into the cryptojacking landscape including a measurement of code characteristics an estimate of expected mining revenue and an evaluation of current blacklist-based countermeasures",
        "reference": "Memory-bound cryptocurrencies like Monero can be mined  from a browser with JavaScript, better than with a dedicated rig. However, this technology enables cryptojacking, whereby a site covertly mines for cryptocurrencies on its visitors’ computers. We propose a new approach to identify mining scripts. The approach was evaluated on Alexa 1 websites. Cryptojacking is common, as 1 in 500 sites host a mining script. We also measured code characteristics, estimated mining revenue, and evaluated blacklist-based countermeasures.",
        "metrics": {
          "SARI": 34.11368441072357,
          "BLEU": 0.04900259444712502,
          "FKGL": 64.9
        }
      },
      {
        "source": "The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.  contracts :[39],\"are commonly written in a high-level language such as Ethereumu0027s Solidity, and translated to compact low-level bytecode for deployment on the blockchain. Once deployed, the bytecode is autonomously executed, usually by a %Turing-complete virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts :[39],\"are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal consists of an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations. Users of the framework can express security analyses in a declarative fashion: a security analysis is expressed in a logic specification written in the \\souffle language.",
        "prediction": "The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have quickly become prominent, with applications predicted in law, business, commerce, and governance. These contracts are usually created using a programming language like Ethereum's Solidity, which is then turned into a smaller, simpler form of code to be used on the blockchain. Once deployed, this code is autonomously executed, often by a Turing-complete virtual machine. Like all programs, smart contracts can be vulnerable to malicious attacks due to poor programming practices, languages, and tools, including buggy compilers. At the same time, smart contracts are high-value targets, often commanding large amounts of cryptocurrency. Therefore, developers and auditors need security frameworks that can analyze low-level bytecode to detect potential vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal includes an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode into semantic logic relations. Users of the framework can express security analyses clearly, using a logic specification written in the Souffle language.",
        "reference": "Smart contracts have appeared thanks to blockchain technology. These autonomous programs are used in law, business, commerce, and governance. Smart contracts are written in a high-level language such as Ethereum’s Solidity and translated to bytecode. that live and run on the blockchain are predicted in Once deployed on the blockchain, the bytecode is autonomous. Smart contracts are vulnerable to malicious attacks due to poor programming, languages and toolchains Smart contracts can command large amounts of cryptocurrency. Developers and auditors need frameworks to detect security vulnerabilities. Vandal is a security analysis framework for Ethereum smart contracts. Vandal converts Ethereum Virtual Machine bytecode to semantic logic relations. Users can express security analyses in the Soufflé language.",
        "metrics": {
          "SARI": 46.70835486062201,
          "BLEU": 0.24978382295858587,
          "FKGL": 12.7
        }
      },
      {
        "source": "Vandal is both fast and robust, successfully analysing over 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.",
        "prediction": "Vandal is both fast and robust, successfully analyzing more than 95% of 141,000 unique contracts in an average of 4.15 seconds. It performs better than the best current tools—Oyente, EthIR, Mythril, and Rattle—under similar conditions",
        "reference": "Vandal is both efficient (95% accuracy), fast (141k in 4.15 s) and robust. It outperforms the state of the art tools like Oyente, EthIR, Mythril, and Rattle",
        "metrics": {
          "SARI": 50.578336701488034,
          "BLEU": 0.18973813416364887,
          "FKGL": 11.3
        }
      },
      {
        "source": "As Bitcoinu0027s popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds.",
        "prediction": "As Bitcoin's popularity has grown over the decade since its creation it has become more at risk of attacks from various kinds of people",
        "reference": "One of the most powerful potential adversaries is the country of China, which has expressed adversarial positions regarding the cryptocurrency and demonstrated powerful capabilities to influence it.",
        "metrics": {
          "SARI": 33.731884057971016,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "For decentralised P2P networks, it is very important to have a mechanism in place that allows the nodes to control resource usage and prevent flooding and denial-of-service attacks with spam. In this paper, we discuss and compare the different approaches to fully decentralised resource control that are used by projects in the cryptocurrency space. The introduced methods are then applied to design a decentralised exchange for Namecoin names (or more generally, crypto assets) as an example.",
        "prediction": "For decentralised P2P networks, it is very important to have a mechanism in place that allows the nodes to manage how resources are used and stop attacks that overwhelm the system with junk messages. In this paper, we discuss and compare different approaches to fully decentralised resource control used by projects in the cryptocurrency space. We then apply these methods to design a decentralised exchange for Namecoin names or more generally, crypto assets as an example",
        "reference": "In decentralised P2P networks nodes need a way  to control resource usage and prevent flooding and denial-of-service attacks. We compared different approaches to decentralised resource control in cryptocurrency projects. We designed a decentralised exchange for crypto assets, for  example Namecoin names, with these methods.",
        "metrics": {
          "SARI": 33.949917467659915,
          "BLEU": 0.08749430573610924,
          "FKGL": 13.2
        }
      },
      {
        "source": "Cryptocurrency is a rapid developing financial technology innovation which has attracted a large number of people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have triggered panic and chain reactions towards the application and adoption of cryptocurrency and have caused public security related events. In this paper, we analyze the dynamics and systemic risk of the cryptocurrency market based on the public available price history. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.",
        "prediction": "Cryptocurrency is a rapidly developing financial technology innovation that has attracted many people around the world. The rapid changes and large price swings of cryptocurrency, along with the differing views of financial authorities in various countries, have caused fear and reactions regarding the use and acceptance of cryptocurrency, leading to issues related to public safety. In this paper, we analyze the dynamics and systemic risk of the cryptocurrency market based on the publicly available price history. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.",
        "reference": "Cryptocurrency is a financial technology innovation which has attracted many people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have caused panic. In this paper, we analyze the dynamics and risks of the cryptocurrency market. Consistent with public perception, our analysis reveals that the cryptocurrency market is fragile and unstable.",
        "metrics": {
          "SARI": 41.20303177944967,
          "BLEU": 0.34280415489256744,
          "FKGL": 16
        }
      },
      {
        "source": "Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by treating it as a financial asset. We position cryptocurrency by comparing its dynamic characteristics with two traditional and massively adopted financial assets: foreign exchange and stock. Our investigation suggests that the dynamics of cryptocurrency are more similar to stock. As to the robustness and clustering structure, our analysis shows cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "prediction": "Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by treating it as a financial asset. We position cryptocurrency by comparing its dynamic characteristics with two traditional and widely used financial assets: foreign exchange and stock. Our research shows that cryptocurrency behaves more like stocks. Additionally, our analysis indicates that the cryptocurrency market is more fragile than the stock market, making it currently a high-risk financial market.",
        "reference": "Cryptocurrency was initially an implementation of digital currency, then derivatives were created in various fields. This paper aims to help understand cryptocurrency. We compared cryptocurrency with foreign exchange and stock. Our investigation suggests that cryptocurrency is similar to stock. Our analysis shows that cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "metrics": {
          "SARI": 37.139544993639824,
          "BLEU": 0.27368194676749763,
          "FKGL": 15.8
        }
      },
      {
        "source": "The smart device owning rate such as smart phone and smart watch is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time, blockchain-based cryptocurrency is becoming a nonnegligible type of currency and the total value of all types of cryptocurrency has reached USD 200 billion. Therefore, it is a natural demand to support cryptocurrency payment on mobile devices. Considering the poor infrastructure and low penetration of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are the two main obstacles for mobile payment using cryptocurrency. We propose two different schemes for cryptocurrency mobile payment, one involves a centralized bank and the other one does not require any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.",
        "prediction": "The smart device owning rate such as smartphones and smartwatches is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time blockchain technology is making cryptocurrencies important and their total value has reached USD 200 billion. Therefore there is a growing demand to support cryptocurrency payments on mobile devices. Considering the poor infrastructure and low penetration of financial services in developing countries this combination is especially attractive. However the high storage cost and payment processing latency are the two main obstacles for mobile payment using cryptocurrency. We propose two different schemes for cryptocurrency mobile payment: one involves a centralized bank while the other does not require any centralized party. Additionally we provide a solution for the bank to meet KYC (know your customer) and AML (anti-money laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.",
        "reference": "The owning rate of smart devices is higher than ever before and mobile payment has become a major payment method. Cryptocurrency is becoming an important type of currency and the total value of cryptocurrencies has reached USD 200 billion. Therefore, support of cryptocurrency payment on mobile devices is a natural demand. Considering the poor infrastructure and the low usage of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are two obstacles for cryptocurrency mobile payment. We propose two solutions, one with a centralized bank and the other one without any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) expectations about cryptocurrency mobile payment.",
        "metrics": {
          "SARI": 35.877802895159135,
          "BLEU": 0.3789497445444064,
          "FKGL": 14.1
        }
      },
      {
        "source": "Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.",
        "prediction": "Motivated by recent financial crises, significant research efforts have been put into studying contagion effects and herding behavior in financial markets. Much less has been said about the influence of financial news on these markets. We suggest a new way to measure how financial news connects online, called the News Cohesiveness Index (NCI), and we show that it can indicate potential financial risks. We evaluate the NCI using financial documents from large web news sources daily, from October 2011 to July 2013, and analyze how financial markets interact with related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is a more general and robust measure of systemic risk expressed in news than measures based on simply counting specific terms. Our results indicate that cohesiveness in financial news is highly correlated with and driven by volatility in the financial markets.",
        "reference": "Motivated by recent financial crises, research efforts have been put into studying contagion effects (spread of market disturbances) and herding behaviour (individuals acting collectively without centralized direction) in financial markets. Much less has been said about influence of financial news on financial markets. We propose a measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator, which signals probability of financial system crises. We evaluate the NCI on financial documents from Web news sources and analyse the relation between financial markets and financial news. We hypothesized that strong cohesion in financial news reflects variations in the financial markets. Cohesiveness is a better measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results show that cohesiveness in the financial news is highly correlated with variations in the financial markets.",
        "metrics": {
          "SARI": 35.19948379076947,
          "BLEU": 0.46551470765322045,
          "FKGL": 12.7
        }
      },
      {
        "source": "In this paper, I propose a methodology to study the comovement between the entropy of different financial markets. The entropy is derived using singular value decomposition of the components of stock market indices in financial markets from selected developed economies, i.e., France, Germany, the United Kingdom, and the United States. I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using a dynamic factor model and derive a common factor behind the entropy movements in these four markets.",
        "prediction": "In this paper I propose a methodology to study the comovement between the entropy of different financial markets The entropy is calculated using a mathematical method called singular value decomposition which analyzes stock market data from selected developed countries including France Germany the United Kingdom and the United States I study how a shock in the entropy in the United States affects the entropy in the other financial markets Additionally I model the entropy using a dynamic factor model to identify a common factor behind the entropy movements in these four markets",
        "reference": "In this paper, I propose a methodology to study the comovement (correlation on profits on investment) between the entropy (randomness and uncertainty) of different financial markets. The entropy is based on the components measuring the stock market in financial markets from selected developed economies (France, Germany, the United Kingdom, and the United States). I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using different time variables and obtain a common factor behind the entropy movements in these four markets.",
        "metrics": {
          "SARI": 40.87311234082411,
          "BLEU": 0.45028527671954854,
          "FKGL": 41.5
        }
      },
      {
        "source": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties - none of which passes through company-owned systems, and is to all intents and purposes 'off the grid'. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics. Detecting 'tech-savvy' corporate fraudsters is a constant game of catch-up. It's not only about playing catch-up with the intellect, motives and awareness of the e-fraudster, but also the technologies that can be misused. What's more, the methods through which the technology can be manipulated to secrete, disguise and protect fraudulent activities - all while staying away from corporate networks where they can readily be monitored and detected - are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.",
        "prediction": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. However, mobile phones have evolved into smartphones, and fraudsters have adapted as well. Smartphones offer many functions that can be exploited for data theft or inappropriate contact—all of which occur outside of company-owned systems and are, for all practical purposes, 'off the grid.' Employers must be aware of these risks when issuing devices and ensure that processes are in place to handle suspicions, explains Philip Ridley of CCL-Forensics. Finding skilled corporate fraudsters is an ongoing challenge. It's not just about keeping up with their intelligence, motives, and knowledge, but also the technology they misuse. Furthermore, the methods by which this technology can be manipulated to hide and protect fraudulent activities—while avoiding detection on corporate networks—are constantly evolving. As a result, a company's intellectual property and sensitive data are at risk of sabotage or theft.",
        "reference": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones have functionalities that can be used for data theft or inappropriate contact with other parties. They are out of the systems of the company and are not monitored or controlled. Employers need to be aware of these risks when devices are delivered and have processes when suspicions are raised. Detecting fraudsters with computer skills is a constant game of catch-up. It's not only about playing catch-up with the cyberfraudster, but also the technologies that can be misused. Moreover, the methods assisting fraudulent activities (still out of reach of company systems) are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or theft.",
        "metrics": {
          "SARI": 41.538377896280096,
          "BLEU": 0.31677391686088846,
          "FKGL": 12.8
        }
      },
      {
        "source": "Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes to phishing campaigns, a cadre of file matching algorithms is implemented to detect phishing websites based on their content, employing a custom data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using a variety of different content-based approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "prediction": "Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes in phishing campaigns, a cadre of file matching algorithms is used to find phishing websites by looking at their content, using a custom data set that includes 17,992 phishing attacks targeting 159 different brands. The results of experiments using various content-based approaches demonstrate that some can achieve a detection rate of over 90% while maintaining a low false positive rate.",
        "reference": "Phishers, people who attempt to trick people over the internet to steal their money, continue to modify the web pages used in their attacks to imitate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as changing the source code, which is the programming instructions, or as apparent as adding or removing significant content. To respond to these changes to phishing campaigns, algorithms are used to detect phishing websites based on their content, employing a data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using different approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "metrics": {
          "SARI": 35.180433234957015,
          "BLEU": 0.5383301486697567,
          "FKGL": 15.7
        }
      },
      {
        "source": "Mobile devices are very common in everyone’s day-to- day life. Nowadays such devices come with many features of desktop or laptop. Hence people can use these devices for diverse applications. As the acceptability and usability of such devices are very high, there are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is in hike. So it becomes necessary to digitally analyze such devices requiring cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and an agent based approach for logically acquiring such devices. Also it describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.",
        "prediction": "Mobile devices are very common in everyone’s day-to-day life. Nowadays, such devices come with many features of desktops or laptops, allowing people to use them for various applications. Since these devices are highly accepted and easy to use, there is a risk that they may also be used for illegal activities. The percentage of mobile phones or smartphones involved in cyber crimes is on the rise. Therefore, it is important to analyze these devices using special tools for cyber forensics. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smartphones and describes an agent-based approach for logically acquiring such devices. It also presents a tool developed for acquiring and analyzing Windows Mobile devices and WinCE PDAs for forensic purposes.",
        "reference": "Mobile devices are very common in everyone’s day-to-day life. Nowadays such devices come with many features of computers. People can use these devices for diverse applications. There are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is increasing. So it becomes necessary to digitally analyze such devices with cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and how to acquire such devices. Also it describes a forensic tool for acquiring and analyzing Windows Mobile devices and personal digital assistants.",
        "metrics": {
          "SARI": 43.05273654221494,
          "BLEU": 0.4085956404380667,
          "FKGL": 11.3
        }
      },
      {
        "source": "Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It gives the information about the processing history of an image, and also can expose forgeries present in an image. There have been a few methods proposed to detect different types of image editing operations in a single framework. However, all the operations have to be known a priori in the training phase. But, in real-forensics scenarios it may not be possible to know about the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.",
        "prediction": "Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It provides information about the processing history of an image and can expose any forgeries present. Several methods have been proposed to detect different types of image editing operations within a single framework. However, all the operations must be known in advance during the training phase. In real-forensics scenarios, it may not be possible to know about the editing operations carried out on an image. To address this problem, we propose a novel deep learning-based method that can differentiate between various types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep Siamese neural network. Once the network learns the features that can distinguish between different image editing operations, it can identify operations not present in the training stage. The experimental results demonstrate the effectiveness of the proposed method in detecting and discriminating different image editing operations.",
        "reference": "Detection of image editing is an important task in image forensics. The edit history of an image can expose forgeries in the image. Various methods have been proposed to detect image editing operations. However all of the possible types of image manipulation must be known in advance. But, in the real world, manipulations on images are sometimes unknowable. A novel deep learning-based method can distinguish between different types of image editing operations. This method uses a deep siamese neural network to compare images two by two. Using deep learning, the computer can learn to see features of edited images as compared to unedited and learn to differentiate between different image editing operations. This method works to find out what editing an image has undergone.",
        "metrics": {
          "SARI": 42.47445749630851,
          "BLEU": 0.20988218584302443,
          "FKGL": 13
        }
      },
      {
        "source": "The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices – potentially progressing a wide variety of currently hindered digital investigations.",
        "prediction": "The increasing prevalence of Internet of Things IoT devices has made it inevitable that their relevance to digital forensic investigations will grow in the foreseeable future These devices produced by various vendors often have limited standard interfaces for communication such as USB ports or WiFi/Bluetooth wireless interfaces Meanwhile as there is a growing focus on the security and privacy of user data built-in encryption is becoming standard in consumer-level computing devices including IoT devices Under these circumstances digital forensic investigations face significant challenges when analyzing data from IoT devices \n\nThis work explores the literature on electromagnetic EM side-channel analysis to assist digital forensic investigations on IoT devices EM side-channel analysis is a method that uses unintended electromagnetic signals to listen in on what computers are doing and how they handle data The non-intrusive nature of EM side-channel techniques makes them a practical option for digital forensic investigations as these methods do not require any changes to the target device \n\nThe literature on various EM side-channel analysis attack techniques is discussed selected based on their applicability to IoT device investigation scenarios Insights gained from this background study are used to identify promising future applications of the technique for digital forensic analysis on IoT devices potentially advancing a wide variety of currently hindered digital investigations",
        "reference": "Internet of Things (IoT) devices (devices connected to the internet, collecting and sharing data) will be used in digital forensic investigations in the future. These devices have limited interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing focus on the security and privacy of user data, built-in encryption (which encode information) is becoming commonplace in devices. This presents a significant challenge to digital forensic investigations, where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where electromagnetic emissions are used for eavesdropping on the operations and data handling of devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected according to their applicability in IoT device investigations. The data obtained from our survey are used to identify promising future applications of the technique for digital forensic analysis on IoT devices.",
        "metrics": {
          "SARI": 34.15011769261141,
          "BLEU": 0.36974637807768884,
          "FKGL": 89.9
        }
      },
      {
        "source": "The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is of paramount importance. However, this can be difficult since the attacker has many opportunities and means to hide such changes. In particular, cloud computing, host and guest machine time can be manipulated in various ways by an attacker. Guest virtual machines are especially vulnerable to attacks coming from their (more privileged) host. As such, it is important to guarantee the timeline integrity of both hosts and guests in a cloud, or at least to ensure that the alteration of such timeline does not go undetected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. Further, we describe a novel architecture for host and guest time alteration detection and correction/resilience with respect to compromised hosts and guests. The proposed framework has been implemented on an especially built simulator.",
        "prediction": "The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is very important. However, this can be difficult because attackers have many opportunities and ways to hide these changes. In cloud computing, an attacker can change the time on both host and guest machines in different ways. Guest virtual machines are especially vulnerable to attacks from their more privileged host. Therefore, it is crucial to ensure the timeline integrity of both hosts and guests in a cloud, or at least to make sure that any changes to the timeline do not go undetected. In this paper, we survey the issues related to host and guest machine time integrity in the cloud. We also describe a new architecture for detecting and correcting time alterations regarding compromised hosts and guests. The proposed framework has been implemented on a specially built simulator",
        "reference": "Machine timestamp falsification can be used in cyber attacks. The algorithm detects manipulations effectively. That is why it is important to detect such changes and to reconstruct the actual timeline of events. However, the hacker can try to hide their tracks. In cloud computing, hackers can manipulate host and guest machine time. Guest virtual machines use computing resources provided by a physical machine called a host. Guest virtual machines are especially vulnerable to attacks coming from their host. That is why the timeline integrity of both hosts and guests in a cloud is important. Any manipulation of such a timeline should be detected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. A new algorithm is proposed to detect host and guest time manipulations and to correct or reject them. A simulator has been built for this purpose.",
        "metrics": {
          "SARI": 48.16781449307274,
          "BLEU": 0.30730280712521424,
          "FKGL": 9.5
        }
      },
      {
        "source": "Image tampering, being readily facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. As the most popular multimedia data, JPEG images can be easily tampered without leaving any clues; therefore, JPEG-based forensics , including the detection of double compression, interpolation, rotation, etc., has become an active research topic in multimedia forensics. Nevertheless, the interesting issue of detecting image tampering and its related operations by using the same quantization matrix has not been fully investigated. Aiming to detect such forgery manipulations under the same quantization matrix, we propose a detection method by using shift-recompression -based reshuffle characteristic features. The learning classifiers are applied for classification. Our experimental results indicate that the approach is indeed highly effective in detecting image tampering and relevant manipulations with the same quantization matrix.",
        "prediction": "Image tampering, facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. JPEG forensics, which includes finding issues like double compression and image rotation, has become an important area of study in multimedia. However, the detection of image tampering and related operations that use the same quantization matrix has not been fully investigated. We propose a detection method that utilizes shift-recompression-based reshuffle characteristic features to identify such forgery manipulations. Learning classifiers are applied for classification. Our experimental results indicate that this approach is highly effective in detecting image tampering and relevant manipulations with the same quantization matrix.",
        "reference": "Image tampering has become a serious problem with the advancement of digital techniques. JPEG images can be easily manipulated without leaving any clues. This is why researchers are trying to develop methods for JPEG image manipulation detection. However, methods based on image compression techniques, like quantization, are rarely applied. To detect such manipulations, we propose a detection method to identify inconsistencies in the quantization matrix caused by the manipulation. Then machine learning algorithms are applied to detect image manipulation. Experimental results show that the approach is highly effective in detecting image tampering.",
        "metrics": {
          "SARI": 42.24743595007529,
          "BLEU": 0.14173950230949847,
          "FKGL": 15.9
        }
      },
      {
        "source": "In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations of its monocular camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets of the whole body including the legs, as well as the camera extrinsic and intrinsic parameters. Furthermore, we developed an approach to automatically select a subset of configurations for the calibration process that yields a good trade-off between the number of observations and accuracy. Further, our approach to configuration selection yields substantially better optimization results compared to randomly chosen viable configurations. Hence, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "prediction": "In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations from its monocular camera. Our technique estimates all the important parameters of the model including the joint angle offsets for the whole body and the camera's external and internal settings. We also developed a method to automatically select a subset of configurations for the calibration process which provides a good balance between the number of observations and accuracy. Additionally our configuration selection method achieves significantly better optimization results compared to randomly chosen viable configurations. As a result our system requires fewer configurations to achieve accurate results. Our optimization process is general and the implementation which is available online can easily be applied to different humanoids.",
        "reference": "In this paper, we present a novel approach to accurately calibrate a humanoid and its motions (robot with human form or characteristics) based on observations of its camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets and the camera parameters. We developed an approach to automatically select configurations for the calibration process that produces a good compromise between the number of observations and accuracy. Our approach to configuration selection generates better optimization results compared to randomly chosen viable configurations. Thus, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "metrics": {
          "SARI": 40.17510026144607,
          "BLEU": 0.4725136272678235,
          "FKGL": 16.1
        }
      },
      {
        "source": "In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. From the data of an onboard consumer-grade depth camera, our system estimates the robot's pose to compensate for drift of odometry and maintains a heightmap representation of the environment. Based on this model, our system iteratively computes sequences of safe actions including footsteps and whole-body motions, leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system leads to robust navigation in cluttered environments and the robot is able to traverse highly challenging passages.",
        "prediction": "In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. Using data from a regular depth camera, our system figures out the robot's position to correct errors in movement and keeps a detailed map of the area's height. Based on this model, our system iteratively computes sequences of safe actions, including footsteps and whole-body motions, leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that considers the shapes of both the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system enables robust navigation in cluttered environments, allowing the robot to traverse highly challenging passages",
        "reference": "In this paper, we present a navigation system that allows humanoid robots (robots with human form or characteristics) to autonomously navigate in unknown, cluttered environments. From the data of a camera which can judge depth and distance, our system estimates the robot’s position and maintains a height representation of the environment. Our system determines safe actions (including footsteps and body motions) leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments, our system allows the robot to navigate in clustered environments and traverse challenging passages.",
        "metrics": {
          "SARI": 42.38438015760228,
          "BLEU": 0.4264313666873839,
          "FKGL": 14.9
        }
      },
      {
        "source": "In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict traversal costs corresponding to potential obstacles in regions not yet observable by the robot's sensors. By taking into account the predicted costs for path planning, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.",
        "prediction": "In this paper we introduce an approach to efficient robot navigation through cluttered indoor environments We propose to estimate how many obstacles are nearby based on the objects the robot has already found and use this information to predict how difficult it will be to move through areas that the robot cannot see yet By considering these predicted difficulties for path planning the robot can navigate in a more foresighted manner reducing the risk of getting stuck in cluttered regions As the experimental results demonstrate our method enables the robot to efficiently navigate through environments with clutter and achieves significantly shorter completion times compared to a standard approach that does not use any prediction",
        "reference": "In this paper, we introduce an approach for robot to navigate through cluttered indoor environments. We propose to estimate obstacles based on already detected objects and use them to predict potential obstacles in regions not yet observable by the robot. The robot is then able to navigate in a more anticipated manner and reduces the risk of getting stuck. Our method enables the robot to efficiently navigate through cluttered environments and faster than a standard approach not using any prediction.",
        "metrics": {
          "SARI": 33.5460517274552,
          "BLEU": 0.2246598428671468,
          "FKGL": 48.9
        }
      },
      {
        "source": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, step-by-step following of the human might not be the optimal solution since better paths for the robot exist. We propose to perform a prediction about the human's future movements and use this information in a reinforcement learning framework to generate foresighted navigation actions for the robot. Since frequent occlusions of the human will occur due to obstacles and the robot's constrained field of view, the estimate about the humans's position and the prediction of the next destination are affected by uncertainty. Our approach deals with such situations by explicitly considering occlusions in the reward function such that the robot automatically considers to execute actions to get the human in its field of view. We show in simulated and real-world experiments that our technique leads to significantly shorter paths compared to an approach in which the robot always tries to closely follow the user and, additionally, can handle occlusions.",
        "prediction": "In many situations, users walk on typical paths between specific destinations where the service of a mobile robot is needed. Depending on the environment and the paths, following the human step-by-step might not be the best solution since better routes for the robot may exist. We suggest predicting where the human will move next and using this information in a learning system to help the robot plan its movements better. Since the human will often be blocked from view due to obstacles and the robot's limited sight, estimating the human's position and predicting their next destination can be uncertain. Our approach addresses these situations by considering occlusions in the reward function, allowing the robot to automatically take actions to bring the human back into view. We show in both simulated and real-world experiments that our technique results in significantly shorter paths compared to an approach where the robot always tries to closely follow the user, while also being able to handle occlusions.",
        "reference": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, following the human might not be the optimal solution because better paths for the robot exist. We propose to predict the human’s future movements and use this information to generate navigation actions for the robot. The estimation of the human’s position and the prediction of the next destination are uncertain because of the occlusions due to obstacles and the robot’s limited field of view. Our approach deals with considering occlusions so the robot automatically considers to execute actions to get the human in its field of view. We show that our technique leads to significantly shorter paths compared to an approach in which the robot follows the user and, additionally, can handle occlusions.",
        "metrics": {
          "SARI": 36.45900027591826,
          "BLEU": 0.33739193019216834,
          "FKGL": 15
        }
      },
      {
        "source": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem, especially when this person moves around since the robot’s field of view is constrained and the environment structure typically poses further visibility constraints that influence the perception of the user. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. As the robot needs time to reach the possible search locations, we take this time as well as the visibility constraints into account when computing effective search locations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.",
        "prediction": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a difficult problem, especially when the person moves around because the robot can only see a limited area, and the layout of the environment can create visibility challenges that affect how the robot perceives the user. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. As the robot needs time to reach the possible search locations, we take this time and the visibility constraints into account when computing effective search locations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.",
        "reference": "An autonomous service robot often first has to search for a user to execute a task. This is a challenging problem, especially when this person moves around because the robot’s field of view is limited. The environment also poses visibility limitations that influence the perception of the user. In this paper, we propose a method that estimate the user’s observability in the environment. When estimating effective search locations, we take into account the time for the robot to reach the search location and the visibility limitations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments demonstrate that our approach leads to a significantly shorter search time compared to other approach.",
        "metrics": {
          "SARI": 43.39785190839709,
          "BLEU": 0.47657647400422776,
          "FKGL": 14.5
        }
      },
      {
        "source": "Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Considering dynamic obstacles introduces another level of complexity, since they can lead to necessary replanning or collisions at later stages. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.",
        "prediction": "Perception of the local environment is crucial for mobile robots to navigate safely in dynamic settings. Most robots, such as humanoids and smaller wheeled robots, rely on flat surfaces. For humanoid robots, a simple 2D map of the area isn't enough because they can step over and onto objects and need to know the height of these objects. Additionally, accounting for dynamic obstacles adds another layer of complexity, as these can lead to necessary replanning or collisions later on. In this paper, we present a framework that first extracts flat areas from height maps and identifies dynamic obstacles. Our system then uses this information to create a set of prediction maps, allowing for efficient path planning in real time with low CPU usage. We demonstrate through simulations and real-world experiments that our framework keeps computation times well under 10ms per cycle, enabling forward-thinking real-time 3D footstep planning",
        "reference": "Perception of the environment is necessary for mobile robots to navigate safely in dynamic environments. Most robots (humanoids and wheeled robots) rely on planar regions. For humanoids (robots with human form or characteristics), a 2D map is not sufficient because they can step over and onto objects, so they need height information. Dynamic obstacles bring another level of complexity, because it can lead to necessary replanning or collisions. In this paper, we present a framework that extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be planned in real time at low processor cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms and allows for real-time 3D footstep planning.",
        "metrics": {
          "SARI": 44.27878008127053,
          "BLEU": 0.4080051466235638,
          "FKGL": 12.7
        }
      },
      {
        "source": "In this paper, a lab automation drone notional concept is introduced. Here, a robotic limb is attached to a robotic rotorcraft. The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high throughput systems (HTS). The resulting drone could augment existing HTS operations. The 6 degree-of-freedom (DOF) arm and gripper design are presented. Test-and-evaluation approach and results are also given.",
        "prediction": "In this paper a lab automation drone notional concept is introduced Here a robotic limb is attached to a robotic rotorcraft The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high throughput systems HTS The resulting drone could augment existing HTS operations The design of the arm and gripper which can move in six different directions is shown Additionally the test-and-evaluation approach and results are provided",
        "reference": "In this paper, a concept of lab automation drone is introduced. Here, a robotic arm is attached to a drone. The arm’s gripper allows the drone to manipulate objects such as chips and test tubes often used in scientific experimentations like high throughput systems. The drone could augment existing high throughput systems operations. The 6 degree-of-freedom arm and gripper design are presented. Results of evaluations and tests are also given.",
        "metrics": {
          "SARI": 32.28652243129947,
          "BLEU": 0.24056804477989047,
          "FKGL": 36.1
        }
      },
      {
        "source": "Robot companionship has become more popular in past years. However, humanoid gait might be somewhat unstable for these applications. Even with miniature humanoids, falls occur frequently. Thus, wheel attachments have been added onto a miniature humanoid, so it can move faster and with more stability than walking. In addition, with such attachments a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as an experimentation and performance evaluation platform. This paper discusses preliminary work regarding robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "prediction": "Robot companionship has become more popular in past years. However, humanoid movement can be a bit wobbly for these applications. Even with miniature humanoids, falls occur frequently. To address this, wheel attachments have been added to a miniature humanoid, allowing it to move faster and more steadily than by walking. Additionally, with these attachments, a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as a platform for experimentation and performance evaluation. This paper discusses preliminary work regarding robot companionship applications using a miniature humanoid that can fetch different toys based on voice commands.",
        "reference": "Robot companionship has become more popular in past years. However, humanoid (robots with human form or characteristics) walking might be unstable. Even with miniature humanoids, falls occur frequently. Wheels have been added onto a miniature humanoid, so it can move faster and with more stability than walking. With wheels, a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used for experimentation and evaluation. This paper discusses preliminary work for robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "metrics": {
          "SARI": 42.69015164016201,
          "BLEU": 0.5277722722294131,
          "FKGL": 10.2
        }
      },
      {
        "source": "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds of up to 6m/sec. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning based navigation. With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest. In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot. Derived from the classic image classification algorithms, our classifier has been constructed in the form of a fast 39-layered Inception model, that evaluates the presence of roads using the tomographic reconstructions of the input frames. Based on the Inception-v3 architecture, our system performs better in terms of processing complexity and accuracy than many existing models for imitation learning. The data used for training the system has been captured from the drone, by flying it in and around urban and semi-urban streets, by experts having at least 6-8 years of flying experience. Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process. With the extensive amount of drone data that we collected, we have been able to navigate successfully through roads without crashing or overshooting, with an accuracy of 98.44%.",
        "prediction": "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds of up to 6m/sec. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning-based navigation. With the ever-increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domains, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest.\n\nTo address these tasks, we propose an imitation learning-based, data-driven solution for UAV autonomy in navigating through city streets by imitating an expert pilot. Our classifier is based on traditional image classification methods and is designed as a fast 39-layer Inception model. It checks for roads by analyzing reconstructed images from the input frames. Based on the Inception-v3 architecture, our system performs better in terms of processing complexity and accuracy than many existing models for imitation learning.\n\nThe data used for training the system has been captured from the drone while flying it in and around urban and semi-urban streets, by experts with at least 6-8 years of flying experience. Permissions were obtained from the necessary authorities to ensure minimal risk to pedestrians during the data collection process. With the extensive amount of drone data collected, we have been able to navigate successfully through roads without crashing or overshooting, achieving an accuracy of 98.44%.",
        "reference": "Current academic and industrial research is interested in autonomous vehicles. MAVNet computational efficiency enables the drone to fly up to 6m/sec. The solution is compared with other recent methods. Drones are increasingly used in the civilian and commercial domain and need to be autonomous. Governments set guidelines on the operation ceiling of civil drones. So, road-tracking based navigation is attracting interest. Researchers propose data-driven solutions allowing drones to autonomously navigate city streets, learning to fly by imitating an expert pilot. The algorithm, based on the Inception model, detects roads using tomographic reconstructions. The Inception model is an extension of classic image classification algorithms. The Inception-v3 architecture has better accuracy than many existing models of imitation learning. The data used for training the system was captured from the drone over urban streets, navigated by an expert pilot. Data collection requires special permission to ensure pedestrian security. The drone can navigate successfully through roads without crashing or overshooting; its accuracy is 98.44%.",
        "metrics": {
          "SARI": 42.65926014016494,
          "BLEU": 0.2063644343830616,
          "FKGL": 14.1
        }
      },
      {
        "source": "Advances in electronics and sensor technology have widened the scopes of networked drones to include applications as diverse as surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording for their operations. Large drones are used singly in missions while small ones are used in formations or swarms. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "prediction": "Improvements in electronics and sensor technology have expanded the uses of networked drones to include various tasks such as surveillance, video recording, operations, entertainment, advertising, signal broadcasting, transportation, and delivery. These applications require video recording for their operations. Large drones are used individually for missions, while small drones are used in formations or swarms. Small drones are proving to be useful in civilian applications. Considering small drones for tasks such as group flight, entertainment, and signal emission has led to the deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulations show that drone formations can effectively display messages.",
        "reference": "Advances in technology allowed drones to be used for surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording. Large drones are used individually while small ones are used in groups. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "metrics": {
          "SARI": 35.51625957184218,
          "BLEU": 0.5167460544129578,
          "FKGL": 11.7
        }
      },
      {
        "source": "This study focuses on the digital marketing capabilities of tourism SMEs. The study addresses the question of how the use of ICT-based tools benefit the organisational capabilities of a company. By adopting marketing as a set-of-skills approach, the study provides new insights into the existing tourism literature on e-marketing. Initial findings indicate that the digital marketing capabilities of companies are transformed through ICT-based uptake. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that the use of ICT-based tools transforms digital marketing capabilities from a set of abilities that enables tourism SMEs not only to float in a web-marketing stream, but also to lead such a stream.",
        "prediction": "This study focuses on the digital marketing capabilities of tourism SMEs. It addresses how ICT-based tools benefit a company's organizational capabilities. The study looks at marketing as a set of skills and helps us understand more about e-marketing in tourism. Initial findings indicate that the digital marketing capabilities of companies are transformed through the use of ICT tools. Four major capabilities were identified, each evolving as a result of using these tools. A key finding is that the use of ICT tools changes digital marketing capabilities from merely allowing tourism SMEs to survive in a web-marketing environment to enabling them to lead that environment",
        "reference": "This study focuses on the digital marketing capabilities of tourism small and medium-sized enterprises (SMEs). The study questions on how information and communication technology (ICT) tools can help the organisational capabilities of a company. The study provides new insights from tourism studies on digital marketing. Findings show that digital marketing capabilities of companies are transformed by ICT tools. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that ICT tools transform these capabilities in a set of abilities for SMEs in web-marketing.",
        "metrics": {
          "SARI": 45.908857844817334,
          "BLEU": 0.3381645479263956,
          "FKGL": 12.4
        }
      },
      {
        "source": "The prevalence and rapid development of the Internet and mobile technology in recent decades has revamped our living styles and daily habits. To ride on the digital trend, more business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. The rise of Key Opinion Leaders (KOLs), social media platforms, and Omni-channel retailing have attracted countless business entities to consider the adoption of digital marketing tools for promoting and advertising their brands and products. However, with the increasing diversity of the types of digital marketing tools, they must be carefully selected based on a multiple number of criterion. In this paper, a fuzzy-AHP method is proposed and developed for assisting industry practitioners in systematically and effectively evaluate and select proper digital marketing tool(s) for adoption. The developed method not only streamlines the internal business process of digital marketing tool selection, but it also increases the practitioner's effectiveness of achieving the pre-defined strategic marketing objectives.",
        "prediction": "The prevalence and rapid development of the Internet and mobile technology in recent decades have revamped our living styles and daily habits. To keep up with the digital trend, more business activities have moved into the digital world. Marketing and advertising are typical areas that have been transformed digitally. The growth of Key Opinion Leaders (KOLs), social media, and Omni-channel retailing has encouraged many businesses to consider using digital marketing tools to promote and advertise their brands and products. However, with the increasing variety of digital marketing tools available, businesses must choose them carefully based on several criteria. In this paper, a fuzzy-AHP method is proposed to help industry practitioners systematically and effectively evaluate and select the right digital marketing tool(s) for adoption. This method not only streamlines the internal processes for selecting digital marketing tools but also enhances practitioners' effectiveness in achieving their pre-defined strategic marketing objectives.",
        "reference": "The development of the Internet and mobile technology in recent decades has modified our living styles and daily habits. More business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. Many businesses consider the adoption of digital marketing tools for their promotion and advertisement, due to the rise of Key Opinion Leaders (person with expert status and influence), social media platforms, and Omni-channel retailing (multiple channels but a uniform experience for the customer). However, with the increasing diversity of digital marketing tools, they must be carefully selected. In this paper, a method to organize and analyze complex decisions is proposed and developed for assisting businesses to evaluate and select appropriate digital marketing tools. The developed method rationalizes and simplifies the process of digital marketing tool selection. It also increases effectiveness of achieving strategic marketing objectives.",
        "metrics": {
          "SARI": 36.41308531972135,
          "BLEU": 0.32939095893653714,
          "FKGL": 15.1
        }
      },
      {
        "source": "In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be primarily contingent on changes in muscle size. Given this assumption, there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength. However, the conclusion that a change in muscle size affects a change in strength is surprisingly based on little evidence. We suggest that these changes may be completely separate phenomena based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with detraining, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load resistance training, yet divergent results in strength.",
        "prediction": "In this paper, we revisit a topic originally discussed in 1955: the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to depend mainly on changes in muscle size. Because of this assumption, a lot of focus has been placed on programs aimed at increasing both muscle size and strength. However, the idea that changing muscle size improves strength is surprisingly supported by very little evidence. We suggest that these changes may be completely separate phenomena based on three points: (1) the weak correlation between changes in muscle size and muscle strength after training; (2) the loss of muscle mass with detraining, yet maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load resistance training, but with differing results in strength.",
        "reference": "In this paper we revisit a topic originally discussed in 1955, namely the lack of evidence that muscle growth plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be related to changes in muscle size. Given this assumption, training programs made for increasing both muscle size and strength were popular. However, the conclusion that a change in muscle size has an influence on strength is surprisingly based on little evidence. We suggest that these changes may be based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with the interruption of training, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load strength training, yet different results in strength.",
        "metrics": {
          "SARI": 41.58578489741067,
          "BLEU": 0.5734701651569435,
          "FKGL": 14.3
        }
      },
      {
        "source": "Low-intensity occlusion (50-100 mm hg) training provides a unique beneficial training mode for promoting muscle hypertrophy. Training at intensities as low as 20% 1 repetition maximum with moderate vascular occlusion results in muscle hypertrophy in as little as 3 weeks. A typical exercise prescription calls for 3 to 5 sets to volitional fatigue with short rest periods. The metabolic buildup causes positive physiologic reactions, specifically a rise in growth hormone that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "prediction": "Low-intensity occlusion (50-100 mm hg) training provides a unique and beneficial way to promote muscle hypertrophy. Training at low intensity, specifically at 20% of the maximum weight someone can lift once, along with some blood flow restriction, can lead to muscle growth in just 3 weeks. A typical exercise plan includes doing 3 to 5 sets of exercises until you feel tired, with short breaks in between. The buildup of substances in the body has positive effects, especially by raising growth hormone levels, which are higher than those seen with more intense exercise. Occlusion training is useful for individuals who cannot handle high loads due to joint pain, postoperative recovery, cardiac rehabilitation, untrained athletes, and astronauts",
        "reference": "Low-intensity occlusion training, which is a training method with blood flow restriction, provides a unique beneficial training mode for muscle growth. Training at intensities as low as 20% of maximum strength with moderate occlusion results in muscle growth in 3 weeks. A typical exercise consists of 3 to 5 sets to muscle failure with short rest periods. The metabolic accumulation causes positive physiologic reactions, specifically a rise in growth hormone, which is one of the main hormone responsible of muscle growth, that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "metrics": {
          "SARI": 30.371954415641994,
          "BLEU": 0.235278267076428,
          "FKGL": 13.5
        }
      },
      {
        "source": "Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. We and others have demonstrated that aerobic exercise acutely and chronically alters protein metabolism and induces skeletal muscle hypertrophy. These findings promote an antithesis to the status quo by providing novel perspective on skeletal muscle mass regulation and insight into exercise-countermeasures for populations prone to muscle loss.",
        "prediction": "Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. We and others have shown that aerobic exercise quickly and over time changes how proteins are used in the body and helps increase muscle size. These findings challenge the conventional view by offering a new perspective on how skeletal muscle mass is regulated and providing insights into exercise strategies for people at risk of muscle loss",
        "reference": "Current belief suggests cardio exercise training has minimal effect on muscle size. We and others have demonstrated that cardio exercise alters protein metabolism (responsible for protein synthesis and muscle growth) and induces muscle growth. These findings against the current belief provide a novel perspective on muscle mass regulation and insight into exercise to prevent muscle loss.",
        "metrics": {
          "SARI": 45.89259542252317,
          "BLEU": 0.20557729789070478,
          "FKGL": 13.4
        }
      },
      {
        "source": "Stretch training is widely used in a variety of fitness-related capacities such as increasing joint range of motion, preventing contractures and alleviating injuries. Moreover, some researches indicate that stretch training may induce muscle hypertrophy; however, studies on the topic have been primarily relegated to animal and in vitro models. The purpose of this brief review was to evaluate whether stretch training is a viable strategy to induce muscle hypertrophy in humans. Of the 10 studies identified, 3 observed some significantly positive effects of stretch training on muscle structure. Intriguingly, in these studies, the stretching was carried out with an apparatus that aided in its performance, or with an external overload. Of the 5 available studies that integrated stretching into a resistance training programme, 2 applied the stretching in the interset rest period and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to confer beneficial changes in muscle size and architecture; alternatively, albeit limited evidence suggests that when stretching is done with a certain degree of tensile strain (particularly when loaded, or added between active muscle contractions) may elicit muscle hypertrophy.",
        "prediction": "Stretch training is widely used in various fitness-related areas such as increasing joint range of motion preventing contractures and alleviating injuries Some research suggests that stretch training might help muscles grow bigger but most studies have been done on animals or in lab settings This brief review aims to evaluate whether stretch training is a viable strategy to promote muscle growth in humans Of the 10 studies identified 3 reported significant positive effects of stretch training on muscle structure Interestingly in these studies stretching was performed with equipment that assisted the process or involved external weights Among the 5 studies that included stretching within a resistance training program 2 utilized stretching during rest periods between sets and these were the studies that showed increased muscle growth In conclusion passive low-intensity stretching does not seem to lead to beneficial changes in muscle size and structure However limited evidence suggests that stretching with some level of tension especially when done with weights or between muscle contractions may encourage muscle growth",
        "reference": "Stretch training is used in a variety of fitness capacities such as increasing flexibility, preventing contractures (shortening of muscle) and reduce injuries. Moreover, some researches indicate that stretch training may induce muscle growth. The purpose of this review was to evaluate if stretch training is a viable strategy to induce muscle growth in humans. Of the 10 studies identified, 3 observed positive effects of stretch training on muscles. In these studies, the stretching was performed with external help like equipment or load. Of the 5 available studies that integrated stretching into strength training, 2 applied the stretching in the rest period between the sets and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to produce beneficial changes in muscles but evidence suggests that when stretching is done with a certain intensity (particularly when loaded, or added between active muscle contractions), it may induce muscle growth.",
        "metrics": {
          "SARI": 32.88793331751162,
          "BLEU": 0.18856968768539137,
          "FKGL": 70
        }
      },
      {
        "source": "Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. Therefore, the purpose of this review is to discuss the effects of cycle training on muscle size and strength of the lower extremity and the possible mechanisms for increasing muscle size with cycle training. It is plausible that cycle training requires a longer period to significantly increase muscle size compared to typical resistance training due to a much slower hypertrophy rate. Cycle training induces muscle hypertrophy similarly between young and older age groups, while strength gain seems to favor older adults, which suggests that the probability for improving in muscle quality appears to be higher in older adults compared to young adults. For young adults, higher-intensity intermittent cycling may be required to achieve strength gains. It also appears that muscle hypertrophy induced by cycle training results from the positive changes in muscle protein net balance.",
        "prediction": "Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. Therefore, the purpose of this review is to discuss the effects of cycle training on muscle size and strength of the lower extremity and the possible mechanisms for increasing muscle size with cycle training. Cycle training may take longer to noticeably increase muscle size than regular weight training because it leads to muscle growth at a slower rate. Cycle training helps increase muscle size in both young and older people, but older adults seem to gain strength more effectively. This means older adults may have a better chance of improving muscle quality than younger adults. For young adults, higher-intensity intermittent cycling may be needed to achieve strength gains. It also appears that muscle hypertrophy from cycle training results from positive changes in muscle protein net balance.",
        "reference": "Cycling training is performed as a major part of any training program which objective is to improve cardiovascular capacities and health. However, the effect of cycling training on muscle size and strength still requires a better comprehension. Therefore, the purpose of this review is to discuss the effects of cycling training on muscle size and strength of the legs and the possible mechanisms for increasing muscle size with cycling training. It is plausible that cycling training requires a longer period to significantly increase muscle size compared to strength training due to a much slower muscle growth rate. Cycling training induces muscle growth similarly between young and older people, while strength gain seems to favor older adults. For young adults, series of high-intensity cycling may be required to achieve strength gains. It also appears that muscle growth induced by cycling training results from the positive changes in protein balance, which is the balance between protein intake which leads to muscle growth and protein loss which leads to muscle loss).",
        "metrics": {
          "SARI": 39.71671673296921,
          "BLEU": 0.3932630917688038,
          "FKGL": 12
        }
      },
      {
        "source": "Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship between training dose/volume and physiological response has been hypothesized to exist. However, the majority of available evidence supports a clear dose-response relationship between resistance training volume and physiological responses, such as muscle hypertrophy and health outcomes. Additionally, there is a paucity of data to support the inverted U-shaped response. The overarching principle argued herein is that volume is the most easily modifiable variable that has the most evidenced-based response with important repercussions, be these muscle hypertrophy or health-related outcomes.",
        "prediction": "Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. There is a proposed idea that too much or too little exercise can lead to different effects on the body's response. However, the majority of available evidence supports a clear relationship between the amount of resistance training and physiological outcomes such as muscle growth and health benefits. Additionally, there is limited data to support the proposed idea. The overarching principle argued herein is that volume is the most easily adjustable factor that has the strongest evidence-based response with important implications for both muscle growth and health-related outcomes.",
        "reference": "Strength training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is helpful for treating and preventing diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship (which means that the efficiency increases until a point, and then starts decreasing) between training volume and physiological response has been hypothesized to exist. However, evidence supports a clear correlation between resistance training volume (number of exercises, sets, etc.) and physiological responses, such as muscle growth and health outcomes. Additionally, there is a lack of data to support the inverted U-shaped response. The principle suggested is that volume is the most easily modifiable variable that has the most important response, for muscle growth or health outcomes.",
        "metrics": {
          "SARI": 38.37179898594301,
          "BLEU": 0.4613422865383364,
          "FKGL": 12.5
        }
      },
      {
        "source": "Although the effects of short versus long inter-set rest intervals in resistance training on measures of muscle hypertrophy have been investigated in several studies, the findings are equivocal and the practical implications remain unclear. Current evidence indicates that both short and long inter-set rest intervals may be useful when training for achieving gains in muscle hypertrophy. Novel findings involving trained participants using measures sensitive to detect changes in muscle hypertrophy suggest a possible advantage for the use of long rest intervals to elicit hypertrophic effects. However, due to the paucity of studies with similar designs, further research is needed to provide a clear differentiation between these two approaches.",
        "prediction": "Although the effects of short and long rest times between sets in weight training on muscle growth have been studied in several research projects, the findings are mixed and the practical implications remain unclear. Current evidence indicates that both short and long rest intervals may be useful for achieving gains in muscle hypertrophy. New findings involving trained participants using sensitive measures to detect changes in muscle hypertrophy suggest a possible advantage for using long rest intervals to promote muscle growth. However, because there are not many studies with similar designs, further research is needed to clearly differentiate between these two approaches.",
        "reference": "The effects of short versus long rest intervals between sets in strength training on muscle growth have been investigated in several studies but the findings and the practical implications are unclear. Current evidence indicates that both short and long rest intervals may be useful for muscle growth. Novel findings suggest for trained people a possible advantage for the use of long rest intervals to induce muscle growth. However, due to the lack of studies with similar methodologies, further research is needed to provide a clear differentiation between these two approaches.",
        "metrics": {
          "SARI": 45.16116525567822,
          "BLEU": 0.408499333881144,
          "FKGL": 14.3
        }
      },
      {
        "source": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review describes a cellular memory in skeletal muscle in which hypertrophy is 'remembered' such that a fibre that has previously been large, but subsequently lost its mass, can regain mass faster than naive fibres. A new cell biological model based on the literature, with the most reliable methods for identifying myonuclei, can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells before hypertrophic growth. Even if subsequently subjected to grave atrophy, the higher number of myonuclei is retained, and the myonuclei seem to be protected against the elevated apoptotic activity observed in atrophying muscle tissue. Fibres that have acquired a higher number of myonuclei grow faster when subjected to overload exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit in the elderly, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "prediction": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review discusses how skeletal muscle can 'remember' its growth. A muscle fiber that was once large but lost some size can grow back faster than a new, untrained fiber. A new cell biological model, based on the literature, provides reliable methods for identifying myonuclei and explains this phenomenon. According to this model, previously untrained fibers recruit myonuclei from activated satellite cells before they begin to grow larger. Even if these fibers later experience significant atrophy, they retain a higher number of myonuclei, which appear to be protected against the increased cell death seen in atrophying muscle tissue. Fibers that have more myonuclei grow faster when subjected to overload exercise, thus making these nuclei a functionally important 'memory' of previous strength. This muscle memory might last a very long time in humans, as myonuclei are stable for at least 15 years and may even be permanent. However, it is harder to recruit myonuclei in the elderly. If long-lasting muscle memory also exists in humans, early strength training should be considered as valuable public health advice.",
        "reference": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, it has been said that it occurs only in the brain. This review describes a cellular memory in muscles in which muscle growth is 'remembered' such that a fibre that has lost its mass, can regain it faster than untrained fibres. A new model, with the most reliable methods for identifying myonuclei (nuclei in muscle fiber), can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells (quiescent cells acting as a reserve population of cells for regeneration of muscle) before muscle growth. Even after grave muscle loss, the higher number of myonuclei is retained, and the myonuclei seem to be protected against programmed cell death observed during muscle loss. Fibres that have acquired a higher number of myonuclei grow faster during strength exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit for old people, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "metrics": {
          "SARI": 33.82248309666771,
          "BLEU": 0.41367335603649824,
          "FKGL": 11.6
        }
      },
      {
        "source": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several observational studies lend support to this hypothesis, with an inverse relationship noted between the frequency of eating and adiposity. The purpose of this narrative review is to present and discuss a meta-analysis with regression that evaluated experimental research on meal frequency with respect to changes in fat mass and lean mass. Feeding frequency was positively associated with reductions in fat mass and body fat percentage as well as an increase in fat-free mass. However, sensitivity analysis of the data showed that the positive findings were the product of a single study, casting doubt as to whether more frequent meals confer beneficial effects on body composition. In conclusion, although the initial results of this meta-analysis suggest a potential benefit of increased feeding frequencies for enhancing body composition, these findings need to be interpreted with circumspection.",
        "prediction": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several studies suggest that this idea is true, showing that eating less often is linked to having more body fat. This review aims to present and discuss a meta-analysis that evaluated experimental research on meal frequency and its effects on changes in fat mass and lean mass. The analysis found that more frequent meals were associated with reductions in fat mass and body fat percentage, as well as an increase in fat-free mass. However, further analysis revealed that these positive findings came mainly from one study, raising questions about whether eating more often truly benefits body composition. In conclusion, while the initial results of this meta-analysis suggest a potential advantage of increased meal frequency for improving body composition, these findings should be considered carefully.",
        "reference": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps for better weight maintenance. Several studies support this hypothesis, with a link between the frequency of eating and being overweight. This review aims to present and discuss a research on meal frequency with respect to changes in body mass. Eating frequency was positively associated with reductions in fat mass and increasing in fat-free mass. However, the positive findings were the product of a single study, leaving a doubt about the beneficial effect of frequent meals on body composition. In conclusion, although the initial results suggest a potential benefit of increased eating frequencies for enhancing body composition, these findings need to be carefully interpreted.",
        "metrics": {
          "SARI": 43.28302698114151,
          "BLEU": 0.34791594751284466,
          "FKGL": 12.6
        }
      },
      {
        "source": "Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by resting metabolic rate reduction and fat accumulation. Ten weeks of resistance training may increase lean weight by 1.4 kg, increase resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of resistance training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Resistance training may assist prevention and management of type 2 diabetes by decreasing visceral fat, reducing HbA1c, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Resistance training may enhance cardiovascular health, by reducing resting blood pressure, decreasing low-density lipoprotein cholesterol and triglycerides, and increasing high-density lipoprotein cholesterol. Resistance training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Resistance training may be effective for reducing low back pain and easing discomfort associated with arthritis and fibromyalgia and has been shown to reverse specific aging factors in skeletal muscle.",
        "prediction": "Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by a reduction in resting metabolic rate and fat accumulation. Ten weeks of resistance training may increase lean weight by 1.4 kg, raise resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. The benefits of resistance training include improved physical performance, better movement control, faster walking speed, greater functional independence, enhanced cognitive abilities, and increased self-esteem. Additionally, resistance training can help prevent and manage type 2 diabetes by lowering body fat around the organs, decreasing HbA1c levels, increasing the number of glucose transporters, and improving how the body uses insulin. It may also enhance cardiovascular health by reducing resting blood pressure, lowering low-density lipoprotein cholesterol and triglycerides, and raising high-density lipoprotein cholesterol. Furthermore, resistance training may promote bone development, with studies showing a 1% to 3% increase in bone mineral density. It can also be effective in reducing low back pain and easing discomfort associated with arthritis and fibromyalgia, and it has been shown to reverse specific aging factors in skeletal muscle.",
        "reference": "Per decade, inactive adults experience a muscle loss of 3% to 8%, a reduction of resting metabolic rate (energy consumed by the body at rest) and fat accumulation. Ten weeks of strength training may increase fat-free mass by 1.4 kg, resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of strength training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Strength training may assist prevention and management of type 2 diabetes by decreasing abdominal fat, reducing HbA1c which is a form of hemoglobin linked to a sugar, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Strength training may enhance cardiovascular health, by reducing resting blood pressure, decreasing 'bad' cholesterol and triglycerides (main constituents of body fat), and increasing 'good' cholesterol. Strength training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Strength training may be effective for reducing low back pain, joint and muscle pain, and reverse specific aging factors in muscles.",
        "metrics": {
          "SARI": 33.284402909909424,
          "BLEU": 0.39407573965298237,
          "FKGL": 14.4
        }
      },
      {
        "source": "We hypothesized that compared to post-exercise protein intake, co-ingestion of alcohol would down-regulate translation initiation signaling and decrease rates of MPS.",
        "prediction": "We hypothesized that, compared to post-exercise protein intake, consuming alcohol along with protein would reduce the processes that start protein production and lower the rate of muscle protein synthesis",
        "reference": "We assumed that consumption of protein and alcohol (compared to protein only) would reduce protein synthesis.",
        "metrics": {
          "SARI": 42.8594584286804,
          "BLEU": 0,
          "FKGL": 18.1
        }
      },
      {
        "source": "Eight healthy physically active male subjects (age 21.4±4.8 yr, body mass (BM) 79.3±11.9 kg, peak oxygen uptake (VO2peak) 48.1±4.8 mL·kg−1·min−1, leg extension one repetition maximum (1RM) 104±20 kg; values are mean ± SD) who had been participating in regular exercise (3 times wk−1 for >6 months) volunteered for this study.",
        "prediction": "Eight healthy physically active male subjects (average age 21 years (with a range of about 4 years) average body weight 79 kg (with a variation of about 12 kg) average peak oxygen uptake 48 mL per kg per minute (with a variation of about 5 mL) and average leg extension strength of 104 kg (with a variation of about 20 kg) who had been participating in regular exercise (3 times a week for more than 6 months) volunteered for this study",
        "reference": "Eight healthy physically active men (3 trainings per week for at least 6 months) volunteered for this study.",
        "metrics": {
          "SARI": 50.73460624744898,
          "BLEU": 0.09789933552269599,
          "FKGL": 32.5
        }
      },
      {
        "source": "The study employed a randomized counter-balanced, cross-over design in which each subject completed bouts of consecutive resistance, continuous and intermittent high-intensity exercise with either post-exercise ingestion of alcohol-carbohydrate (ALC-CHO), alcohol-protein (ALC-PRO) or protein only (PRO) beverages on three separate occasions.",
        "prediction": "The study employed a randomized counter-balanced, cross-over design in which each subject completed bouts of consecutive resistance, continuous and intermittent high-intensity exercise with either post-exercise ingestion of alcohol-carbohydrate, alcohol-protein, or protein only beverages on three separate occasions",
        "reference": "Subjects did high intensity exercise and just after consumed an alcohol-carbohydrate, an alcohol-protein or a protein drink in three separate occasions.",
        "metrics": {
          "SARI": 36.22230255546151,
          "BLEU": 0,
          "FKGL": 27.2
        }
      },
      {
        "source": "Resistance exercise consisted of eight sets of five repetitions at ∼80% of 1RM.",
        "prediction": "Resistance exercise involved performing eight sets of five repetitions using about 80% of the maximum weight one can lift once",
        "reference": "Training consisted of eight sets of five repetitions of leg extension at 80% of their maximum strength.",
        "metrics": {
          "SARI": 49.3309187217729,
          "BLEU": 0.23418123261847468,
          "FKGL": 12.3
        }
      },
      {
        "source": "After completion of the final set, subjects rested for 5 min before commencing 30 min of continuous cycling at ∼63% PPO (∼70% VO2peak).",
        "prediction": "After completion of the final set, subjects rested for 5 minutes before starting 30 minutes of steady cycling at about 63% of their maximum power output around 70% of their peak oxygen uptake",
        "reference": "After 5 minutes rest, subjects cycled for 30 minutes at medium intensity,",
        "metrics": {
          "SARI": 41.65227112965705,
          "BLEU": 0,
          "FKGL": 15
        }
      },
      {
        "source": "Upon completion, subjects rested on the bike for 2 min before undertaking 10×30 s high intensity intervals at ∼110% of PPO, with 30 s active recovery (∼50% PPO) between each work bout.",
        "prediction": "After completing the initial activity, subjects rested on the bike for 2 minutes. They then performed 10 sets of 30 seconds of intense exercise at around 110% of peak performance output, with 30 seconds of light exercise at around 50% of peak performance output between each set.",
        "reference": "then 5 minutes at high intensity.",
        "metrics": {
          "SARI": 35.2030539311241,
          "BLEU": 0,
          "FKGL": 11.3
        }
      },
      {
        "source": "Immediately following exercise and after 4 h recovery, subjects ingested a 500 mL solution of either protein (PRO, 25 g whey protein powder; ISO8, Musashi, Melbourne, VIC Australia) or an energy-match in the form of CHO (25 g maltodextrin, International Health Investments, Helensvale, QLD Australia).",
        "prediction": "Immediately following exercise and after 4 hours of recovery participants drank a 500 mL beverage that contained either 25 g of whey protein powder from Musashi Melbourne Australia or an equal amount of carbohydrates in the form of maltodextrin from International Health Investments Helensvale Australia",
        "reference": "Immediately following exercise and after 4 h recovery, subjects consumed a 500mL drink of protein or carbohydrates.",
        "metrics": {
          "SARI": 46.21468354099272,
          "BLEU": 0.11912074202372534,
          "FKGL": 23.2
        }
      },
      {
        "source": "Furthermore, a CHO-based meal (1.5 g·kg−1 BM) was consumed ∼2 h post-exercise, immediately after the muscle biopsy, according to recommendations for post-exercise glycogen recovery [24].",
        "prediction": "Furthermore a carbohydrate-based meal 1.5 grams per kilogram of body mass was consumed approximately 2 hours after exercise immediately following the muscle biopsy in line with recommendations for recovery of glycogen after exercise 24",
        "reference": "A carbohydrate meal was consumed 2 h after exercise, according to recommendation for glycogen recovery.",
        "metrics": {
          "SARI": 46.76875378100423,
          "BLEU": 0,
          "FKGL": 14.6
        }
      },
      {
        "source": "The 8 h time frame represents an important phase of post-exercise recovery [1] as well as the period during which blood alcohol concentrations are likely to be elevated by a post-event drinking binge [14].",
        "prediction": "The 8 h time frame represents an important phase of post-exercise recovery and is also the time when blood alcohol levels may be higher due to binge drinking after the event",
        "reference": "The 8 h period after exercise is an important phase for recovery but also the period during which blood alcohol level may be the highest after excessive consumption of alcohol.",
        "metrics": {
          "SARI": 43.62314013924658,
          "BLEU": 0,
          "FKGL": 14.2
        }
      },
      {
        "source": "The alcohol ingestion protocol (1.5 g·kg−1 BM; 12±2 standard drinks) began 1 h post-exercise and was consumed in 6 equal volumes of 1 part vodka (∼60 mL) to four parts orange juice (∼240 mL, 1.8 g CHO·kg−1 BM) during a 3 h period.",
        "prediction": "The alcohol ingestion protocol involved consuming 1.5 grams of alcohol for each kilogram of body weight, which amounts to about 12 standard drinks, plus or minus 2 drinks. This protocol began 1 hour after exercise and was consumed in 6 equal portions, consisting of 1 part vodka (approximately 60 mL) mixed with four parts orange juice (approximately 240 mL, providing 1.8 grams of carbohydrates per kilogram of body weight) over a 3-hour period",
        "reference": "The alcohol consumption started 1 h after exercise with 6 drinks (60mL of vodka and 240mL of orange juice) and during 3 h.",
        "metrics": {
          "SARI": 37.6417318067583,
          "BLEU": 0,
          "FKGL": 10.4
        }
      },
      {
        "source": "The focus of the early post-exercise period (i.e., 1–8 h) is to enhance physiological processes that are critical for reversing the exercise-induced disturbances to homeostasis and physiological function and for promoting adaptations to training [1].",
        "prediction": "The focus of the early post-exercise period (i.e., 1–8 h) is to enhance physiological processes that are critical for reversing the exercise-induced disturbances to homeostasis and physiological function and for promoting adaptations to training [1]",
        "reference": "The period after exercise (1-8h) enhances the body's processes that reverse the effects of the exercise and promotes adaptation to the training.",
        "metrics": {
          "SARI": 29.556595365418897,
          "BLEU": 0,
          "FKGL": 16
        }
      },
      {
        "source": "For the PRO condition, orange juice was consumed with a matched volume of water in place of the alcohol.",
        "prediction": "For the PRO condition orange juice was consumed with an equal amount of water instead of the alcohol",
        "reference": "For the protein drink, orange juice was consumed with water instead of alcohol.",
        "metrics": {
          "SARI": 55.76469454178433,
          "BLEU": 0.3026300230972924,
          "FKGL": 8
        }
      },
      {
        "source": "Subjects ingested the beverages within 5 min every 30 min.",
        "prediction": "Subjects drank the beverages within 5 min every 30 min",
        "reference": "Subjects consumed the drinks in 5 minutes every 30 minutes.",
        "metrics": {
          "SARI": 37.202380952380956,
          "BLEU": 0,
          "FKGL": 6
        }
      },
      {
        "source": "Blood, cell signaling and mRNA data were analyzed by two-way ANOVA (two factor: time × treatment) with repeated measures and myofibrillar protein synthesis was analyzed by one-way ANOVA with repeated measures.",
        "prediction": "Blood, cell signaling, and mRNA data were analyzed using a statistical method that examines two factors: time and treatment, while accounting for repeated measurements. Additionally, myofibrillar protein synthesis was analyzed using a one-way ANOVA with repeated measures",
        "reference": "Data (blood, protein synthesis, responses, etc.) were analyzed several times for the 3 different consumptions of drinks.",
        "metrics": {
          "SARI": 38.698701971013186,
          "BLEU": 0,
          "FKGL": 15.2
        }
      },
      {
        "source": "The first novel finding of this study was that mTOR signaling and rates of myofibrillar protein synthesis (MPS) following concurrent resistance, continuous and intermittent high-intensity exercise, designed to mimic the metabolic profile of many team sports, were impaired during the early (8 h) recovery phase by the ingestion of large amounts (1.5 g•kg−1 BM) of alcohol.",
        "prediction": "The first novel finding of this study was that the mTOR signaling pathway and muscle protein synthesis (MPS) after various exercises, including resistance training and high-intensity workouts, were impaired during the early (8 h) recovery phase when large amounts (1.5 g•kg−1 BM) of alcohol were consumed",
        "reference": "The first finding of this study was that the mTOR enzyme (responsible for muscle growth) and protein synthesis (after exercise) were reduced by an excessive alcohol consumption during the 8 h period of recovery.",
        "metrics": {
          "SARI": 49.07319853256033,
          "BLEU": 0.1966040574203662,
          "FKGL": 13.4
        }
      },
      {
        "source": "These outcomes were most evident (37% reduction in rates of MPS) when alcohol was consumed in the absence of post-exercise protein intake, as is likely to occur when intoxication reduces the athlete's compliance to sound recovery practices.",
        "prediction": "These outcomes were most evident showing a 37% decrease in muscle protein synthesis when alcohol is drunk without post-workout protein This situation is likely to occur when intoxication reduces the athlete's adherence to effective recovery practices",
        "reference": "Consequences were most evident when alcohol was consumed without protein (37 % reduction of protein synthesis).",
        "metrics": {
          "SARI": 37.7278581274745,
          "BLEU": 0,
          "FKGL": 20.9
        }
      },
      {
        "source": "However, a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate MPS [8] during post-exercise recovery, the intake of alcohol reduced MPS by ∼24%, representing only a partial ‘rescue’ of the anabolic response compared with protein alone.",
        "prediction": "However a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate MPS during post-exercise recovery the consumption of alcohol lowered MPS by about 24% This means it only partially improved the muscle growth response compared to having protein on its own",
        "reference": "A second finding was that even with protein consumption, alcohol reduced protein synthesis by around 24%.",
        "metrics": {
          "SARI": 44.45877699606232,
          "BLEU": 0.08801997699764694,
          "FKGL": 24
        }
      },
      {
        "source": "The mechanistic target of rapamycin complex 1 (mTORC1) is a central node for integrating nutrient (i.e. amino acid) and exercise/contraction signal transduction [31], [32].",
        "prediction": "The mTORC1 complex is a central part of integrating nutrient (like amino acids) signals and exercise-related signals in the communication pathways within the body",
        "reference": "mTOR is important in absorption of nutrients (like amino acids) and in sending signals to the body to produce an effort.",
        "metrics": {
          "SARI": 53.39743191145379,
          "BLEU": 0.17797644045771208,
          "FKGL": 16.2
        }
      },
      {
        "source": "In conclusion, the current data provide the novel observation that alcohol impairs the response of MPS in exercise recovery in human skeletal muscle despite optimal nutrient provision.",
        "prediction": "In conclusion the current data provide the novel observation that alcohol reduces the effect of muscle protein synthesis during recovery after exercise even when nutrients are optimally provided",
        "reference": "The data from this study show that alcohol reduce the protein synthesis after exercise even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 43.12457166837245,
          "BLEU": 0,
          "FKGL": 20.1
        }
      },
      {
        "source": "The quantity of alcohol consumed in the current study was based on amounts reported during binge drinking by athletes.",
        "prediction": "The quantity of alcohol consumed in the current study was based on amounts reported during heavy drinking episodes by athletes",
        "reference": "The quantity of alcohol consumed in this study was based on consumption by athletes during binge drinking.",
        "metrics": {
          "SARI": 41.04089009375353,
          "BLEU": 0.40157733283424196,
          "FKGL": 11.1
        }
      },
      {
        "source": "However, published reports suggest intakes of some individuals can be significantly greater [9], [50], which is of concern for many reasons related to health and safety [13].",
        "prediction": "However published reports suggest that some people take in much more which raises concerns for many reasons related to health and safety",
        "reference": "However, reports show that some individuals can consume higher quantities, which is a concern for health and safety issues.",
        "metrics": {
          "SARI": 37.866559333724645,
          "BLEU": 0,
          "FKGL": 10.7
        }
      },
      {
        "source": "Recommended nutritional strategies to maximize recovery in skeletal muscle include protein for enhancing rates of protein synthesis and carbohydrate for replenishing glycogen stores [2],[3].",
        "prediction": "Suggested methods to help muscles recover include protein for enhancing rates of protein synthesis and carbohydrates to help with muscle energy [2],[3]",
        "reference": "Nutritional strategies to optimize muscle recovery include proteins which are recommended to allow protein synthesis (production of new proteins which improve, for example, muscle mass) and carbohydrates to recharge glycogen stores (energy storage).",
        "metrics": {
          "SARI": 34.22347905626246,
          "BLEU": 0,
          "FKGL": 15.4
        }
      },
      {
        "source": "Regrettably, there has been difficulty in finding an educational message with alcohol consumption related to sports performance that has resonance with athletes.",
        "prediction": "Regrettably there has been difficulty in finding a message about how drinking alcohol affects athletic performance that resonates with athletes",
        "reference": "Regrettably, it is difficult to find an educational message about alcohol consumption and sport performances that speaks to athletes.",
        "metrics": {
          "SARI": 30.851921997755326,
          "BLEU": 0,
          "FKGL": 14.6
        }
      },
      {
        "source": "Given the need to promote protein synthesis that underpins adaptation, repair and regeneration of skeletal muscle the results of the current study provide clear evidence of impaired recovery when alcohol is consumed after concurrent (resistance, continuous and intermittent high-intensity) exercise even in the presence of optimal nutritional conditions.",
        "prediction": "Given the need to promote protein synthesis that supports adaptation repair and regeneration of skeletal muscle the results of the current study provide clear evidence of reduced recovery when alcohol is consumed after different types of exercise including weightlifting steady exercise and high-intensity bursts even in the presence of optimal nutrition",
        "reference": "Considering the importance of protein synthesis (adaptation, repair and regeneration of muscles), the results of this study provide evidence of reduced recovery when alcohol is consumed after exercise, even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 47.76418559389642,
          "BLEU": 0.26220676436185975,
          "FKGL": 26.7
        }
      },
      {
        "source": "We propose our data is of paramount interest to athletes and coaches.",
        "prediction": "We propose our data is very important to athletes and coaches",
        "reference": "We consider our data crucial for athletes and coaches.",
        "metrics": {
          "SARI": 40.18037518037518,
          "BLEU": 0,
          "FKGL": 6.4
        }
      },
      {
        "source": "Our findings provide an evidence-base for a message of moderation in alcohol intake to promote recovery after exercise with the potential to alter current sports culture and athlete practices.",
        "prediction": "Our findings support a balanced approach to drinking alcohol to aid recovery after exercise which could change current sports culture and athlete practices",
        "reference": "Our findings provide evidence for a message of moderation in alcohol consumption to promote recovery after exercise, with the potential to change sport cultures and athlete practices.",
        "metrics": {
          "SARI": 26.77331649344455,
          "BLEU": 0,
          "FKGL": 13.4
        }
      },
      {
        "source": "Muscle contraction and the intake of leucine-rich protein sources activate independent but complimentary signaling responses that converge at the mechanistic target of rapamycin (mTOR) to stimulate protein translation enhancing rates of muscle protein synthesis [4]–[6].",
        "prediction": "Muscle contraction and consuming proteins that are high in leucine cause different but helpful signals that meet at a key point called the mechanistic target of rapamycin mTOR which stimulates protein translation and enhances muscle protein synthesis rates 4–6",
        "reference": "Muscle contraction and consumption of leucine-rich protein (essential amino acid composing proteins) induce reactions which increase the protein synthesis.",
        "metrics": {
          "SARI": 41.57176554674622,
          "BLEU": 0,
          "FKGL": 18.5
        }
      },
      {
        "source": "The ingestion of ∼20–25 g of high quality protein soon after exercise [7], repeated every 4 h [8] has been shown to maximise the anabolic response in skeletal muscle.",
        "prediction": "Consuming about 20 to 25 grams of good protein soon after exercise [7] and repeating this every 4 hours [8] has been shown to increase muscle growth",
        "reference": "Consumption of 20-25 g of high quality protein after exercise and repeated every 4 h showed an optimal anabolic response (increasing muscle mass) of the muscles.",
        "metrics": {
          "SARI": 29.804883069828726,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "The cultural environment surrounding some sports often involves the intake of large amounts of alcohol after training and competition, with athletes in several team sports being particularly at risk of “binge drinking” practices [9]–[11].",
        "prediction": "The cultural environment surrounding some sports often involves drinking a lot of alcohol after training and competitions. Athletes in several team sports are particularly at risk of engaging in binge drinking practices",
        "reference": "Several studies showed that athletes are more likely to drink alcohol to excess (50-65% drink above dangerous level).",
        "metrics": {
          "SARI": 36.56410256410256,
          "BLEU": 0,
          "FKGL": 13.1
        }
      },
      {
        "source": "The outcomes of binge drinking after exercise are likely to include the direct effect of alcohol on physiological processes as well as the indirect effect on the athlete's recovery due to not eating or resting adequately as a result of intoxication.",
        "prediction": "The outcomes of binge drinking after exercise are likely to include the immediate impact of alcohol on the body and how it indirectly affects the athlete's recovery by preventing proper eating or resting due to intoxication",
        "reference": "Consequences of an excessive consumption after exercise are direct effect on the body but also indirect effect on the recovery due to an inappropriate eating and rest.",
        "metrics": {
          "SARI": 37.25701790879714,
          "BLEU": 0,
          "FKGL": 19.7
        }
      },
      {
        "source": "Although the concurrent consumption of carbohydrate can partially offset the deleterious effects of alcohol intake on post-exercise glycogen resynthesis [14], the effect of alcohol consumption on muscle protein synthesis is unknown.",
        "prediction": "Although eating carbohydrates at the same time can help reduce the negative effects of drinking alcohol on the recovery of energy stores in muscles after exercise, we still don't know how alcohol consumption affects muscle protein synthesis",
        "reference": "Consumption of carbohydrates can partially counter balance the effects of alcohol on the recharge of glycogen stores but the effect on muscle protein synthesis are unknown.",
        "metrics": {
          "SARI": 43.533102030805566,
          "BLEU": 0,
          "FKGL": 20.1
        }
      },
      {
        "source": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis (MPS) in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.",
        "prediction": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis (MPS) in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.",
        "reference": "The aim of this study was to determine the effect of alcohol on anabolism and protein synthesis in humans during recovery following an effort an athlete could do (in rugby or football for example).",
        "metrics": {
          "SARI": 45.236471389422825,
          "BLEU": 0.19616278811348586,
          "FKGL": 27.5
        }
      },
      {
        "source": "Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the age-associated loss of muscle mass and is one of the main contributors to musculoskeletal impairments in the elderly. Previous research has demonstrated that resistance training can attenuate skeletal muscle function deficits in older adults, however few articles have focused on the effects of resistance training on functional mobility. The purpose of this systematic review was to 1) present the current state of literature regarding the effects of resistance training on functional mobility outcomes for older adults with skeletal muscle function deficits and 2) provide clinicians with practical guidelines that can be used with seniors during resistance training, or to encourage exercise. We set forth evidence that resistance training can attenuate age-related changes in functional mobility, including improvements in gait speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in progressive resistance training activities, and should be admonished to move along a continuum of exercise from immobility, toward the recommended daily amounts of activity.",
        "prediction": "Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the loss of muscle mass that happens as people age and contributes to problems with bones and muscles in older adults. Previous research has shown that resistance training can help reduce muscle function issues in older adults; however, few articles have focused on how resistance training affects their ability to move. The purpose of this systematic review was to 1) present the current literature on the effects of resistance training on functional mobility for older adults with muscle function issues and 2) provide clinicians with practical guidelines for using resistance training with seniors or encouraging them to exercise. We provide evidence that resistance training can improve age-related changes in functional mobility, including gait speed, balance (both static and dynamic), and reducing the risk of falls. Older adults should be encouraged to engage in progressive resistance training activities and should be guided to gradually increase their activity levels from immobility to the recommended daily amounts of exercise.",
        "reference": "Human aging results in a variety of changes to muscles. Sarcopenia is the age-associated muscle loss and is one of the main contributors to muscle disorders in older adults. Previous research has demonstrated that strength training can attenuate muscle function deficits in older adults, however few articles have focused on the effects of strength training on mobility. The purpose of this review was to present the effects of strength training on mobility for older adults with muscle function deficits and to provide guidelines that can be used with seniors during strength training. We present evidence that strength training can attenuate age-related changes in mobility, including improvements in walking speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in strength training activities, and should avoid immobility.",
        "metrics": {
          "SARI": 37.126078946009535,
          "BLEU": 0.27305551375945464,
          "FKGL": 16.6
        }
      },
      {
        "source": "Maximizing the hypertrophic response to resistance training (RT) is thought to be best achieved by proper manipulation of exercise program variables including exercise selection, exercise order, length of rest intervals, intensity of maximal load, and training volume. An often overlooked variable that also may impact muscle growth is repetition duration. Duration amounts to the sum total of the concentric, eccentric, and isometric components of a repetition, and is predicated on the tempo at which the repetition is performed. We conducted a systematic review and meta-analysis to determine whether alterations in repetition duration can amplify the hypertrophic response to RT. Results indicate that hypertrophic outcomes are similar when training with repetition durations ranging from 0.5 to 8 s. From a practical standpoint it would seem that a fairly wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at volitionally very slow durations (>10s per repetition) is inferior from a hypertrophy standpoint, although a lack of controlled studies on the topic makes it difficult to draw definitive conclusions.",
        "prediction": "Maximizing muscle growth from resistance training (RT) is thought to be best achieved by carefully managing various factors such as which exercises you do, the order in which you perform them, how long you rest between sets, how heavy the weights are, and the overall training volume. An often overlooked variable that may also impact muscle growth is repetition duration. This refers to the total time taken in the concentric, eccentric, and isometric phases of a repetition, influenced by the tempo at which the repetition is performed. We conducted a systematic review and meta-analysis to determine whether changes in repetition duration can enhance the muscle growth response to RT. Results indicate that muscle growth outcomes are similar when training with repetition durations ranging from 0.5 to 8 seconds. Practically, it seems that a fairly wide range of repetition durations can be used if the main goal is to maximize muscle growth. Findings suggest that training at very slow durations greater than 10 seconds per repetition may not be as effective for muscle growth although a lack of controlled studies on this topic makes it difficult to draw definitive conclusions",
        "reference": "Maximizing the muscle growth following strength training can be done by manipulating variables like exercise selection, exercise order, rest intervals, intensity of maximal load, and training volume (number of exercises, sets, etc.). A neglected variable that also may impact muscle growth is repetition duration. Total duration of a repetition is the sum of the contraction, the lengthening and isometric (static contraction) and is based on the tempo. We conducted a review and analysis of multiple studies to determine if modification of repetition duration can amplify the muscle growth. Results indicate that muscle growth is similar when training with repetition durations from 0.5 to 8 s. Using a wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at very slow durations (>10s per repetition) is less efficient for muscle growth, although a lack of studies on the topic makes it difficult to draw definitive conclusions.",
        "metrics": {
          "SARI": 39.4782485219302,
          "BLEU": 0.30511044261368203,
          "FKGL": 12.5
        }
      },
      {
        "source": "Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also refers to specific physiological processes or states occurring after exercise that are distinct from the physiology of either the exercising or the resting states. In this context, recovery of the cardiovascular system after exercise occurs across a period of minutes to hours, during which many characteristics of the system, even how it is controlled, change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. Furthermore, some of these changes may provide insight into when the cardiovascular system has recovered from prior training and is physiologically ready for additional training stress. This review focuses on the most consistently observed hemodynamic adjustments and the underlying causes that drive cardiovascular recovery and will highlight how they differ following resistance and aerobic exercise. Primary emphasis will be placed on the hypotensive effect of aerobic and resistance exercise and associated mechanisms that have clinical relevance, but if left unchecked, can progress to symptomatic hypotension and syncope. Finally, we focus on the practical application of this information to strategies to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities of this state. We will explore appropriate field measures, and discuss to what extent these can guide an athlete's training.",
        "prediction": "Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also includes specific physiological processes or states that occur after exercise, which are distinct from both the exercising and resting states. In this context, recovery of the cardiovascular system after exercise occurs over a period of minutes to hours, during which many characteristics of the system, including its control mechanisms, change over time. Some of these changes may be necessary for long-term adaptation to exercise training, while others can lead to cardiovascular instability during recovery. Additionally, some changes can help us understand when the cardiovascular system has recovered from earlier training and is ready for more exercise.\n\nThis review examines the most common changes in blood flow and the underlying reasons during cardiovascular recovery, emphasizing how recovery differs between resistance and aerobic exercise. The main focus will be on the blood pressure-lowering effects of both types of exercise, along with the related mechanisms that are important in a clinical setting. If left unchecked, these effects can progress to symptomatic hypotension and syncope. Finally, we will discuss the practical application of this information to strategies aimed at maximizing the benefits of cardiovascular recovery or minimizing vulnerabilities during this state. We will explore appropriate field measures and consider how they can guide an athlete's training.",
        "reference": "Recovery from exercise refers to the time period between the end of exercise and the return to a resting or recovered state. It also refers to specific physiological processes occurring after exercise. Recovery of the cardiovascular system occurs across a period of minutes to hours, during which many characteristics of the system change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. These changes may provide insight into when the cardiovascular system has recovered and is physiologically ready for additional training. This review focuses on the most consistently observed blood flow adjustments and the causes that drive cardiovascular recovery and how they differ following strength and cardio exercise. First we focus on the effect leading to low blood pressure of cardio and strength exercise and associated mechanisms, which can progress to symptomatic hypotension (low blood pressure) and fainting. Finally, we focus on the practical application to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities. We will explore measures, and discuss how these can guide an athlete's training.",
        "metrics": {
          "SARI": 41.56723069733024,
          "BLEU": 0.3777624521614537,
          "FKGL": 14.6
        }
      },
      {
        "source": "Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed \"exercise as medicine\". Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often overlooked as a target for exercise-based therapy. Despite the vast range of severity in kidney disease (e.g., pre-dialysis, dialysis, transplant), exercise has a potential role in all patients suffering from the condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'.",
        "prediction": "Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed \"exercise as medicine.\" Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, which affects about 10% of people, is often overlooked as a target for exercise therapy. Despite the wide range of severity in kidney disease (e.g., pre-dialysis, dialysis, transplant), exercise can play a beneficial role for all patients suffering from this condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'",
        "reference": "Exercise and physical activity are key tools in the treatment and prevention of several medical conditions including joint disorders and diabetes. Exercise can reduce cardiovascular risk, inflammation, illness causing muscle loss, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often neglected as a target for exercise-based therapy. Despite the vast range of severity in kidney disease, exercise has a potential role in all patients suffering from the condition. In this review, we summarise the potential important role of exercise in the management of kidney disease and how this form of 'medicine' should be used.",
        "metrics": {
          "SARI": 26.801023910727253,
          "BLEU": 0.48316018094947244,
          "FKGL": 15
        }
      },
      {
        "source": "Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The pooled standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% CI=-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.",
        "prediction": "Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The pooled standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% CI=-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.",
        "reference": "Exercise with blood flow restriction (BFR) is used to increase strength in healthy individuals. However, its effects on individuals with knee pain are unknown. The objective is to determine the effectiveness of adding BFR to strength exercise for pain relief and improvement of function in patients with knee pain. We conducted a review and analysis of multiple studies. Studies that compared strength exercise with or without BFR to treat knee pain and function. The results showed that resistance exercises with BFR was not more effective than strength exercises for reducing pain and improving knee function in patients with knee pain.",
        "metrics": {
          "SARI": 53.695739894623216,
          "BLEU": 0.4291712856458387,
          "FKGL": 9.7
        }
      },
      {
        "source": "In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports). In this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review recommends safe and effective whole-body aerobic exercise at moderate intensity (i.e., 50-65% of maximum oxygen intake, 30-60 min per session, 3-4 times a week) that primarily focuses on the major muscle groups for the prevention and treatment of hypertension. Resistance exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but resistance exercise is contraindicated in patients with hypertension who have chest symptoms such as chest pain.",
        "prediction": "In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports).\n\nIn this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review suggests safe and effective full-body aerobic exercise at a moderate intensity (about 50-65% of maximum oxygen use, for 30-60 minutes per session, 3-4 times a week) that mainly targets the major muscle groups to help prevent and treat hypertension. Additionally, resistance exercise should be performed at low intensity without breath-holding and should be used as supplementary exercise; however, it is contraindicated for patients with hypertension who have chest symptoms such as chest pain.",
        "reference": "In Japan, 43 million patients had high blood pressure in 2010. The management of this condition and the importance of lifestyle changes for the prevention and treatment of high blood pressure has been recognized in Japan. In particular, increasing the levels of activities of daily living and physical exercise. In this review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of high blood pressure. This review recommends full body cardio exercise at moderate intensity (i.e., 50-65% of maximum intensity, 30-60 min per session, 3-4 times a week) that focuses on the major muscle groups. Strength exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but it is contraindicated in patients with high blood pressure who have chest symptoms such as chest pain.",
        "metrics": {
          "SARI": 38.24080940919355,
          "BLEU": 0.5365208446271468,
          "FKGL": 15
        }
      },
      {
        "source": "Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common sense advices call for late inception of intense, strength training-related activities, like weight lifting and plyometrics, which are usually postponed at the end of the growth age, even among sport practitioners. However, such advices seem to have a mainly anecdotal nature. Current literature does not seem to have any particular aversion against the practice of strength training by children and adolescents, provided that some safety rules are followed, like medical clearance, proper instruction from a qualified professional and progressive overload. At the same time, several studies provide consistent findings supporting the benefits of repeated, intense physical efforts in young subjects. Improved motor skills and body composition, in terms of increased fat free mass, reduced fat mass and enhanced bone health, have been extensively documented, especially if sport practice began early, when the subjects were pubescent. It can be therefore concluded that strength training is a relatively safe and healthy practice for children and adolescents.",
        "prediction": "Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common advice suggests that intense strength training activities like weight lifting and plyometrics should be started later in life usually after growth has finished even for athletes. However, this advice often relies on personal beliefs rather than solid evidence. Current literature does not seem to oppose strength training for children and adolescents as long as safety measures are followed such as obtaining medical clearance receiving proper instruction from a qualified professional and ensuring progressive overload. Additionally, several studies consistently support the benefits of repeated intense physical efforts in young individuals. Improved motor skills and body composition including increased fat-free mass reduced fat mass and better bone health have been well-documented especially when sports practice begins early during puberty. Therefore, it can be concluded that strength training is a relatively safe and healthy practice for children and adolescents.",
        "reference": "Physical activity has many benefits like preventing diseases and improving general health. In popular opinion, it is recommended to start intense physical activities like weight lifting and plyometrics (exercises like sprint and jumps) after adolescence. But this advice does not seem to be based on facts. We did not find any recommendation against strength training at a young age if it is well supervised with safety rules, medical clearance, proper instruction from a professional and progressive overload (gradual increase of stress on muscle and nervous system). Several studies showed many benefits of repeated, intense physical efforts in young people. Improved motor skills and body composition (muscle mass increased, fat mass decreased, healthier bone) were found in studies, especially if sport practice began during puberty. We can conclude that strength training is a safe and healthy practice for children and adolescents.",
        "metrics": {
          "SARI": 52.02307232414077,
          "BLEU": 0.265667765004514,
          "FKGL": 15.4
        }
      },
      {
        "source": "recently, there has been a renewed public interest in IFast. Given the importance of nutrition in optimizing athletic performance, there is a concern about the effects of IFast on athletics. Looking at high-intensity, endurance, and resistance exercises, studies have been varied but are uniform in showing that there is no benefit to athletic performance while fasting.",
        "prediction": "Recently there has been renewed public interest in IFast Since nutrition is important for improving athletic performance there are concerns about the effects of IFast on athletes Studies on high-intensity endurance and resistance exercises have shown varied results but consistently indicate that there is no benefit to athletic performance while fasting",
        "reference": "Nowadays, people are interested in intermittent fasting (an eating pattern where you cycle between periods of eating and fasting) Nutrition is important for the optimization of sport performance so there is a concern about the effects of intermittent fasting on performance. Studies showed that there are no benefit in high-intensity, endurance or strength training while fasting.",
        "metrics": {
          "SARI": 40.62866785446912,
          "BLEU": 0.08436598682586012,
          "FKGL": 26.7
        }
      },
      {
        "source": "Nearly every physically active person encounters periods in which the time available for exercise is limited (e.g., personal, family, or business conflicts). During such periods, the goal of physical training may be to simply maintain (rather than improve) physical performance. Similarly, certain special populations may desire to maintain performance for prolonged periods, namely athletes (during the competitive season and off-season) and military personnel (during deployment). In general populations, endurance performance can be maintained for up to 15 weeks when training frequency is reduced to as little as 2 sessions per week or when exercise volume is reduced by 33–66% (as low as 13–26 minutes per session), as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size (at least in younger populations) can be maintained for up to 32 weeks with as little as 1 session of strength training per week and 1 set per exercise, as long as exercise intensity (relative load) is maintained; whereas, in older populations, maintaining muscle size may require up to 2 sessions per week and 2–3 sets per exercise, while maintaining exercise intensity. Our primary conclusion is that exercise intensity seems to be the key variable for maintaining physical performance over time, despite relatively large reductions in exercise frequency and volume.",
        "prediction": "Nearly every physically active person encounters periods in which the time available for exercise is limited e.g., personal, family, or business conflicts. During such times, the goal of physical training may be to simply maintain physical performance rather than improve it. Similarly, certain special populations, such as athletes during the competitive season and off-season, and military personnel during deployment, may also want to maintain their performance for longer periods. \n\nFor general populations, endurance performance can be kept up for up to 15 weeks if people reduce their training to just 2 sessions a week or lower their exercise volume by 33–66% to 13–26 minutes per session, as long as they maintain exercise intensity heart rate. Strength and muscle size can be maintained for up to 32 weeks in younger populations with just 1 session of strength training per week and 1 set per exercise, provided they keep the exercise intensity relative load high. In older populations, maintaining muscle size may require up to 2 sessions per week and 2–3 sets per exercise while also keeping the intensity up.\n\nOur main conclusion is that exercise intensity is the key factor for maintaining physical performance over time, even with significant reductions in exercise frequency and volume.",
        "reference": "Physically active people encounter periods with limited time available for exercise. During such periods, the goal of training may be to simply maintain physical performance. Some people may also desire to maintain performance for prolonged periods, like athletes during competitive season or militaries during deployment. Endurance performance can be maintained for up to 15 weeks with 2 trainings per week, or with a reduction of 13–26 minutes per training, as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size can be maintained for up to 32 weeks with 1 strength training per week and 1 set per exercise, as long as exercise intensity (load) is maintained. In older populations, maintaining muscle size may require up to 2 trainings per week and 2–3 sets per exercise, while maintaining exercise intensity. Exercise intensity seems to be the key variable for maintaining physical performance, even with reduction of frequency and volume (number of exercises, sets, etc.)",
        "metrics": {
          "SARI": 43.14713153881065,
          "BLEU": 0.321027060926006,
          "FKGL": 14.4
        }
      },
      {
        "source": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients--primarily protein and carbohydrate--in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been postulated that the timing of nutritional consumption may be more important than the absolute daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. Theoretically, consuming the proper ratio of nutrients during this time not only initiates the rebuilding of damaged muscle tissue and restoration of energy reserves, but it does so in a supercompensated fashion that enhances both body composition and exercise performance. Several researchers have made reference to an anabolic “window of opportunity” whereby a limited time exists after training to optimize training-related muscular adaptations. However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Not only is nutrient timing research open to question in terms of applicability, but recent evidence has directly challenged the classical view of the relevance of post-exercise nutritional intake with respect to anabolism.",
        "prediction": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients—primarily protein and carbohydrate—in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been suggested that the timing of nutritional intake may be more important than the overall daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. Basically, eating the right mix of nutrients after exercise helps repair muscle and restore energy. This timing also improves body shape and exercise ability. Several researchers have referred to an anabolic “window of opportunity” in which there is limited time after training to optimize muscular adaptations. However, the importance—and even the existence—of a post-exercise ‘window’ can vary based on several factors. Nutrient timing research is still being evaluated, and recent evidence has directly challenged the traditional view of the significance of post-exercise nutritional intake for building muscle.",
        "reference": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients around an exercise session. Some people say that this approach can produce dramatic improvements in body composition. It has even been hypothesized that the timing may be more important than the total daily intake of nutrients. The post-exercise period is considered the most important part of nutrient timing. In theory, consuming the proper ratio of nutrients during this time allows the rebuilding of damaged muscle tissue and restoration of energy reserves, but also enhances both body composition and exercise performance. Researchers have made reference to an anabolic “window of opportunity” (a limited time after training to optimize muscular adaptations). However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Research is uncertain about the applicability of nutrient timing. Besides, the relevance of post-exercise nutritional intake concerning anabolism (increasing muscle mass) has been challenged by recent evidence.",
        "metrics": {
          "SARI": 42.15708336943428,
          "BLEU": 0.4136493489484759,
          "FKGL": 13.7
        }
      },
      {
        "source": "Lack of time is among the more commonly reported barriers for abstention from exercise programs. The aim of this review was to determine how strength training can be most effectively carried out in a time-efficient manner by critically evaluating research on acute training variables, advanced training techniques, and the need for warm-up and stretching. When programming strength training for optimum time-efficiency we recommend prioritizing bilateral, multi-joint exercises that include full dynamic movements (i.e. both eccentric and concentric muscle actions), and to perform a minimum of one leg pressing exercise (e.g. squats), one upper-body pulling exercise (e.g. pull-up) and one upper-body pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights based on training goals, availability, and personal preferences. Weekly training volume is more important than training frequency and we recommend performing a minimum of 4 weekly sets per muscle group using a 6–15 RM loading range (15–40 repetitions can be used if training is performed to volitional failure). Advanced training techniques, such as supersets, drop sets and rest-pause training roughly halves training time compared to traditional training, while maintaining training volume. However, these methods are probably better at inducing hypertrophy than muscular strength, and more research is needed on longitudinal training effects. Finally, we advise restricting the warm-up to exercise-specific warm-ups, and only prioritize stretching if the goal of training is to increase flexibility.",
        "prediction": "Lack of time is one of the most common barriers to participating in exercise programs The aim of this review was to determine how strength training can be effectively carried out in a time-efficient manner by evaluating research on training variables advanced techniques and the necessity of warm-up and stretching\n\nWhen planning strength training to save time we suggest focusing on exercises that use both sides of the body and involve multiple joints You should do at least one leg exercise like squats one upper-body pulling exercise like pull-ups and one upper-body pushing exercise like bench presses Exercises can be performed with machines and/or free weights depending on your training goals available equipment and personal preferences\n\nWeekly training volume is more important than training frequency so we recommend performing a minimum of 4 sets per muscle group each week using a loading range of 6–15 repetitions 15–40 repetitions can be used if training is performed to volitional failure Advanced training techniques such as supersets drop sets and rest-pause training can roughly halve overall training time compared to traditional training while maintaining training volume However these methods may be better at promoting muscle growth hypertrophy than increasing muscle strength and more research is needed on their long-term effects\n\nFinally we advise limiting the warm-up to exercises that are specific to your workout and to prioritize stretching only if your goal is to increase flexibility",
        "reference": "Lack of time is one of the most common reason for not exercising. The aim of this review was to determine how strength training can be time-efficient by evaluating research on training variables, advanced training techniques, and the need for warm-up and stretching. We recommend choosing bilateral (with the two arms or legs), multi-joint (multiple joints involved) exercises that include full dynamic movements and to perform a minimum of one leg pressing exercise (e.g. squats), one pulling exercise (e.g. pull-up) and one pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights. Weekly training volume (number of exercises, sets, etc.) is more important than training frequency and we recommend a minimum of 4 weekly sets per muscle group using a load that permits to do 6 to 15 repetitions. Advanced training techniques like combining exercises with no rest or shortening the rest time while using lighter weights can cuts the training time in half, while maintaining training volume. However, these methods are probably better to increase muscle mass than strength. Finally, we advise exercise-specific warm-ups, and stretching only if the goal of training is to increase flexibility.",
        "metrics": {
          "SARI": 37.275190386819816,
          "BLEU": 0.2712095289283005,
          "FKGL": 95.3
        }
      },
      {
        "source": "Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequency on a volume-equated basis. Meta-regression analysis of non-volume-equated studies showed a significant effect favoring higher frequencies, although the overall difference in magnitude of effect between frequencies of 1 and 3+ days per week was modest. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "prediction": "Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequencies when the training volume was kept the same. Analysis of studies that did not control for training volume showed a notable benefit for higher training frequencies, but the difference in effect between training once a week and three or more times a week was small. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when the volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency for muscle groups based on personal preference.",
        "reference": "Training frequency is an important variable for hypertrophy (increasing muscle mass) in strength training. The purpose of this paper was to conduct a review and analysis of multiple studies investigating the effects of weekly training frequency on hypertrophy. Results showed no significant difference between higher and lower frequency with the same training volume (number of exercises, sets, etc.). Analysis of studies with different training volumes showed better results for higher frequencies, although the difference between frequencies of 1 and 3+ days per week was small. In conclusion, there is strong evidence that resistance training frequency does not significantly impact muscle hypertrophy when training volume is the same. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "metrics": {
          "SARI": 47.77729726798752,
          "BLEU": 0.4857298161976296,
          "FKGL": 14.7
        }
      },
      {
        "source": "A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Forced repetitions/drop sets, supersets, and heavy negatives, in particular, have been purported to enhance the hypertrophic response to resistance exercise. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to resistance training programs.",
        "prediction": "A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Techniques like forced repetitions, drop sets, supersets, and heavy negatives are said to help increase muscle growth through resistance training. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide insight into possible applications for resistance training programs.",
        "reference": "A variety of training techniques have been suggested to support muscle growth. Forced repetitions/drop sets (use lighter weight or receiving help to continue the set), supersets (combining exercises with no rest), and heavy negatives (lowering phase of the lift), are pretended to enhance hypertrophy (increasing muscle mass). This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to strength training.",
        "metrics": {
          "SARI": 44.25943872196393,
          "BLEU": 0.31402406522179843,
          "FKGL": 13.5
        }
      },
      {
        "source": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. Although both groups are known to display impressive muscularity, it is not clear which method is superior for hypertrophic gains. It has been shown that many factors mediate the hypertrophic process and that mechanical tension, muscle damage, and metabolic stress all can play a role in exercise-induced muscle growth. Therefore, the purpose of this paper is twofold: (a) to extensively review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and (b) to draw conclusions from the research as to the optimal protocol for maximizing muscle growth.",
        "prediction": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders usually lift moderate weights and take short breaks, which causes a lot of metabolic stress. In contrast, powerlifters routinely train with high-intensity loads and take lengthy rest periods between sets. Although both groups are known for their impressive muscularity, it is not clear which method is better for gaining muscle size. Many factors influence the muscle growth process, including mechanical tension, muscle damage, and metabolic stress. Therefore, the purpose of this paper is twofold: to extensively review the literature on the mechanisms of muscle hypertrophy and their application to exercise training, and to draw conclusions from the research about the best practices for maximizing muscle growth.",
        "reference": "Increasing muscle mass is the goal of many people who lift weights. Research is lacking, however, as to the optimal way to induce muscle growth. Bodybuilders train with moderate loads and short rest intervals. Powerlifters, on the other hand, train with high-intensity loads and lengthy rest periods. Although both groups are known to display impressive muscularity, it is not clear which method is superior for increasing muscle mass. It has been shown that many factors mediate the process of muscle growth and that mechanical tension, muscle damage, and metabolic stress all can play a role in muscle growth. Therefore, the purpose of this paper is to review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and to draw conclusions from the research as to the optimal way to induce muscle growth.",
        "metrics": {
          "SARI": 36.34082612008492,
          "BLEU": 0.4467705629551226,
          "FKGL": 12
        }
      }
    ]
  ],
  "non_iterative": [
    {
      "SARI": 38.461399274549166,
      "BLEU": 0.18712524035293002,
      "FKGL": 11.872000000000002
    },
    [
      {
        "source": "Abstract   Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. This work describes an innovative mobile application for wildfire information management that operates on Windows Phone devices and acts as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management. Several tasks can be accomplished from the AEGIS App, such as routing, spatial search for closest facilities and firefighting support infrastructures, access to weather data and visualization of fire management data (water sources, gas refill stations, evacuation sites etc.). An innovative feature of AEGIS App is the support of these tasks by a digital assistant for artificial intelligence named Cortana (developed by Microsoft for Windows Phone devices), that allows information utilization through voice commands. The application is to be used by firefighting personnel in Greece and is potentially expected to contribute towards a more sophisticated transferring of information and knowledge between wildfire confrontation operation centers and firefighting units in the field.",
        "prediction": "This paper discusses a new mobile application designed to help manage wildfire information. The app runs on Windows Phone and works alongside the web-based AEGIS platform aimed at wildfire prevention and management. Users can perform various tasks with the AEGIS App, such as finding routes, locating nearby facilities, accessing weather data, and viewing fire management information like water sources and evacuation sites. A unique feature of the AEGIS App is its use of Cortana, a digital assistant by Microsoft, which allows users to get information using voice commands. The app is intended for firefighting teams in Greece and is expected to improve communication of information between command centers and units in the field.",
        "reference": "Novel mobile devices and applications can be used in wildfire confrontation by helping users to access data and information and coordinate personnel and vehicles. This paper describes a new Windows Phone application for wildfire information management which is complementary to the web version of the AEGIS platform for wildfire prevention and management. The AEGIS App can be used for searching for the closest firefighting infrastructures, as well as access to weather data, and visualization of water sources, gas refill stations, evacuation sites, etc. The AEGIS App supports these tasks through voice commands with Microsoft Cortana (AI digital assistant for Windows Phone).  The application is designed for firefighters in Greece but it can help communication between wildfire coordination and local firefighting units.",
        "metrics": {
          "SARI": 39.33008619291954,
          "BLEU": 0.10286205921410455,
          "FKGL": 13.3
        }
      },
      {
        "source": "In recent years, mobile technologies have developed and applied in education fields, and some mobile emerging carriers with mobile technologies include a personal digital assistant (PDA), smart phone, and e-book. Some of the mobile carriers combines context-aware technologies or involve into wireless network environments to make effective use through the combination of learning scenarios and technologies, and then provide students new learning experiences differed from the past learning. In view of the application of mobile technology in education, some previous studies have addressed that mobile learning is a meaningful learning that can improve the interaction between students and the situations and reach the purposes of learning. In the main stream of mobile learning, using mobile carriers with suitable learning methods or strategies in mobile learning activities for different students to enhance learning have gradually become a important and concern issue. The purpose of this study is to investigate the learning achievement and learning attitude of elementary school students on a campus plant learning activity when they use mobile carriers and competitive learning strategies. The experimental results show that the competitive learning group of students have better learning performance than non-competitive learning group of students. After completing the learning activity, the two groups of students presented high positive attitudes towards learning.",
        "prediction": "Mobile technology has recently been used more in education, with devices like personal digital assistants, smartphones, and e-books. These technologies often work with wireless networks to create new learning experiences for students. Research shows that mobile learning can enhance interaction between students and their learning environments, making it more effective. A key focus of mobile learning is to use appropriate devices and strategies to improve learning for different students. This study aims to assess how elementary school students perform and feel about learning when they participate in a campus plant activity using mobile devices and competitive learning strategies. The results indicate that students who engaged in competitive learning performed better than those who did not. Both groups reported positive attitudes toward their learning after the activity.",
        "reference": "Mobile technologies, such as personal digital assistants (PDA), smartphones, and e-books, are increasingly used in the educational field. Some mobile carriers combine learning scenarios and context-aware technologies or wireless network environments providing students with new learning experiences. Mobile learning is effective as it can improve the interaction between students and situations. An important question in the mobile learning field is how to enhance learning through the use of mobile carriers with suitable learning methods in student activities. This study investigates learning achievement and attitude of elementary school students when they use mobile carriers and competitive learning strategies during campus plant learning activities. The competitive learning group of students has better learning performance than the non-competitive one. After the activity, both groups of students were positive about learning.",
        "metrics": {
          "SARI": 43.349630400113206,
          "BLEU": 0.12663476005240473,
          "FKGL": 12.7
        }
      },
      {
        "source": "This paper proposes an electrophysiological wireless patient monitoring system which integrates a Wireless ECG signal transmitter, GPS device and a mobile phone to acquire physiological signals and transmit them to a local server via Bluetooth wireless technology. Four kinds of monitor units were specially designed for a wireless communication, including a control center , a local monitor unit, mobile devices (personal digital assistant; PDA), and a Web page (for both patient and doctor). The use of various monitor units is created to fulfill different medical personnel requirements and wishes. This application was developed to promote the mobility and flexibility for the patients and also for the medical personnel, which further will improve both the quality of health care and lifestyle of the patient.",
        "prediction": "This paper describes a wireless monitoring system for patients that uses a Wireless ECG transmitter, GPS device, and mobile phone to collect and send health signals to a local server using Bluetooth. Four types of monitoring units were designed for wireless communication: a control center, a local monitor, mobile devices like PDAs, and a web page for patients and doctors. These units cater to the different needs of medical staff. This system aims to enhance the mobility of both patients and medical personnel, improving the quality of healthcare and patients' lifestyles.",
        "reference": "This paper suggests a system to monitor patients wirelessly using a device that transmits ECG signals, a GPS device, and a mobile phone. The signals are sent to a local server via Bluetooth technology. Four types of monitor units were created for wireless communication: a control center, a local monitor unit, mobile devices such as personal digital assistants (PDA), and a web page accessible to both patients and doctors. Various monitor units are used to satisfy different medical personnel needs. This application was developed to boost the quality of healthcare and patient lifestyle by increasing mobility and flexibility for both patients and medical personnel.",
        "metrics": {
          "SARI": 52.11364491948124,
          "BLEU": 0.29749246790396755,
          "FKGL": 12.2
        }
      },
      {
        "source": "As various kinds of output devices emerged, such as highresolution printers or a display of PDA(Personal Digital Assistant), the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging image with high quality. One of the largest problems on image enlargement is the exaggeration of the jaggy edges. To remedy this problem, we propose a new interpolation method, which uses artificial neural network to determine the optimal values of interpolated pixels. The experimental results are shown and evaluated. The effectiveness of our methods is discussed by comparing with the conventional methods.",
        "prediction": "As different output devices like high-resolution printers and PDAs became popular, the need for high-quality image enlargement grew. This paper presents a new way to enlarge images without losing quality. A major issue with enlarging images is the rough edges that can appear. To solve this, we introduce a new method that uses artificial neural networks to find the best values for the extra pixels. We show and evaluate our experimental results, discussing how well our method works compared to traditional ones.",
        "reference": "The rise of output devices like high-resolution printers and PDA displays has increased the need for high-quality resolution conversion. The paper proposes a new method to make images bigger while maintaining high quality. The main issue with enlarging images is that jagged edges can become exaggerated. To solve this problem, we suggest a new method that helps us to estimate the value of the newly generated pixels using a neural network. The experiment's results are presented and analyzed. We evaluate the effectiveness of our methods by comparing them to traditional approaches.",
        "metrics": {
          "SARI": 43.69982454400472,
          "BLEU": 0.19416840610405467,
          "FKGL": 10.9
        }
      },
      {
        "source": "A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours. For them, everyday tasks like navigating through a train or metro network map becomes demanding. We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant, non-invasive sound. This technique was implemented inside a Personal Digital Assistant (PDA) portable device. In this implementation, colour information is extracted from the input image and categorised according to how human observers segment the colour space. This information is subsequently converted into sound and sent to the user via speakers or headphones. In the original implementation, it is possible for the user to send its feedback to reconfigure the system, however several features such as these were not implemented because the current technology is limited.We are confident that the full implementation will be possible in the near future as PDA technology improves.",
        "prediction": "Many people struggle to see or tell apart colors. For these individuals, tasks like reading a train or metro map can be difficult. We have developed a new method to extract color information from everyday sights and translate it into pleasant sounds for visually impaired users. This method has been built into a portable Personal Digital Assistant (PDA). It works by taking an image, extracting the color information, and organizing it like humans perceive color. This information is then turned into sound and played through speakers or headphones. In the initial version, users could give feedback to adjust the system, but some features were not included due to current technology limits. We believe that with advances in PDA technology, complete implementation will be possible soon.",
        "reference": "A significant percentage of people have trouble differentiating or seeing colors. Simple tasks like using a train or subway map can be difficult for them. We introduce a new method to help visually impaired users perceive color information using non-invasive, pleasant sounds generated from everyday stimuli. We put this technique into a portable device called a Personal Digital Assistant (PDA). In this implementation, we extract color information from the input image and organize it based on how people perceive color. Then, the information is changed into sound and sent to the user through speakers or headphones. In the first version, the user could give feedback to change the system, but we couldn't add all the features due to technology limitations. We believe that we can add all the features soon as PDA technology advances.",
        "metrics": {
          "SARI": 56.7023106041611,
          "BLEU": 0.24528710568754214,
          "FKGL": 9.4
        }
      },
      {
        "source": "Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients but also incurs a great financial cost to the society. A medicine decision support system in a Personal Digital Assistant (PDA), with a barcode reader, can provide an overview of the patientsu0027 complete medicine use, and detect unsuitable drugs and drug combinations. Focusing on the elderly, our aim was to evaluate if a mobile medicine decision support system with a barcode reader is useful and user-friendly for nurses in home care. The participants received a comprehensive overview from the patientsu0027 medicine use and noted drug-drug interactions, therapeutic duplications and warnings for drugs unsuitable for elderly people. The nurses regarded that the decision support system increased prevention and safety, was useful and user-friendly. Our findings suggest that most of the content and functions were regarded as important. Therefore, this decision support system might be a useful tool for district nurses.",
        "prediction": "Using medicines incorrectly can lead to more hospital visits for older people, causing them unnecessary pain and increasing costs for society. A medicine decision support system in a Personal Digital Assistant with a barcode reader can show all the medicines a patient is using and identify unsafe drugs or combinations. We wanted to see if this mobile system is helpful and easy to use for nurses working in home care for the elderly. The nurses received a detailed overview of patients' medicine use, noticed drug interactions, duplicates, and alerts for drugs that are not suitable for older adults. They found the decision support system helpful, increased safety, and easy to use. Our results indicate that nurses valued most of the features and functions. This system could be a valuable resource for district nurses.",
        "reference": "Misusing medicines can make older people more likely to end up in the hospital. This causes patient suffering and financial cost to society. A support system for medicine decision, with a bar code reader, in a PDA can help identify inappropriate medication use and combinations by providing a complete overview of the patient's medication The goal was to evaluate if a mobile medicine decision support system with a barcode reader is helpful and easy to use for home care nurses working specially with older patients. The participants were provided with a complete overview of the patient's medication usage, and identified any drug interactions, duplicated medications, and any warnings about drugs that are not suitable for elderly patients. The nurses found the decision support system to be helpful, and easy to use, and improving the prevention and safety measures. Our results indicate that the majority of the content and features were considered significant. So, this system could be helpful for district nurses.",
        "metrics": {
          "SARI": 48.94130978501828,
          "BLEU": 0.22015416208087524,
          "FKGL": 10.7
        }
      },
      {
        "source": " :[0],\"access to patient appointment schedules can help clinicians manage time and problems better. Many large health care organizations manage clinical appointment scheduling via enterprise resource scheduling systems which are poorly accessible by clinicians. Also, staff other than the clinicianu0027s personal assistant(s) may manage scheduling, making it difficult for clinicians to stay informed of changes. Many a clinician today uses a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician.",
        "prediction": "Access to patient appointment schedules can help doctors manage their time and issues more effectively. Many large healthcare organizations use complex systems to manage appointments, which are not easy for doctors to access. Other staff members may handle the scheduling, making it hard for doctors to know about changes. Many doctors today use a personal digital assistant (PDA) for basic scheduling. Our \"PalmOversite\" project shows it is possible to connect appointment information from large systems to a PDA calendar, making schedule details much easier for doctors to access.",
        "reference": "Knowing patients' appointment schedules can help clinicians manage time and problems more effectively. Healthcare organizations often use enterprise resource scheduling systems for clinical appointment scheduling, but these systems may not be easily accessible by clinicians. Moreover, scheduling may not only be handled by the clinician's assistant(s), creating challenges for clinicians to stay updated on any changes. Many clinicians today use a personal digital assistant (PDA) with a basic calendar. Our project called 'PalmOversite' shows the practicability of integrating enterprise appointment schedule information into a PDA calendar. This makes it easier for clinicians to access their schedule information.",
        "metrics": {
          "SARI": 42.67485101333651,
          "BLEU": 0.163133011806286,
          "FKGL": 11.3
        }
      },
      {
        "source": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. Currently, a SES employee visits each health establishment to collect this information by hand, process it and type it into an electronic medical record system  this :[81],\"paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify and upload monthly bacteriology data into the PIH-EMR. After an initial implementation period, we performed a pilot study to test the use of this system. We completed a baseline assessment in two health districts and then implemented the electronic system in one of the districts while the control site continued to use the paper-based system during the same period. The PDA-based system had a processing time of 6.2 days, significantly lower than measurements for both the baseline [54.8] and control sites [64.4] (both pu003c0.0001). It was also able to reduce the frequency of discrepancy from 10.1% to 2.8% (pu003c0.0001) and receive positive feedback from the users. Finally, the system’s cost would be recuperated in three months from time savings due to increased work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors and costs.",
        "prediction": "Partners In Health and its sister organization in Lima, Peru, Socios En Salud, provide treatment for most patients with multidrug-resistant tuberculosis in Peru, working alongside the Peruvian National TB Program. Monthly tests are crucial for this treatment and must be collected from different health centers in the city. Currently, a Socios En Salud employee goes to each health center to manually gather and enter this information into an electronic system. This paper describes the creation and use of a digital assistant-based system to collect, check, and upload monthly testing data into the electronic medical records. After starting this new system, we conducted a study to see how well it worked. We assessed two health districts and used the electronic system in one district while the other continued using paper records. The new system took about 6.2 days to process data, much faster than the previous paper method, which took over 54 days. It also reduced reporting mistakes from 10.1% to 2.8% and received positive feedback from users. The cost of the system would be recovered in three months because of the time saved through this improved efficiency. We plan to study the system further to see its effects on delays, errors, and costs.",
        "reference": "PIH (Partners In Health) and SES (Socios En Salud) provide treatment for most of the MDR-TB (multidrug-resistant tuberculosis) patients in Peru along with the Peruvian NTP (National TB Program). Monthly bacteriology tests are necessary for this treatment and must be collected from various health establishments located throughout the city. An employee from SES is currently going to each health establishment and collecting monthly bacteriology test results by hand, processing them, and typing them into an electronic medical record system. In this paper, we explain how we made a digital system using a personal digital assistant (PDA) to gather, check, and upload monthly bacteriology data into the PIH-EMR (Partners In Health Electronic Medical Record).  After an initial period of putting the system into action, we conducted a test study to evaluate the effectiveness of using this system. We first evaluated the current system in two health districts, then introduced the electronic system in one district while the other continued using the paper-based system during the same time. The PDA-based system took only 6.2 days to process, which was significantly lower than both the baseline (54.8) and control sites (64.4) (both less than 0.0001). The PDA-based system reduced differences from 10.1% to 2.8% and received positive feedback from users. The system would pay for itself in three months due to increased efficiency and time savings. A larger study will be conducted to assess the impact of this system on delays, errors, and costs.",
        "metrics": {
          "SARI": 48.45800446873319,
          "BLEU": 0.2461295858886792,
          "FKGL": 9.4
        }
      },
      {
        "source": "In order to facilitate knowledge transfer between specialists and generalists and between experts and novices, and to promote interdisciplinary communication, there is a need to provide methods and tools for doing so. This interdisciplinary research team developed and evaluated a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage/tele-consultation when the presenting problem was chest pain. The combined human factors methods of cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase were used to develop the DST. A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period. During this time, the DST was used by the nine nursing coordinators who provide tele-triage/tele-consultation 24/7. This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses, its potential for use by nurses less experienced in cardiac care, and for its potential use in an interdisciplinary team environment.",
        "prediction": "To improve communication between specialists, generalists, experts, and beginners, there is a need for better methods and tools. A research team created and tested a decision support tool on a personal digital assistant for cardiac triage and consultation when patients had chest pain. They used methods to understand how people work and design the tool effectively. A pilot study was done in a cardiac care hospital over three months, where nine nursing coordinators used the tool 24/7. This study confirmed that the tool was well-designed and helpful for experienced cardiac nurses, as well as showing potential use for less experienced nurses and in team settings.",
        "reference": "To improve communication between specialists, generalists, experts, and novices and encourage interdisciplinary collaboration, we need to develop methods and tools. An interdisciplinary research team created and tested a decision support tool (DST) on a personal digital assistant (PDA) to aid cardiac tele-triage/tele-consultation for patients with chest pain. To create the DST (Decision Support Tool), we used both cognitive work analysis and ecological interface design methods during the requirements-gathering and design phases. A trial was done for 3 months at a quaternary cardiac care hospital During the trial, nine nursing coordinators used DST for 24/7 tele-triage/tele-consultation. This clinical trial confirmed that the design is useful for advanced cardiac care nurses, as well as for less experienced nurses and interdisciplinary teams.",
        "metrics": {
          "SARI": 49.97067205689348,
          "BLEU": 0.28727432878835424,
          "FKGL": 11.5
        }
      },
      {
        "source": "This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Since conventional computer-assisted language learning is often boring, it exploits a cyber pet game to increase the learner’s interest. M-CALL runs on a personal digital assistant with public wireless LAN for mobile learning. It grows a cyber pet by solving problems of Korean language learning. Korean Proficiency Test (KPT), a nationally certified Korean language test, was used as problem sets. It consists of cyber pet game, mobile learning courseware, mobile learning system, and mobile tutoring. It provides various functions for Korean language learning. Currently, the prototype M-CALL was designed and partly implemented between mobile PDA and personal computer.",
        "prediction": "This paper introduces M-CALL, a mobile tool for learning Korean. To make learning more engaging, it uses a cyber pet game instead of typical methods. M-CALL works on personal digital assistants with public wireless internet for learning on the go. Users grow a cyber pet by solving Korean language problems. The Korean Proficiency Test is used for these problems. M-CALL includes a cyber pet game, mobile learning courseware, a mobile learning system, and mobile tutoring features. It supports various activities for learning Korean. Currently, a prototype of M-CALL has been designed and partially implemented for both mobile PDAs and personal computers.",
        "reference": "M-CALL is a mobile courseware for Korean language learners. It incorporates a cyber pet game to make it more engaging for learners than traditional computer-assisted language learning methods. M-CALL is a mobile learning system that uses a personal digital assistant with public wireless LAN (Computer network in small geographic areas). A cyber pet is raised while solving problems related to Korean language learning. The KPT, a nationally recognized exam for Korean language proficiency, served as the source material for the problem sets. It includes a virtual pet game, a mobile course for learning, a mobile learning platform, and mobile tutoring. It offers multiple features for learning Korean. Right now, a preliminary version of M-CALL has been developed and partially implemented for use on both mobile PDAs and personal computers.",
        "metrics": {
          "SARI": 47.38477915476129,
          "BLEU": 0.20997743260467588,
          "FKGL": 9.4
        }
      },
      {
        "source": "This study compared text entry performance of two stylus-driven soft keyboards for use in hand-held computing devices: the QWERTY and the T9. Participants transcribed text presented on a computer screen into a personal digital assistant (PDA) using a stylus and one of these two keyboards. We introduced a new psychophysical technique for measuring transcription rate that provides a composite measure of speed and accuracy. Using this technique, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparent superior physical characteristics. An ancillary experiment demonstrated that the poorer performance of the T9 layout may have resulted from an increase in visual scanning time due to perceptual grouping of the letters on the keys. Together these findings imply that the QWERTY keyboard layout remains the most effective of the currently available designs for stylus tapping on soft keyboards.",
        "prediction": "This study looked at how well two stylus-operated soft keyboards, QWERTY and T9, performed when typing on handheld devices. Participants typed text shown on a screen into a personal digital assistant using a stylus with one of the keyboards. We used a new method to measure how quickly and accurately they typed. The results showed that the QWERTY keyboard had a higher typing speed than the T9, even though the T9 had better physical design. An additional experiment suggested that the T9's lower performance might be due to longer time spent looking at the letters on the keys. Overall, these results suggest that the QWERTY layout is still the best option for typing with a stylus on soft keyboards.",
        "reference": "This study compared two onscreen keyboards, the QWERTY and the T9, for text entry performance on hand-held computing devices. Participants used a stylus to input text on a PDA using either a QWERTY or T9 keyboard in this study. We created a new way to measure transcription rate that considers both speed and accuracy together. We used this method to determine the fastest possible typing speed for each keyboard. The study found that the QWERTY keyboard had a higher transcription rate than the T9 keyboard, even though the T9 keyboard was physically better. Another test showed that the T9 keyboard's lower performance might be because the grouped letters on its keys require more visual scanning time. Overall, these results suggest that, among the current options, the QWERTY keyboard layout is still the most efficient for typing on touch screens with a stylus.",
        "metrics": {
          "SARI": 47.85044699783114,
          "BLEU": 0.20886544885040395,
          "FKGL": 9.8
        }
      },
      {
        "source": "The proliferation of mobile devices such as smart phones and Personal Digital Assistant (PDA) opens new ways for developing mobile E-commerce systems (so called Mobile-commerce or M-commerce system). In Mcommerce systems, we still see many common components that we have in standard E-commerce applications, such as web servers and database servers. However, these new applications raise some unique challenges. For example, the limitations of mobile devices (e.g., small screen size and reduced CPU performance) implies that software development is partly different from desktop applications. Questions which are posed when designing M-commerce system include: “What kind of information do the users want to access via mobile devices?”, “How can we provide useful applications with limited user input facility?”, “How can we test a distributed M-commerce system?” In this paper, we discuss about some software development issues for mobile commerce systems from our experiences while developing an M-commerce system for tourists. Our prototype, easyHotel, is an useful software that allows booking hotel rooms via mobile phones.",
        "prediction": "Mobile devices like smartphones and PDAs are creating new opportunities for mobile E-commerce, also known as M-commerce. M-commerce systems include many features found in regular E-commerce, such as web servers and database servers. However, these systems face unique challenges. For instance, mobile devices have limitations, like small screens and lower processing power, which affects software development compared to desktop applications. When designing an M-commerce system, important questions arise: What information do users want on their mobile devices? How can we create useful applications with limited input options? How can we test an M-commerce system that is distributed? In this paper, we share our insights on software development issues for mobile commerce based on our experiences while creating an M-commerce system for tourists. Our prototype, easyHotel, helps users book hotel rooms through their mobile phones.",
        "reference": "The increasing use of mobile devices like smartphones and PDAs provides opportunities for the development of new mobile e-commerce systems, also known as M-commerce (Mobile-commerce) systems. M-commerce systems share several components with traditional E-commerce applications, such as web servers and database servers. However, these new applications present some distinct challenges. For example, due to mobile devices' limitations, such as smaller screens and weaker processors (also called CPUs), software development for mobile applications is partially different from desktop applications. Designing an M-commerce system involves asking questions such as: \"What information do users want on mobile devices?\", \"How can we create useful apps with limited user input?\", and \"How do we test a distributed M-commerce system?\" In this paper, we share our experiences developing an M-commerce system for tourists and discuss software development issues for such systems. Our software prototype, easyHotel, is a helpful application enabling users to book hotel rooms through their mobile phones.",
        "metrics": {
          "SARI": 58.37452799325412,
          "BLEU": 0.3149717645584842,
          "FKGL": 10.3
        }
      },
      {
        "source": "It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As emerging mobile platforms such as different kinds of Personal Digital Assistant (PDA) tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common approaches for content adaptation are automated conversion and explicit specification of adapted content, with a trade-off between quality and development/maintenance effort. As alternative avoiding this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework in the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development/maintenance effort.",
        "prediction": "It is well known that web content needs to be adjusted for mobile devices because of their limitations. Since different mobile devices, like Personal Digital Assistants, have various capabilities, we believe that adaptations should be specific to each platform. Common methods for adapting content are automated conversion and manually specifying changes, which involve a balance between quality and the effort needed for development and maintenance. To avoid this balance, we suggest a straightforward object-oriented framework for content adaptation. Our approach builds on the object-oriented WebComposition model and its XML-based implementation. We show how our object-oriented method can make it easier to create and maintain platform-specific content by demonstrating it with an example application.",
        "reference": "It is widely recognized that information, such as web content, should be adjusted for mobile platforms to accommodate the limitations of mobile environments. As new mobile platforms, such as various types of PDAs, differ significantly in their capabilities, we propose that adaptation should be tailored to each platform. Common methods for content adaptation involve automated conversion or explicitly specifying adapted content, balancing the trade-off between quality and the effort required for development and maintenance. Instead of making this trade-off, we suggest using a straightforward object-oriented framework for content adaptation. To make this framework more accessible for use on the web, we build our approach on the object-oriented WebComposition model and its implementation, WCML, which is an application of XML. We use our object-oriented framework in an example application to show how specifying platform-adapted content using an object-oriented approach can lower development and maintenance costs.",
        "metrics": {
          "SARI": 49.685101522351175,
          "BLEU": 0.2254828072067261,
          "FKGL": 15.3
        }
      },
      {
        "source": "In monitoring a patientu0027s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in timely-context aware manner. In this paper a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device — PSD) used in the Healthcare systems are bulky in design and are not flexible and comfortable for the elderly patients. Presented is a unique flexible, as well as detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage and processing of patient related data, which ensures periodic logging of patient data without saturating communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smart-phone and the ECG and temperature acquisition apparatus is implemented Bluetooth. The presented Data Management System — Periodic Model (DMS — PM) is designed to manage wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a Smart phone making use of the existing GSM network.",
        "prediction": "This paper discusses a system that monitors a patient's vital signs using Body Area Networks (BAN). It focuses on how to provide timely and relevant data to doctors. The system is designed specifically for patients with heart issues and includes a comfortable and flexible wearable device called the Patient Sensing Device (PSD). Many current devices are bulky and uncomfortable, especially for elderly patients. The paper introduces a unique PSD that can be worn easily and removed when needed. It also describes a method for sending, storing, and processing patient data that collects information regularly without overloading communication networks. A prototype has been created to transmit the patient's heart rhythm (ECG) and body temperature, which can later include more vital signs. The device uses Bluetooth to connect with a mobile phone. The Data Management System - Periodic Model (DMS - PM) helps manage the wireless communication between the sensor devices and a patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or smartphone, using the existing GSM network.",
        "reference": "Medical practitioners can access a wealth of data sources by using Body Area Networks (BAN) to monitor a patient's real-time vital signs. A BAN refers to a network of wearable or implantable devices worn on or attached to the human body for various purposes. Timely and context-aware delivery of data is crucial. This paper presents a system specifically designed for patients with cardiac disorders, focusing on the design of the sensing device and communication scheme selected. Many of the current wearable physiological devices used in healthcare systems, known as Patient Sensing Devices (PSD), are bulky and uncomfortable for elderly patients due to their inflexible design. We introduce a unique Patient Sensing Device (PSD) that is both flexible and detachable, designed for patient comfort. We also propose a model for transmitting, storing, and processing patient-related data that ensures regular logging of data without overwhelming communication networks. We have developed and implemented a proof-of-concept prototype that can transmit a patient's Electrocardiogram (ECG) signal and body temperature, potentially expanding to other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition apparatus is achieved via Bluetooth. The Data Management System - Periodic Model (DMS - PM) introduced here is created to facilitate the wireless connection between sensor units and the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a smartphone, utilizing the existing GSM network.",
        "metrics": {
          "SARI": 51.98452621748798,
          "BLEU": 0.25365049553635,
          "FKGL": 11.8
        }
      },
      {
        "source": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. Current trend integrates usage of mobile terminals such as smart PDA (personal digital assistant) in a learning system. Such devices can communicate with wireless mobile terminal and realize the intelligence exchange with its surroundings [1]. A LOCH (Language Learning Outside the Classroom) system named One Day Trip is developed to assist overseas students to learn Japanese in real life situations with the help of a PDA. This paper proposes the implementation with Knowledge Management (KM) concepts in the system to aid the teacher and learners in the learning environment.",
        "prediction": "The growth of wireless and mobile technology has created new ways for learning in a single space. There is now a trend to use mobile devices like smartphones and PDAs in education. These devices can connect wirelessly and share information with their environment. A system called One Day Trip helps international students learn Japanese through real-life experiences using a PDA. This paper suggests using Knowledge Management ideas in this system to support both teachers and students in the learning process.",
        "reference": "Wireless and mobile technology progress has allowed more platforms to facilitate learning within a unified space. The current trend is to use mobile devices, such as smart PDAs, in learning systems. These devices can communicate wirelessly with mobile terminals and exchange information with the surrounding environment One Day Trip is a language learning system called LOCH (Language Learning Outside the Classroom), designed to help foreign students learn Japanese in real-life situations using a PDA. This paper suggests incorporating Knowledge Management (KM) concepts in the One Day Trip system to support both the teacher and learners in the learning environment.",
        "metrics": {
          "SARI": 45.047979258229795,
          "BLEU": 0.17978465833029603,
          "FKGL": 9.5
        }
      },
      {
        "source": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko personal digital assistant, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Instead of porting each application to individual platforms, which would prove to be extremely time consuming because of other APIu0027s and networking protocols, we created the Vassar College Messaging Layer. This layer abstracts the Cybiko networking structure and interface, allowing Cybiko applications to be ported to other platforms with minimal code alteration.",
        "prediction": "Recently, we developed applications to teach wired and wireless networking. These were made for the Cybiko personal digital assistant, which was a low-cost alternative to other devices. However, the Cybiko PDA is no longer available. This meant we had to move these applications to other platforms to keep them usable. Instead of adapting each application one by one, which would take a lot of time due to different programming interfaces and protocols, we created the Vassar College Messaging Layer. This layer simplifies the Cybiko networking system, making it easier to adapt Cybiko applications to other platforms with very little code change.",
        "reference": "In recent years, we have created applications for teaching both wired and wireless networking. These applications are designed for the Cybiko personal digital assistant, which offers a cost-effective alternative to other hardware options. The Cybiko PDA was discontinued recently. Consequently, these applications needed to be adapted to other platforms to maintain their usefulness. To avoid the time-consuming task of porting each application to various platforms due to different APIs and networking protocols, we developed the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, making it easier to adapt Cybiko applications to different platforms without needing to make significant changes to the code.",
        "metrics": {
          "SARI": 50.8287096498703,
          "BLEU": 0.3446240138338128,
          "FKGL": 11
        }
      },
      {
        "source": "Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful (and thus more useful), machines. On the other side of this border we find the Personal Digital Assistant  :[66],\"our view, to be useful as an extension of the usersu0027 private sphere, a machine must at least have enough functionality and resources to create trustworthy digital signatures (to speak for the user, as it were). A less resourceful machine can merely act as a memory prothesis, helping the owner remembering addresses and phone  are :[121],\"designed to be tamper resistant, and as such they seem ideal as a minimal machine. However, trustworthy digital signatures can not be created by smartcards alone, simply because the user does not know what is given to the card for  :[163],\"order to be trusted--that is, being able to make trustworthy digital signatures--a smartcard must be supported by some infrastructure outside the card proper. We explore what must be included in such an infrastructure, and demonstrate that trustworthy digital signatures can in fact be made using a standard  :[211],\"argue that based on this fact, nontrivial distributed systems can be constructed by utilizing smartcards; a nontrivial system is one where holders of smartcards are \"first class citizens\". Asserting that a smartcard can act as a trusted machine gives new opportunities for designers of user-centric  that smartcards are :[257],\"here to stay, finding ways to apply them in constructive manners is prudent.",
        "prediction": "Smartcards cannot communicate securely with users, making it hard to use their benefits. We look at how smartcards compare to more powerful devices like Personal Digital Assistants, which can help users manage their private information. To be truly useful, a device should create trustworthy digital signatures, enabling it to represent the user securely. Smartcards alone cannot achieve this because users cannot confirm what information they give to the card. Therefore, smartcards need support from external infrastructure to create reliable digital signatures. We discuss what this infrastructure should include and show that it is possible to use smartcards for trustworthy signatures. This means we can build complex systems where smartcard holders are treated as important users. Recognizing that smartcards are here to stay encourages us to find practical ways to use them.",
        "reference": "Smartcards lack the capability to communicate with the user in a secure manner, unlike other hardware classes. (Smart cards are small, portable devices that have a special chip inside. They can store and process information securely.) Without a way to keep the owner updated, it becomes challenging to take advantage of the useful features of smart cards. We look at how smart cards and powerful machines can be combined to make even better devices. We believe that Personal Digital Assistants (PDAs) are more useful than smartcards because they can extend the user's private sphere. PDAs have enough functionality and resources to create trustworthy digital signatures, which can speak for the user. A less resourceful machine can serve as a memory aid, assisting the owner in remembering addresses and phone numbers, such as [121]. These machines are designed to be tamper-resistant, making them suitable for a minimalistic purpose. To create trustworthy digital signatures, a smartcard needs support from infrastructure outside the card itself. This is because the user is not aware of what information is given to the card, making it impossible for the card to be trusted solely on its own. We investigate the necessary components of this infrastructure and show that reliable digital signatures can indeed be created using a standard method [211]. We propose that smartcards can be utilized to build nontrivial distributed systems, where smartcard holders are considered important participants or \"first class citizens\". Claiming that smartcards can function as trusted machines opens up fresh possibilities for designers focused on user-centric approaches. It is evident that smartcards are here to stay, so it is wise for designers to explore and implement them in meaningful ways.",
        "metrics": {
          "SARI": 40.21465522476911,
          "BLEU": 0.05855169182559638,
          "FKGL": 10.9
        }
      },
      {
        "source": "In this paper we present the main features of a system that we have developed and that provides a new kind of tele-assistance service to elderly people.apart from the functionalities offered by the current tele-assistance services, also provides a high quality, anywhere and at any time assistance. We rate the assistance as high quality because allows the monitoring of vital signs and a local detection of anomalous situations by the device that the person carries in real time. Therefore, it sends alarms autonomously when  this paper we :[84],\"explain the three main functionalities offered by AINGERU: universal assistance anywhere and at any time, monitoring of vital signs irrespective of location or time and remote monitoring by allowing authorized external persons to consult data about monitored persons using the Internet. Moreover we show how those functionalities are achieved by making use of PDAs (Personal Digital Assistant), wireless communication, Semantic Web, Web services and agent technologies.",
        "prediction": "In this paper, we describe a new system we developed for tele-assistance aimed at elderly people. Unlike existing services, our system offers high-quality assistance anytime and anywhere. We consider the assistance high quality because it can monitor vital signs and detect unusual situations in real time with a device that the user carries. This device can autonomously send alarms if needed. We explain three main features of our system: universal assistance available at any time and place, continuous monitoring of vital signs regardless of location, and remote access that allows authorized individuals to view data about the monitored users online. We also detail how we achieve these features using PDAs, wireless communication, Semantic Web, Web services, and agent technologies.",
        "reference": "This paper introduces a new tele-assistance service for elderly people that goes beyond existing services by offering high-quality assistance anywhere and anytime. Our system provides high-quality assistance because it allows real-time monitoring of vital signs and local detection of anomalous situations through the device carried by the person. We explain the three main functions of AINGERU in this paper: universal assistance anywhere and at any time, monitoring vital signs regardless of location or time, and remote monitoring that allows authorized external individuals to access data about monitored persons via the internet. We use PDAs, wireless communication, Semantic Web (aims to enhance the World Wide Web by adding meaning to the information available online), Web services, and agent technologies to achieve the aforementioned functionalities.",
        "metrics": {
          "SARI": 49.423633140913736,
          "BLEU": 0.22135549142098623,
          "FKGL": 13.4
        }
      },
      {
        "source": "Objective: To evaluate the effectiveness of a personal digital assistant (PDA)–based clinical decision support system (CDSS) on nonsteroidal anti-inflammatory drug (NSAID) prescribing safety in the outpatient  :[26],\"The design was a randomized, controlled trial conducted in a university-based resident clinic. Internal medicine residents received a PDA-based CDSS suite. For intervention residents, the CDSS included a prediction rule for NSAID-related gastrointestinal risk assessment and treatment recommendations. Unannounced standardized patients (SPs) trained to portray musculoskeletal symptoms presented to study physicians. Safety outcomes were assessed from the prescriptions given to the SPs. Each prescription was reviewed by a committee of clinicians blinded to participant, intervention group assignment, and baseline or follow-up  :[108],\"Prescriptions were judged as safe or unsafe. :[26],\"The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control  :[135],\"At baseline, the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar (0.27 vs. 0.29, p u003e 0.05). Controlling for baseline performance, intervention participants prescribed more safely than controls after receiving the CDSS (0.23 vs. 0.45 [F = 4.24, p u003c 0.05]). With the CDSS, intervention participants documented more complete assessment of patient gastrointestinal risk from  :[198],\"Participants provided with a PDA-based CDSS for NSAID prescribing made fewer unsafe treatment decisions than participants without the CDSS.",
        "prediction": "The study aimed to see how effective a personal digital assistant system was in helping doctors safely prescribe nonsteroidal anti-inflammatory drugs. It was a randomized trial in a resident clinic where internal medicine residents used a PDA-based decision support system. This system included tools to assess the risk of gastrointestinal problems related to NSAIDs and gave treatment recommendations. Standardized patients acted out musculoskeletal symptoms to the doctors for evaluation. A team of clinicians reviewed the prescriptions to determine if they were safe or unsafe without knowing which group the doctors belonged to. The main focus was on the difference in unsafe NSAID prescriptions between those using the decision support system and those who did not. At the beginning, the rate of unsafe prescriptions was similar for both groups. However, after using the system, the doctors with the decision support system prescribed NSAIDs more safely than those without it. They also did a better job of assessing patients' gastrointestinal risks. Overall, those using the PDA-based system made fewer unsafe treatment choices.",
        "reference": "We did a study to test if a clinical decision support system (CDSS) on a personal digital assistant (PDA) could improve the safety of prescribing nonsteroidal anti-inflammatory drugs (NSAIDs) in outpatient clinics. We conducted a randomized, controlled trial in a university-based resident clinic. Residents in internal medicine were given a CDSS suite based on PDA. Intervention residents were provided with a CDSS that had a prediction rule for assessing gastrointestinal risk related to NSAIDs and recommendations for treatment. Trained individuals, known as standardized patients (SPs), who were equipped with musculoskeletal symptoms, interacted with the physicians participating in the study without prior notification. The study evaluated the safety outcomes of prescriptions given to standardized patients portraying musculoskeletal symptoms. A committee of clinicians who didn't know about the participant, intervention group assignment, or the baseline or follow-up data reviewed each prescription. They determined whether a prescription was safe or unsafe. The study aimed to see if the PDA-based CDSS would result in a reduction of unsafe prescribing of NSAIDs. The main outcome measured the difference in unsafe prescribing between the intervention and control groups. At the beginning of the study, both groups had a similar proportion of unsafe prescriptions. (0.27 vs 0.29) After controlling for the baseline performance, it was found that those who received the CDSS suite prescribed nonsteroidal anti-inflammatory drugs more safely than those who didn't (0.23 vs 0.45, with statistical significance). Participants who had access to the CDSS through a PDA documented a more comprehensive assessment of gastrointestinal risk in patients. Moreover, the presence of the CDSS resulted in fewer unsafe treatment decisions compared to participants who did not have access to the CDSS.",
        "metrics": {
          "SARI": 41.031102588486604,
          "BLEU": 0.08515023136050268,
          "FKGL": 11.1
        }
      },
      {
        "source": "Recently pedestrian navigation has been studied by many researchers, which provides pedestrians with a similar function of car navigation. On the other hand, a 3dimensional (3D) map, which is one of major themes in machine vision research, has been utilized as a simulation tool in city and landscape planning, and other engineering fields. The 3D maps will give more intuitive information compared to conventional 2-dimensional (2D) ones. In this paper we first propose pedestrian navigation based on 3D maps, and describe technologies required and its use situations. Secondly we compare effectiveness of 2D and 3D maps for navigation by object search experiments under conditions: 3D maps with and without texture; display sizes corresponding to a mobile phone and PDA (Personal Digital Assistant). The experimental results show that 3D maps with texture on mobile phone display size, and 3D maps without texture on PDA display size are superior to 2D maps in search time and error rate. From these results, we can say the effectiveness of 3D maps in pedestrian navigation.",
        "prediction": "Recently, many researchers have been looking into pedestrian navigation, which is similar to car navigation. A key tool in this area is the use of 3-dimensional (3D) maps. These maps are important in studies of urban planning and engineering because they provide clearer and more intuitive information than traditional 2-dimensional (2D) maps. In this paper, we first suggest using 3D maps for pedestrian navigation and discuss the technologies needed and situations where they can be used. We then compare the effectiveness of 2D and 3D maps through object search experiments. These experiments test 3D maps with and without textures and display sizes for mobile phones and PDAs (Personal Digital Assistants). The results show that 3D maps with textures on mobile phone displays and 3D maps without textures on PDA displays perform better than 2D maps in terms of search time and accuracy. This supports the idea that 3D maps are more effective for pedestrian navigation.",
        "reference": "Recently many researchers have studied pedestrian navigation, which is similar to car navigation but designed for pedestrians. A 3D map is widely used in machine vision research and various engineering fields, such as city and landscape planning, as a simulation tool. 3D maps are more intuitive than 2D maps. In this paper, we suggest using 3D maps for pedestrian navigation and explain the necessary technology and possible scenarios for its application. We will conduct object search experiments to compare the effectiveness of 2D and 3D maps for navigation. The experiments will consider the following conditions: 3D maps with and without texture, and display sizes equivalent to a mobile phone and a Personal Digital Assistant (PDA). The results of the experiment suggest that 3D maps with texture on a mobile phone display and 3D maps without texture on a PDA display are better than 2D maps in terms of search time and error rate. Based on these results, we can conclude that 3D maps are effective for pedestrian navigation.",
        "metrics": {
          "SARI": 50.490185677571084,
          "BLEU": 0.340035841484961,
          "FKGL": 10.9
        }
      },
      {
        "source": "Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments, that are characterized by low bandwidth communication facilities and ad hoc connection/disconnection to stationary systems. They can be used for information retrieval and information filtering, in which case they evaluate replies and return only the relevant data. Mobile agents as a metaphor of active objects are created on a mobile device such as a Personal Digital Assistant (PDA), will be launched into the information galaxy and are fulfilling the mobile user ‘s task on the services available on networked stationary systems. One transmission channel for these itinerant agents is email. This paper introduces Active M 3 as an example of an active mail framework, which can be regarded as a first approach in authoring mobile agents in a graphical interactive manner. Active M 3 integrates two known concepts: active mail and multimedia mail.",
        "prediction": "Mobile agents are a useful idea in client-server computing, especially for mobile environments with slow internet connections and frequent disconnections. They help in finding and filtering information by assessing responses and sending back only what is important. Mobile agents act like active objects created on devices like Personal Digital Assistants (PDAs) and perform tasks using resources on connected stationary systems. One way to communicate with these mobile agents is through email. This paper presents Active M 3 as an example of a system for creating mobile agents easily and interactively. Active M 3 combines two known ideas: active mail and multimedia mail.",
        "reference": "Mobile agents are a competitive concept in client-server computing, particularly well-suited for mobile environments. These environments are characterized by limited bandwidth communication and the frequent connection and disconnection to stationary systems. They can be utilized for information retrieval and filtering, where they assess responses and provide only the pertinent data. Mobile agents act like active objects that can be created on a mobile device like a PDA. They are sent out to perform tasks on the networked stationary systems and fulfill the user's needs. Email serves as a transmission channel for these mobile agents. This paper presents Active M 3, which is an active mail framework that allows users to create mobile agents using a graphical interface. Active M3 is a framework that combines two concepts: active mail and multimedia mail.",
        "metrics": {
          "SARI": 46.989456854591424,
          "BLEU": 0.20718180430626823,
          "FKGL": 12.3
        }
      },
      {
        "source": "Digital Video Broadcasting — Handheld (DVB-H) is a technology developed as an extension of Digital Video Broadcasting — Terrestrial (DVB-T) with additional features that make it suitable for delivery to mobile devices such as phones and Personal Digital Assistant (PDAs). This paper deals with the conditions of receiving digital video broadcast type services in mobile handheld devices. The DVB-H standard is introduced and the key technology elements on data link layer and physical layer are discussed in some details. Finally the DVB-H terminal and network design is described.",
        "prediction": "Digital Video Broadcasting — Handheld (DVB-H) is a technology that builds upon Digital Video Broadcasting — Terrestrial (DVB-T) and is designed for mobile devices like phones and PDAs. This paper explores how digital video broadcasts can be received on these handheld devices. It introduces the DVB-H standard and discusses important technology aspects related to the data link and physical layers. Finally, it describes the design of DVB-H terminals and networks.",
        "reference": "DVB-H is a technology based on DVB-T that is designed for delivering content to mobile devices like phones and PDAs. This paper discusses how digital video broadcast services can be received on mobile handheld devices. The introduction of the DVB-H standard includes a discussion of key technology elements on the data link layer and physical layer, providing some detailed information. This paper also covers the design of DVB-H terminals and networks.",
        "metrics": {
          "SARI": 51.80453655097399,
          "BLEU": 0.3209837716308108,
          "FKGL": 12.3
        }
      },
      {
        "source": "Summary form only given, as follows. We present a design for a low cost but powerful and high speed communications device powered by IDTu0027s RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus compatible IDT ATM NIC. The device could be used as a two way pager, a set-top box, an Internet terminal, a personal digital assistant or as a video phone. The RV4640 is a very affordable 64 bit RISC processor that executes 175 dhrystone MIPS at 133 MHz. The RV4640 can be connected to a System Interface Chip which will provide I/O and memory control. The IDT77903 ATM card is a full duplexed 25 Mbps NIC with a PCI bus interface and it costs less than $100 (US). Integrating this card into our device as a network interface will make highspeed videoconferencing and multimedia applications easily implementable.",
        "prediction": "We designed an affordable and fast communication device using IDT's RV4640 processor. This device can operate at high speeds because of the IDT ATM NIC that works with a PCI bus. It can function as a two-way pager, set-top box, Internet terminal, personal digital assistant, or video phone. The RV4640 is a low-cost 64-bit RISC processor that runs at 133 MHz and executes 175 MIPS. It can connect to a System Interface Chip for I/O and memory control. The IDT77903 ATM card is a fast 25 Mbps NIC for a PCI bus and costs under $100. By adding this card, the device will support high-speed videoconferencing and multimedia applications easily.",
        "reference": "Summary provided in the following. We propose a low-cost and high-speed communication device that uses IDT's RV4640 as the processing engine. This device is designed with IDT's RV4640 as the processing engine and can achieve high speed due to the PCI bus (a type of computer bus that simplifies the communication between computer components by providing a standardized interface) compatible IDT ATM NIC. This device can serve multiple purposes such as a two-way pager, set-top box, Internet terminal, personal digital assistant, or video phone. The RV4640 is a low-cost 64-bit RISC processor that runs at 133 MHz and can execute 175 dhrystone MIPS. The RV4640 can connect to a System Interface Chip for I/O (a specialized integrated circuit designed to facilitate communication between a computer system and its external devices) and memory control. The IDT77903 ATM card, which costs less than $100 (US), is a NIC with a PCI bus interface that is capable of full duplexing at 25 Mbps. Using the IDT77903 ATM card as a network interface in our device will make it easy to implement high-speed videoconferencing and multimedia applications.",
        "metrics": {
          "SARI": 58.31775992624403,
          "BLEU": 0.28486438639330114,
          "FKGL": 8.2
        }
      },
      {
        "source": "Dr. Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework, which serves as the foundation for the NET natural user interface. The paper discusses MiPadu0027s design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates compelling user advantages for wireless personal digital assistant (PDA) devices, MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in todayu0027s PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built in roller to navigate, the tapping action narrows the number of possible instructions for spoken word understanding. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition is performed. The Dr Who CSR engine uses a unified CFG and n-gram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager.",
        "prediction": "Dr. Who is a Microsoft research project that aims to create a speech-focused way for users to interact with technology, forming the base for the NET natural user interface. The paper reviews the design and ongoing work of MiPad, an application prototype that offers significant benefits for wireless personal digital assistants (PDAs). MiPad combines continuous speech recognition and spoken language understanding to help users perform common tasks using both voice and touch. It addresses issues with small styluses and keyboards found in current PDAs. Unlike a cellphone, MiPad allows users to use both speech and touch. It has a microphone that turns on when a user selects a field. As the user taps the screen or scrolls, these actions help narrow down the possible spoken commands. MiPad operates on a Windows CE Pocket PC with a connected Windows 2000 machine that handles speech recognition. The speech recognition engine uses a combined context-free grammar and n-gram language model, while the spoken language understanding is based on a detailed chart parser and a dialog manager that plans conversations.",
        "reference": "Dr. Who is a Microsoft research project that aims to create a speech-focused system for interacting with technology. This system will serve as the foundation for a natural user interface called NET. The paper discusses MiPad's design and implementation, which are still in progress, as well as a preliminary user study comparing it to existing pen-based PDA interfaces. MiPad is a prototype application that shows how wireless personal digital assistants (PDAs) can benefit from integrating continuous speech recognition (CSR) and spoken language understanding (SLU) technologies. This allows users to perform common tasks using a multimodal interface and wireless technology. It aims to solve the issue of having to use small styluses or type on tiny keyboards on modern PDAs. MiPad does not rely solely on speech interaction, unlike a cellular phone. MiPad has a built-in microphone that turns on when a field is selected. When a user taps the screen or utilizes a built-in roller for navigation, this action reduces the range of potential instructions for spoken word comprehension. MiPad is currently operating on a Windows CE Pocket PC, with speech recognition being carried out on a Windows 2000 machine. The Windows CE Pocket PC is a PDA that runs the Windows Embedded Compact operating system. The Dr Who CSR engine combines a CFG and an n-gram language model. The Dr Who SLU engine uses a chart parser and a plan-based dialog manager.",
        "metrics": {
          "SARI": 46.107238475200376,
          "BLEU": 0.21120892105972303,
          "FKGL": 10.9
        }
      },
      {
        "source": "Little is known about how viable new mobile applications emerge in complex commercial environments. Research in the sociology of technology suggests that the evolution of new applications is a process of social interaction between multiple constituencies, aiming to create a common definition of mobile technology problems and solutions that is obvious only in retrospect. This theory is illustrated by the early evolution of the personal digital assistant (PDA). According to this view, established technological communities attempt to impose their own definitions of key problems and solutions on new mobile technology, thereby forcing newer players to provide a \"total system\" that completely redefines a new application area.",
        "prediction": "Little is understood about how new mobile apps develop in complicated business settings. Studies in technology sociology indicate that the growth of new apps happens through social interactions among different groups. These groups work to create a shared understanding of mobile technology issues and solutions that only make sense in hindsight. This idea is illustrated by the early development of personal digital assistants (PDAs). In this view, established tech communities try to impose their definitions of important problems and solutions on new mobile technologies, which pressures newer companies to create a \"total system\" that redefines the application area completely.",
        "reference": "We don't know much about how new mobile apps succeed in complicated business environments. Research in the sociology of technology indicates that the development of new applications involves social interaction among different groups. These groups work together to establish a shared understanding of mobile technology challenges and solutions, which often becomes clear only in hindsight. The PDA's early evolution exemplifies this theory. This idea suggests that existing technology groups try to define the problems and solutions for new mobile tech, which puts pressure on new players to create a total system that redefines the application area.",
        "metrics": {
          "SARI": 47.807890124514316,
          "BLEU": 0.2540137131267564,
          "FKGL": 14.6
        }
      },
      {
        "source": "In the ubiquitous environment, mobile terminals like personal digital assistant or cellular phone are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. Extracting images and text from Web content, u-PaV generates a Macromedia Flash file, which is a popular multimedia format. Although the presentation of Web content depends on the terminal type, the Web interface of mobile terminals is designed almost the same as that of personal computers. However, comparing mobile and desktop terminals, some difference in device characteristics, such as screen resolution or sound faithfulness, affect the presentation on the terminal. Moreover, when changing the access style between interactive and passive, the presentation should be adjusted for the terminal type because the lacking information is usually not perceived after transformation. To adjust the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content by the logotype of the keyword. Third, we segment the picture of the content into pieces and present one after another using visual effects. These procedures can be applied to any type of terminal.",
        "prediction": "In today's world, mobile devices like smartphones and digital assistants are commonly used to access the internet. We created a system called u-PaV that changes web content into a format suitable for easy viewing on mobile devices. It extracts images and text from web pages and turns them into a Macromedia Flash file, a well-known multimedia format. While mobile devices and personal computers have similar web designs, there are differences between them, such as screen size and sound quality, that affect how the content is displayed. Additionally, when switching from an interactive to a passive viewing style, the content needs to be adjusted for the device because some details might be missed after transformation. To customize the display of web content for different devices, we devised a visual presentation method. First, we analyze keywords to show the emotional tone of a webpage by changing the background color. Second, we highlight the main topic using the keyword's logo. Third, we break down images into parts and show them one at a time with visual effects. This approach can work on any type of device.",
        "reference": "In the ubiquitous environment, devices like personal digital assistants or cell phones are used to access the Internet. We created a system called ubiquitous/universal passive viewer (u-PaV) that converts Web content into a format suitable for easy viewing on mobile devices. u-PaV takes text and images from the Web and creates a multimedia file in Macromedia Flash format, a widely used multimedia format. Mobile devices and personal computers have similar Web interfaces, although they may look different depending on the device. Mobile and desktop terminals have some differences in device characteristics, such as screen resolution and sound quality, which can affect how content is presented on the device. When switching between interactive and passive access styles, the presentation needs to be adjusted for the device type because some information may be lost during transformation and not noticed by the user. We developed a visual presentation method to customize how Web content is displayed based on the device type. First, we analyze keywords to capture the emotional tone of a web page and adjust the screen's background color accordingly. Then, we use the keyword's logo to highlight the main subject of the content. Finally, we divide the content's image into sections and present them one by one using visual effects. These steps can be used on any type of device.",
        "metrics": {
          "SARI": 51.19313033593726,
          "BLEU": 0.26027232629853525,
          "FKGL": 10.4
        }
      },
      {
        "source": "We have developed two-way location aware system, named as “The Beijing Explorer”, which exchanged positioning information and users’ situation to one another using a PDA (Personal Digital Assistant) with built-in wireless LAN and a GPS (Global Positioning System) receiver in real time. Users can see their position and their chats on the screen of a PDA using the system real-timely. The system was used for the guidance of the Palace Museum (Forbidden City) in Beijing, China. We carried on experiments two times using the system. The results of experiments show that the service using positioning data and sharing contents were valuable and interesting. The correct positioning information is important for the guidance system.",
        "prediction": "We created a two-way location-aware system called \"The Beijing Explorer.\" This system allows users to share their locations and situations in real time using a PDA that has wireless internet and GPS. Users can view their position and chat messages on the PDA's screen. The system was tested at the Palace Museum in Beijing, China. We conducted two experiments with it, and the results showed that using location data and sharing content was both useful and engaging. Accurate positioning information is crucial for the guidance system.",
        "reference": "We created a location-aware system called \"The Beijing Explorer\" that uses a PDA with wireless LAN and GPS to exchange positioning and user information in real-time. Users can view their position and chats in real-time on the PDA screen when using the system. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system. The experiment results indicate that the service was useful and engaging because it shared positioning data and content. Accurate positioning information is crucial for the guidance system.",
        "metrics": {
          "SARI": 59.95102643424276,
          "BLEU": 0.38718523467708904,
          "FKGL": 10
        }
      },
      {
        "source": "A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. It is personal and private in an unprecedented manner, and its modus operandi is such that network and power failures will not be rare. Designing distributed systems where PDAs are treated as \"first class citizens\" is a  private :[59],\"assets (electronic money, keys for authentication and opening doors) will be stored in PDAs. Ownership and control of these :[59],\"assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private  :[105],\"introduce the \"open-ended argument\" to describe the design strategy we used for designing a system that is designed to reveal information to the user (as opposed to hide it). We argue and show that when systems are designed this way, the user (a human) is better able to control the system and his personal data, as he can make better decisions than the system itself based on qualitative assessment of the provided information. The system we have designed and implemented under this design guidelines is presented and discussed.",
        "prediction": "A personal machine, known as a private machine and seen in devices like Personal Digital Assistants, is very different from regular machines. It offers a new level of personal privacy, and issues like network or power outages can occur frequently. We need to design distributed systems that treat PDAs as important devices. Personal assets like digital money and keys for access should be stored in PDAs, and users should have ownership and control over these assets. This needs to be part of how we design systems for private use. We propose a design strategy called the \"open-ended argument,\" which focuses on giving information to the user instead of hiding it. When systems are designed this way, users can better control their data and make informed decisions based on the information provided, as they can evaluate it more effectively than the system. We present and discuss the system we designed using these guidelines.",
        "reference": "A Personal Digital Assistant (PDA) used as a private machine, is fundamentally distinct from traditional machines because it is a personalized device. It offers an unprecedented level of personal and private experience, and its operation is designed to handle network and power failures, which are expected to occur frequently. In designing distributed systems, PDAs are given high importance and treated as \"first class citizens\". This means that important assets like electronic money and authentication keys will be stored in PDAs. The user should retain ownership and control of these assets and the media that stores and communicates them. This principle should be incorporated into the design of private systems. We introduced the concept of the \"open-ended argument\" to describe our design strategy, which focuses on creating systems that are intended to provide information to the user rather than hiding it. We argue and demonstrate that when systems are designed in this manner, the user (a human) gains improved control over both the system and their personal data. By enabling users to make decisions based on qualitative assessment of the information provided, they are better equipped to make informed choices compared to relying solely on the system's automated decisions. We have created a system based on the design guidelines and we will explain it.",
        "metrics": {
          "SARI": 41.57039672663159,
          "BLEU": 0.10973092630464545,
          "FKGL": 10.7
        }
      },
      {
        "source": "Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the development and initial evaluation of a personal digital assistant (PDA)-based decision support system (DSS) for pediatric depression screening in ages 8 to 18 years of age by pediatric advanced practice nurse (APN) students. Three aspects are described: selection of depression screening instrument; integration of the instrument into the PDA; and quantitative (usage) and qualitative (focus group) evaluation. Only one third of eligible patients were screened. Twenty percent of those screened were identified as at risk for mood disorder. The barriers to screening identified through focus groups included a lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, as well as perceived or real cultural barriers. Suggestions for educational, research, and interventions to integrate clinical based PDA-based screening are discussed.",
        "prediction": "Depression is often overlooked in children and teenagers. This paper aims to explain how a personal digital assistant (PDA)-based decision support system was developed and evaluated for screening depression in youth aged 8 to 18 by advanced practice nurse students. Three main areas are covered: choosing a depression screening tool, adding it to the PDA, and evaluating its use through numbers and focus groups. Only one third of eligible patients were screened and 20 percent of those screened were found to be at risk for mood disorders. Focus groups revealed barriers to screening, such as lack of time, knowledge, protocols for intervention, resources for referrals, issues with PDA usability, and concerns about motivation and comfort among preceptors, as well as cultural barriers. The paper discusses suggestions for education, research, and ways to improve PDA-based screening in clinical settings.",
        "reference": "Depression often goes unnoticed in many pediatric settings. This paper aims to explain the creation and early assessment of a decision support system (DSS) that uses personal digital assistants (PDAs) for screening pediatric depression among children aged 8 to 18 years, carried out by pediatric advanced practice nurse (APN) students. Three aspects are discussed: choosing a depression screening tool, incorporating the tool into the PDA, and evaluating it through quantitative (usage) and qualitative (focus group) assessments. Only one third of eligible patients were screened. 20% of the screened patients were found to be at risk for a mood disorder. Barriers to screening were identified through focus groups, including lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, and cultural barriers. The paper discusses suggestions for integrating PDA-based screening into clinical practice through education, research, and interventions.",
        "metrics": {
          "SARI": 46.73266183008134,
          "BLEU": 0.32133101860735225,
          "FKGL": 13.4
        }
      },
      {
        "source": "This study explored the potential of the application of wireless and mobile computing technologies to be used in improving the efficiency of patient care and education and future developments in information and communication technologies to support healthcare professionals and medical students in healthcare research, medical education and training. The design used for this study was a systematic review of published materials obtained from EMBASE and MEDLINE online databases, and the Cochrane Library database, including personal observations. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistant with expected growth of more than 75% by year-end 2007. In addition, wireless and mobile computing technologies allows Personal Digital Assistant to connect directly to networks or the Internet. Studies relating to processes of patient care and should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies is only beginning to take its first step in improving patient care and education. They have shown a positive impact on patient safety, health care efficiency, and ultimately patient satisfaction.",
        "prediction": "This study looked at how wireless and mobile computing technologies can improve patient care and education. It also examined how future technology can help healthcare professionals and medical students in research and training. The study used a systematic review of articles from databases like EMBASE, MEDLINE, and the Cochrane Library, along with personal observations. Currently, over 50% of healthcare professionals and medical students use Personal Digital Assistants, and this number is expected to grow by more than 75% by the end of 2007. These technologies allow Personal Digital Assistants to connect directly to networks or the Internet. Research on patient care processes should consider mobile computing as a way to save time. Wireless and mobile computing is just starting to improve patient care and education, showing positive effects on patient safety, healthcare efficiency, and overall patient satisfaction.",
        "reference": "This study examined how wireless and mobile computing technologies can enhance patient care and education. It also explored how advancements in information and communication technologies can support healthcare professionals and medical students in research, education, and training. We conducted a systematic review for this study, analyzing published materials from databases such as EMBASE, MEDLINE, and the Cochrane Library, as well as personal observations. Over 50% of healthcare professionals and medical students currently use Personal Digital Assistants, and this number is expected to grow by more than 75% by the end of 2007. Wireless and mobile computing technologies enable Personal Digital Assistants to connect directly to networks or the Internet. Studies should examine whether mobile computing technologies can save time in patient care processes. Wireless and mobile computing technologies are still in the early stages of improving patient care and education. These technologies have positively impacted patient safety, healthcare efficiency, and patient satisfaction.",
        "metrics": {
          "SARI": 67.88348611015515,
          "BLEU": 0.5005503389674065,
          "FKGL": 13.3
        }
      },
      {
        "source": "Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore practical training is an effective learning opportunity for developing the practical nursing care ability of nursing students. Moreover, at hospitals, which are important training facilities, with regard to medical safety, the use of learning tools that produce electrical waves is not possible. So, we created a learning support environment that facilitates the imagination of nursing techniques, and enables effective preparation, review, and learning at anytime and anywhere using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named \"digital nursing dictionary\" that we developed and the evaluation of the practices using it.",
        "prediction": "Nursing students learn a lot through hands-on training by working with patients. This practical training helps them improve their nursing skills. However, at hospitals, certain learning tools that use electrical waves cannot be used for safety reasons. To address this, we created a learning environment that helps students imagine nursing techniques and allows them to prepare and review their learning anytime and anywhere using a portable digital assistant (PDA). This paper discusses the educational materials we developed called the \"digital nursing dictionary\" and evaluates how effective they are in practice.",
        "reference": "Nursing students can learn through practical training and by interacting with patients in real medical settings. Practical training is effective for developing the practical nursing care ability of nursing students. Moreover, hospitals, which are important training facilities, do not allow the use of learning tools that produce electrical waves due to medical safety concerns. We developed a learning support environment that helps nursing students visualize and practice techniques. It allows for convenient preparation, review, and learning using a portable digital assistant (PDA) device, enabling anytime and anywhere practical training. We developed an educational tool called the 'digital nursing dictionary' and evaluated its effectiveness, as outlined in this paper.",
        "metrics": {
          "SARI": 36.077019186118434,
          "BLEU": 0.12644272777886614,
          "FKGL": 12.7
        }
      },
      {
        "source": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. Manual reconstruction of web pages for these devices would ease the problem; however, the large variation of display capabilities will greatly increase the burden of web page designers since they have to customize a web page for each possible display device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "prediction": "The small size and low resolution of mobile devices make it hard to use web applications. Web pages are often too big for the screens of devices like PDAs and WebTVs. Fixing pages for each type of device would be difficult for designers. This paper suggests a system that automatically splits a web document into smaller sections based on screen size and the document's structure. It also provides extra information like summaries to help users navigate. The system shows the segments and structure of a web document to help users find information more easily.",
        "reference": "Small screens and low resolutions on mobile devices are a major obstacle for the widespread adoption of web applications in a wireless environment. PDA screens are often too small to display entire web pages. Low-resolution devices like WebTV face the same problem. Manually reconstructing web pages for these devices would help, but the wide range of display capabilities would make it challenging for web page designers as they would have to customize each web page for every possible display device, increasing their workload. This paper suggests a system for segmenting and presenting documents. The system divides a web document into logical segments automatically, using information about the screen size, document structure, and content. The system also extracts summaries and overviews to make navigation easier. The system optimizes the use of the screen for information finding by showing the segments and structure of a web document.",
        "metrics": {
          "SARI": 42.71381603367457,
          "BLEU": 0.1260227968739712,
          "FKGL": 8.2
        }
      },
      {
        "source": "Take 8-bit microcontroller (C8051F005) as the nucleus and study a kind of voltage harmonic monitor device that satisfies the long-range monitor. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). This device has two work modes: ”native” and ”remote”. It also can communicate with monitor center through telephone line, serial port, IC card, PDA (Personal Digital Assistant), etc.",
        "prediction": "This study focuses on an 8-bit microcontroller called C8051F005 to design a device that monitors voltage harmonics over long distances. The device uses a process called Fast Fourier Transform (FFT) for harmonic analysis. It has two modes of operation: \"native\" and \"remote.\" Additionally, it can connect to a monitoring center using various methods like telephone lines, serial ports, IC cards, and PDAs.",
        "reference": "Study a voltage harmonic monitor device for long-range monitoring using the 8-bit microcontroller C8051F005 as the core. The harmonic analysis uses FFT. The device has two modes: \"native\" and \"remote\". It can communicate with a monitoring center via different methods such as telephone line, serial port, IC card, and PDA (Personal Digital Assistant).",
        "metrics": {
          "SARI": 41.33561180069954,
          "BLEU": 0.18898744561308461,
          "FKGL": 10.5
        }
      },
      {
        "source": "With network and small screen device improvements, such as wireless abilities, increased memory and CPU speeds, users are no longer limited by location when accessing on-line information. We are interested in studying the effect of users switching from a large screen device, such as a desktop or laptop to use the same web page on a small device, in this case a PDA (Personal Digital Assistant). We discuss three common transformation approaches for display of web pages on the small screen: Direct Migration, Linear and Overview. We introduce a new Overview method, called the Gateway, for use on the small screen that exploits a user’s familiarity of a web page. The users in an initial study prefer using the Gateway and Direct Migration approach for web pages previously used on the large screen, despite the common Linear approach used by many web sites.",
        "prediction": "With improvements in networks and small screens, like wireless capabilities and faster memory and processors, users can now access online information from anywhere. We are studying how users transition from large devices, like desktops and laptops, to small devices, specifically Personal Digital Assistants (PDAs), while using the same web page. We explore three common ways to display web pages on small screens: Direct Migration, Linear, and Overview. We also introduce a new approach called the Gateway, which takes advantage of users' familiarity with a web page. In an initial study, users preferred the Gateway and Direct Migration methods for websites they had previously accessed on large screens, even though many websites use the Linear method.",
        "reference": "Improvements in network and small-screen devices, such as wireless capabilities, faster CPU speeds, and increased memory, have eliminated location restrictions for accessing online information. We want to study the impact of users switching from a desktop or laptop to a PDA (Personal Digital Assistant) when accessing the same web page. We are looking at three common ways to display web pages on small screens: Direct Migration, Linear, and Overview. Gateway is our new Overview method for small screens that use the user's familiarity with a web page. Many websites use the Linear method, but an initial study showed that users prefer using the Gateway and the Direct Migration method for web pages.",
        "metrics": {
          "SARI": 50.31882445179985,
          "BLEU": 0.30532927221492123,
          "FKGL": 12.3
        }
      },
      {
        "source": "The limitations and constraints of mobile systems need to be adequately addressed in software development. We have been developing a taxonomy of risks based on SEIu0027s risk questionnaire and applied it during the development of a negotiation support system for a Personal Digital Assistant (PDA). In our planned research, we will explore how we can better integrate existing risk management strategies and Agile Methods (AM).",
        "prediction": "It is important to consider the limitations of mobile systems during software development. We created a list of risks using SEI's risk questionnaire while developing a negotiation support system for a Personal Digital Assistant. In our future research, we will look into how to better combine current risk management techniques with Agile Methods.",
        "reference": "During software development, restrictions due to mobile systems need to be addressed. Based on SEI's risk questionnaire, we developed a classification of risks and applied it while developing a negotiation support system for a PDA. Risk management strategies and Agile Methods and how we can integrate them better will be studied in our future research.",
        "metrics": {
          "SARI": 46.92547268805815,
          "BLEU": 0.20765973021044618,
          "FKGL": 12.6
        }
      },
      {
        "source": "Expansions of mobile services and private data have required increased level of protection. Speaker recognition, one of the biometric technologies, arises lots of research interests for its simple, cheap and convenient characteristics. In this paper, a robust speaker recognition system which facilitates reliable authentication with multi-channel voices is presented. A large multi-channel corpus, including mobile phone, personal digital assistant (PDA), telephone and microphone, is collected to evaluate the system performance.",
        "prediction": "The growth of mobile services and personal data has led to a need for better protection. Speaker recognition, a type of biometric technology, has gained a lot of attention because it is simple, inexpensive, and convenient. This paper introduces a strong speaker recognition system that allows for reliable identification with voices from different sources. A large collection of voice recordings from mobile phones, personal digital assistants, telephones, and microphones is gathered to test how well the system works.",
        "reference": "The growth of mobile services and private data necessitates higher levels of protection. Speaker recognition, a technology using the voice of users to identify them, leads to a lot of research interest because of its simple, cheap, and convenient characteristics. This work presents a powerful speaker recognition system that makes the authentification with multi-channel voices easier. Data are collected on mobile phones, PDAs, telephones, and microphones to evaluate the system's performance.",
        "metrics": {
          "SARI": 53.537635749902336,
          "BLEU": 0.23654315459099468,
          "FKGL": 12.1
        }
      },
      {
        "source": "This paper presents a method to generate unique and nevertheless highly random pseudonyms in a distributed environment. More precisely, each user can now generate his pseudonym locally in his personal security environment, e.g. in his smart card or his personal digital assistant. There is no need for any information interchange between issuing parties or global data (especially keys), except unique identifiers for each user and each device of the system. Additionally the holder can prove, that he generated a specific pseudonym without revealing his identity and he can reveal his identity by disclosing the pseudonym. Whereas the verifier of a disclosed pseudonym can be sure, that the presenter of the pseudonym is the holder of the pseudonym (i.e. the person which originally generated it). The identifier of the user and the identifier of the useru0027s device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form.",
        "prediction": "This paper describes a way for users to create unique and random pseudonyms on their own devices. Each user can generate their pseudonym in a secure space like a smart card or personal digital assistant. There is no need to share sensitive information or data between different parties, except for unique identifiers for each user and their devices. The user can prove they created a specific pseudonym without showing their identity, and they can reveal their identity by sharing the pseudonym. The verifier can trust that the person presenting the pseudonym is the original creator. User and device identifiers will be used to generate unique pseudonyms, but both will be stored in an encrypted form to protect the user's anonymity.",
        "reference": "This work presents a method to generate unique and random pseudonyms in a distributed system. Every user can now create his pseudonym locally in their own personal security environment, e.g. in his chip card or PDA. No information exchange is required between issuing parties or global data, except for unique identifiers for each user and device in the system. The holder can also prove that he generated a distinct pseudonym without revealing his identity; by divulging it, he can reveal his identity. In that case, the verifier of a divulgated pseudonym can be sure that the one who presents the pseudonym is holding it. (i.e. the person that originally created it) The user identifier and device's identifier will be used to create unique pseudonyms but to ensure anonymity with pseudonyms, both identifiers will be stored within the pseudonym in encrypted form.",
        "metrics": {
          "SARI": 36.95604329772863,
          "BLEU": 0.1575664902009553,
          "FKGL": 11.1
        }
      },
      {
        "source": "In todayu0027s mobile information society, location-based services play an increasingly important role. These services are to be accessed by users with a mobile end device for the use of city maps, route planning, navigation, or traffic information. Mobile end devices, however, do not have computing power or storage capacity comparable to that of a personal computer or laptop. These deficits can be bypassed by employing special methods in development of the respective applications. This paper outlines the experience gained in the development of a prototype for route computation in public transport networks to be used on a personal digital assistant. Subdivision of the route computation process into various steps for minimizing the computation expenditure on the mobile end device plays a significant role.",
        "prediction": "In today's mobile information society, location-based services are becoming more important. These services help users with mobile devices to access city maps, plan routes, navigate, and get traffic information. However, mobile devices cannot match the computing power or storage of personal computers or laptops. To address this issue, special methods are used in developing these applications. This paper shares experiences from creating a prototype for route calculation in public transport networks for personal digital assistants. Dividing the route calculation process into different steps is essential for reducing the computing demands on mobile devices.",
        "reference": "Location-based service's role in today's mobile information society is increasing . Users can access city maps, route planning, navigation, or traffic information with their mobile devices. Compared to laptops or personal computers, mobile devices do not have the same computing power or storage capacity. This shortage can be overcome by using specific methods during the development of the respective application. This paper shows the gained experience from developing a prototype designed for a PDA that calculates routes in public transport networks. Dividing the route computation process into multiple steps to reduce the computational load on the mobile device is crucial.",
        "metrics": {
          "SARI": 45.479884871126934,
          "BLEU": 0.16435828218646203,
          "FKGL": 12.9
        }
      },
      {
        "source": "DIANE is a digital assistant system that aims to fasten the doctor access to various informations at the hospital such as health care facility, medical records, and also human resource data. The fasten access could be achieved by implementing face recognition and live streaming as part of the digital assistant system.",
        "prediction": "DIANE is a digital assistant system designed to help doctors quickly access information at hospitals. This includes details about health care facilities, medical records, and staff data. It uses face recognition and live streaming to make access faster.",
        "reference": "DIANE is a digital assistant system that allows the doctor a faster access patient and hospital information. This could be done with face recognition and live streaming.",
        "metrics": {
          "SARI": 55.247650648465864,
          "BLEU": 0.21475571940851748,
          "FKGL": 9.4
        }
      },
      {
        "source": "Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time.",
        "prediction": "Digital assistants are becoming more common in our everyday lives. Users can do several tasks quickly while interacting with these assistants",
        "reference": "Digital assistants are becoming popular in our daily lives. It will allow users to do multiple tasks in a faster way.",
        "metrics": {
          "SARI": 43.79811226255775,
          "BLEU": 0.14762263352475288,
          "FKGL": 9.7
        }
      },
      {
        "source": "As extensive experimental research has shown individuals suffer from diverse biases in decision-making. In our paper we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, managerial decisions which are based on different levels of organizational complexity and different incentive systems suffer from biases known from descriptive decision theory. The results illustrate how biases in combination with each other and in different organizational contexts affect organizational performance. We find that, contrary to intuition, some combinations of biases significantly improve organizational performance while these biases negatively affect organizational performance when they occur separately. This might evoke considerations whether decision-making should be as rational as possible.",
        "prediction": "Research shows that people often have biases when making decisions. Our paper examines how these biases affect managers during group decision-making and the overall performance of their organizations. In our simulations, we looked at how managerial decisions are influenced by the complexity of the organization and different reward systems, leading to known biases. The results show that the way biases interact with each other and the context of the organization can influence performance in different ways. Surprisingly, some combinations of biases can actually improve organizational performance, whereas when these biases occur on their own, they tend to hurt performance. This raises questions about whether decision-making should aim to be as rational as possible",
        "reference": "Research showed that people suffer from diverse biases (disproportionate weight in favor of or against an idea) in decision-making. We analyzed the effects of decision-making biases of managers on organizational performance. In the simulations, managers decisions which are based on organizational skills and rules, suffer from several known biases. The results illustrate how combined biases in different contexts affect organizational performance. Contrary to expectations, some combinations of biases improve organizational performance. However, when they occur separately, they decrease performance. This raises questions about the importance of rationality in decision-making.",
        "metrics": {
          "SARI": 38.96879049470323,
          "BLEU": 0.1416742768382276,
          "FKGL": 14.2
        }
      },
      {
        "source": "Big data, the enhanced ability to collect, store and analyze previously unimaginable quantities of data in tremendous speed and with negligible costs, delivers immense benefits in marketing efficiency, healthcare, environmental protection, national security and more. The central tenets of the current privacy framework, the principles of data minimization and purpose limitation, are severely strained by the big data technological and business reality. As we increasingly interact with these artificial agents in unsupervised settings, with no human mediators, their seeming autonomy and increasingly sophisticated functionality and behavior, raises legal and philosophical questions. The focus on the machine is a distraction from the debate surrounding data driven ethical dilemmas, such as privacy, fairness and discrimination. The machine may exacerbate, enable, or simply draw attention to the ethical challenges, but it is humans who must be held accountable. Policymakers should seek to devise agreed-upon guidelines for ethical data analysis and profiling. Such guidelines would address the use of legal and technical mechanisms to obfuscate data; criteria for calling out unethical, if not illegal, behavior; categories of privacy and non-privacy harms; and strategies for empowering individuals through access to data in intelligible form.",
        "prediction": "Big data allows us to collect and analyze massive amounts of information quickly and cheaply, which helps in areas like marketing, healthcare, and security. However, current privacy rules that focus on limiting data collection and setting clear purposes are challenged by this technology. As we use automated systems more without human oversight, they seem more independent and complex, which brings up legal and ethical issues. The focus on machines can distract us from important discussions about privacy, fairness, and discrimination. While machines can highlight these problems, it's people who need to be responsible. Policymakers should create clear guidelines for ethical data use that tackle issues like data privacy, define inappropriate behaviors, and help people understand and access their own data.",
        "reference": "Big data (the ability to collect, store and analyze huge quantities of data in a really fast and cheap way) offers immense benefits in marketing, healthcare, environment, national security and more. The principles of privacy, limitation of data collection and use are stretched by the business and technological reality of big data. Our increasing interactions with these technologies without human supervision, and their growing autonomy, functionality and behavior raises legal and philosophical questions. The focus on the machine is a distraction from the debate on ethical dilemmas about data like privacy, fairness and discrimination. The machine may influence the ethical challenges, but the humans must remain responsible. Responsible people should find guidelines for ethical data analysis and collection. These guidelines would deal with mechanisms to protect data, criteria about unethical or illegal behavior, privacy harms, and strategies to give people access to data.",
        "metrics": {
          "SARI": 37.49254394776074,
          "BLEU": 0.08997952028029389,
          "FKGL": 13.5
        }
      },
      {
        "source": "Amazon's Echo, and Apple's Siri have drawn attention from different user groups; however, these existing commercial VUIs support limited language options for users including native English speakers and non-native English speakers. we conducted a usability study of the Google Home Smart Speaker with 20 participants including native English and non-native English speakers to understand their differences in using the Google Home Smart Speaker. The findings show that compared with their counterparts, the native English speakers had better and more positive user experiences in interacting with the device. It also shows that users' English language proficiency plays an important role in interacting with VUIs. The findings from this study can create insights for VUI designers and developers for implementing multiple language options and better voice recognition algorithms in VUIs for different user groups across the world.",
        "prediction": "Amazon's Echo and Apple's Siri have attracted different user groups, but they offer limited language options for both native and non-native English speakers. We conducted a usability study with 20 participants using the Google Home Smart Speaker to explore how different users interact with the device. Our findings indicate that native English speakers had a more positive experience compared to non-native speakers. Additionally, a user's English proficiency significantly affects their interaction with voice user interfaces (VUIs). These results can help VUI designers create better support for multiple languages and improve voice recognition for diverse users worldwide.",
        "reference": "The VUIs (voice user interface) like Amazon’s Echo or Apple’s Siri are popular nowadays. However, they have limited language options for users. We studied the usability of the Google Home Smart Speaker with native and non-native English speakers to understand their differences in using this device. The results show that the native English speakers had a better experience with the device than the non-native. It also shows that language skills play an important role. The results of this study can help VUI developers for improving language options and voice recognition in the devices.",
        "metrics": {
          "SARI": 42.02713333195846,
          "BLEU": 0.17957505672563398,
          "FKGL": 13.1
        }
      },
      {
        "source": "the smart speakers cannot distinguish human voice from machine voice. a method to identify which of human or machine is sending voice commands to a smart speaker is desired. to prevent such machine-voice based attacks to a smart speaker in absence of residents, we propose a system consisting of a speaker and a microphone array to detect the existence of a human nearby, supposing it can be incorporated in a smart speaker in the future.",
        "prediction": "Smart speakers often cannot tell the difference between human voices and machine voices. We need a way to identify whether a human or a machine is giving voice commands to a smart speaker. To protect against attacks using machine voices when no one is home, we suggest a system that includes a speaker and a microphone array to sense if a human is nearby, which could be added to smart speakers in the future.",
        "reference": "The smart speakers cannot differentiate between human voice and machine voice. A method to identify if the voice command is from a human or a machine is required. We propose a system composed of a speaker and microphones to detect the presence of humans. It could help to prevent attacks on a smart speaker with a machine voice in absence of residents",
        "metrics": {
          "SARI": 37.073087045525185,
          "BLEU": 0.13072257374897137,
          "FKGL": 10.6
        }
      },
      {
        "source": "As smart speakers with voice interaction capability permeate continuously in the world, more and more people will gradually get used to the new interaction medium–voice. Although speech recognition, natural language processing (NLP) have been greatly improved over the past few years, users still may encounter errors from time to time like “cannot understand”, “no requested audio resource (such as music)”, which can frustrate users. Therefore, when an error message is reported, it is vital that the smart speaker gives an effective and proper response. However, currently the response strategies adopted by leading smart speaker brands in China differed mainly on two dimensions: “apology or not” and “humor or neutral”. We explored user’s preference of response strategies under two error scenarios——“cannot understand” and “no requested audio resource”. Two dependent variables (satisfaction and perceived sincerity of response) were measured. The results showed that participants were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no significant impact on the perception of satisfaction and sincerity. But in the “cannot understand” scenario, humorous expression decreased perceived sincerity.",
        "prediction": "As smart speakers become more common, many people are getting used to using voice for interaction. Although speech recognition and natural language processing have improved, users still face errors like \"cannot understand\" or \"no requested audio resource\" which can be frustrating. It's important for smart speakers to respond effectively to these errors. Currently, the main brands of smart speakers in China have different response strategies based on whether they apologize and whether they use humor. We looked at user preferences for these responses in two error situations: \"cannot understand\" and \"no requested audio resource.\" We measured two things: user satisfaction and perceived sincerity of the response. The results showed that users were more satisfied and felt the responses were more sincere when the smart speaker apologized in both situations. In the \"no requested audio resource\" case, humor did not affect satisfaction or sincerity. However, in the \"cannot understand\" case, using humor reduced perceived sincerity.",
        "reference": "Smart speakers are becoming popular around the world and people will get used to this new way of interacting with the voice. Even with the progress of speech recognition and natural language processing (interactions between computers and human language), users may still get errors like “cannot understand” or “no requested audio resource” which can frustrate users. So, it is important that the smart speaker gives an effective and proper response during an error message. The responses of the most popular brand of smart speakers are based on 2 elements, apology and humor. We studied user’s preference in two error scenarios : “cannot understand” and “no requested resource” Satisfaction of the user and perceived sincerity of the response were measured. The results showed that users were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no impact on the satisfaction or sincerity. But in the “cannot understand” scenario, humor decreased perceived sincerity.",
        "metrics": {
          "SARI": 41.54189934513873,
          "BLEU": 0.20314808948510402,
          "FKGL": 11.1
        }
      },
      {
        "source": "A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process (specifically, drug molecules, compounds in high throughput screening libraries, combinatorial chemistry building blocks and nondrug molecules). Significant differences were observed between the pharmacophore profiles obtained for the drug molecules and those obtained for the high-throughput screening compounds, which appear to be closely related to the nondrug pharmacophore distribution. It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the property-based optimization of compound selection and library design processes, thus improving the odds of success in lead discovery projects.",
        "prediction": "A pharmacophore analysis was used to study and compare different types of compounds important for drug discovery. This includes drug molecules, compounds from screening libraries, building blocks from combinatorial chemistry, and non-drug molecules. There were noticeable differences between the pharmacophore profiles for drug molecules and those from high-throughput screening compounds, which seem related to the patterns seen in non-drug compounds. It is suggested that looking at these pharmacophore profiles can help choose and design compounds more effectively, increasing the chances of success in finding new drug leads.",
        "reference": "A pharmacophore analysis investigated and compared different compounds of the drug discovery process. Significant differences were observed between the pharmacophore profiles of the drug molecules and the high-throughput screening compounds, which appear to be related to the nondrug pharmacophore distribution. It is suggested that the analysis could be used as an additional tool for the optimization of compound selection and library design processes..",
        "metrics": {
          "SARI": 33.543999812367616,
          "BLEU": 0.15111922271914643,
          "FKGL": 13
        }
      },
      {
        "source": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been previously designed for this express purpose, and provides a layered hierarchy of meaningful terms which can be linked to.",
        "prediction": "Using semantic web vocabulary to annotate bioassay protocols helps make experiment descriptions readable by machines. Protocols are written in short scientific English, which limits software analysis. With a detailed ontology, important information can be captured through a series of statements known as semantic web triples (subject, predicate, object). Proper annotation allows assays to be searched, grouped, labeled, and assessed in various ways, similar to other fields in drug discovery informatics. The BioAssay Ontology (BAO) was created for this purpose and provides a structured list of meaningful terms for linking.",
        "reference": "Annotation of bioassay protocols with web vocabulary can make experiment descriptions machine-readable. Protocols use concise scientific English which prevents most of analysis by software algorithms. There is a sufficient ontology, so the pertinent information can be written using semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated. The BioAssay Ontology (BAO) has been designed for this purpose, and provides many meaningful terms.",
        "metrics": {
          "SARI": 41.7250203889467,
          "BLEU": 0.2457440149239608,
          "FKGL": 13.8
        }
      },
      {
        "source": "Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Subsequent bioinformatic analysis is typically done with the help of open source tools, where each application performs a single step towards the final result. This situation leaves the bioinformaticians with the tasks to combine the tools, manage the data files and meta-information, document the analysis, and ensure reproducibility.",
        "prediction": "Next generation sequencing produces large amounts of data with billions of readings from many samples. Bioinformatic analysis usually involves using open source tools, each completing a specific part of the overall process. This means bioinformaticians have to combine the tools, organize the data and its details, keep track of the analysis, and make sure the results can be repeated.",
        "reference": "Next generation sequencing (NGS) produces datasets of billions of reads and thousands of samples. The analysis is typically done with open source tools, performing a single step towards the final result. The bioinformaticians have to combine the tools, manage the files and information, document the analysis, and ensure reproducibility.",
        "metrics": {
          "SARI": 35.00742194710898,
          "BLEU": 0.20842177184202676,
          "FKGL": 11
        }
      },
      {
        "source": "SPSS Clementinel2.0 statistical software was used to mine the association rules between Etiology and traditional Chinese medicine (TCM), Syndromes and TCM, Symptoms and TCM. The classic Apriori algorithm is useful to mine cases of influenza treated by contemporary famous old Chinese medicine.",
        "prediction": "SPSS Clementine 2.0 software was used to find patterns between causes of disease, traditional Chinese medicine, syndromes, and symptoms. The well-known Apriori algorithm helps analyze cases of influenza treated by notable traditional Chinese medicine practitioners.",
        "reference": "SPSS Clementinel2.0 statistical software was used to extract data and find relations between traditional Chinese medicine and other medicine. The classic Apriori algorithm is useful to find cases of influenza treated by old Chinese medicine.",
        "metrics": {
          "SARI": 43.95576416155034,
          "BLEU": 0.22256580677190393,
          "FKGL": 11.4
        }
      },
      {
        "source": "Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype and phenotype in humans and other species. a Bayesian framework for genotype estimation for mixtures of multiple bacteria, named as Genetic Polymorphisms Assignments (GPA) has reduced the false discovery rate (FDR) and mean absolute error (MAE) in single nucleotide variant (SNV) identification.",
        "prediction": "Finding antimicrobial resistant bacteria in metagenomics samples is important for public health and food safety. Next-generation sequencing technology helps us understand genetic differences and the relationship between genes and traits in humans and other species. A method called Genetic Polymorphisms Assignments uses a Bayesian approach to estimate genotypes in mixtures of different bacteria. This method has lowered the chances of incorrect discoveries and reduced errors in identifying single nucleotide variants.",
        "reference": "Identifying bacterias that are resistant to antimicrobial in samples of genetic material collected from the environment is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype (genetic code of an individual) and phenotype (physical characteristics of an indvidual) in humans and other species. The Genetic Polymorphism Assignements (GPA) is a framework which determines the genetic constitution of multiple bacteria while reducing the number of false positive results and improving the accuracy of calculations when identifying the variation of a single nucleotide.",
        "metrics": {
          "SARI": 37.48596847199019,
          "BLEU": 0.13239813221035454,
          "FKGL": 13.7
        }
      },
      {
        "source": "The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (altering the genetic sequence) and gene regulation (without altering the genetic sequence). These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "prediction": "The CRISPR/Cas9 system is a new technology used for precise changes to DNA, including editing genes and controlling gene activity without changing the DNA. To use this system effectively, researchers need to design single guide RNAs (sgRNAs) that are both effective and specific. However, creating these sgRNAs is difficult because it involves many factors to consider.",
        "reference": "The CRISPR/Cas9 system is a powerful technology for gene editing and gene regulation. These applications require the design of single guide RNAs (which are enzymes that can delete, insert or alter DNA) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "metrics": {
          "SARI": 35.77428870261439,
          "BLEU": 0.21828343098230216,
          "FKGL": 11.8
        }
      },
      {
        "source": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals; it might improve the health of individuals and communities, and, if successful, might be seen as a public good. However, enhancing future generations will require In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. Remarkably, the necessary involvement of women in an enhancing scenario has not been discussed by its proponents. The present discourse on moral obligations of future generations, although not referring to women, seems to imply that women might be required, morally, if not legally, to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. Despite the lack of a unified feminist conclusion in the discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.",
        "prediction": "Gene editing may help solve problems related to health care distribution. It could improve the health of future generations and align with public health goals, benefiting individuals and communities. Successful gene editing might be seen as a public good. However, improving future generations will rely on In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. Interestingly, the need for women's involvement in this process has not been widely addressed. Current discussions about our responsibilities to future generations suggest that women might be expected to reproduce using IVF, even if this is not explicitly stated. The process of enhancing future generations raises gender issues, especially if artificial wombs are not developed. These are complex topics that need to consider perspectives from both women and men. Although feminist viewpoints on human genome modification are varied, it is crucial to clarify the role of women in this context.",
        "reference": "Gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals. However, enhancing future generations will require In Vitro Fertilisation (IVF) and Pre-implantation Genetic Diagnosis. The necessary involvement of women in an enhancing scenario has not been discussed. The discourse on moral obligations of future generations seems to imply that women might be required to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. There are no unified feminist conclusion about the merits and risks of human genome modification. There is also an urgent need to clarify the role of women in this scenario.",
        "metrics": {
          "SARI": 29.467592720033693,
          "BLEU": 0.2391725467583803,
          "FKGL": 11.9
        }
      },
      {
        "source": "CRISPR-Cas is a tool that is widely used for gene editing. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins, which are powerful molecules that inhibit the CRISPR-Cas system, may have the potential to promote better utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. Additionally, more in-depth research on these proteins would help researchers to better understand the co-evolution of bacteria and phages. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "prediction": "CRISPR-Cas is a popular tool for editing genes. However, it can sometimes cause unexpected changes in other genes due to long-term activity. Anti-CRISPR proteins are strong molecules that can stop the CRISPR-Cas system, and they may help improve gene editing, especially in gene therapy. Moreover, further studies on these proteins could give insights into how bacteria and viruses have evolved together. Thus, gathering and combining information on different types of anti-CRISPRs is important.",
        "reference": "CRISPR-Cas is a tool used for gene editing. However, unintended genetic modifications may occur. Anti-CRISPR proteins (molecules that inhibit the CRISPR-Cas system) may improve utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. More research on these proteins would help to understand the co-evolution of bacteria and bacteriophages (virus that infects and replicates within bacteria). It is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "metrics": {
          "SARI": 31.86013345045793,
          "BLEU": 0.1150981922303983,
          "FKGL": 10.2
        }
      },
      {
        "source": "CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its function of gene perturbation, a variety of gene editing techniques have been developed to achieve different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen.",
        "prediction": "CRISPR has become a popular area of research because it is very effective and precise in editing DNA. Several gene editing methods have been created based on how CRISPR works to change genes. It can accurately break and fix specific parts of DNA, allowing genes to be removed, repaired, turned off, or turned on. The easy preparation and use of CRISPR/Cas9 tools make it a powerful method for testing many samples at once.",
        "reference": "CRISPR is a popular research area known for its efficiency and specificity in editing DNA sequences. A variety of gene editing techniques have been developed for different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it a powerful tool.",
        "metrics": {
          "SARI": 25.313209912297367,
          "BLEU": 0.1548289085615233,
          "FKGL": 8.1
        }
      },
      {
        "source": "The CRISPR-Cpf1 system has been successfully applied in genome editing. However, target efficiency of the CRISPR-Cpf1 system varies among different gRNA sequences. Using machine learning technology, a SVM model was created to predict target efficiency for any given gRNAs. the first web service application, CRISPR-DT (CRISPR DNA Targeting), to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and specificity is available.",
        "prediction": "The CRISPR-Cpf1 system is useful for editing genes, but its effectiveness varies with different gRNA sequences. To address this, a machine learning model called SVM was developed to predict how well certain gRNAs will work. There is now a web service called CRISPR-DT that helps users create the best gRNAs for the CRISPR-Cpf1 system, taking into account both effectiveness and accuracy.",
        "reference": "The CRISPR-Cpf1 system (a DNA-editing technology) has been successfully applied in gene editing. However target efficiency of the CRISPR-Cpf1 system varies among different gRNA (piece of RNA that serves as a guide to DNA-targeting enzymes) sequences. Using machine learning technology, a support-vector machine model (algorithm used for data classification) was created to predict the target efficiency for any given gRNAs. CRISPR-DT (CRISPR DNA Targeting) is available. It is the first web service application to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and target specificity (recognition mecanism).",
        "metrics": {
          "SARI": 20.21595339396289,
          "BLEU": 0.06297902312651597,
          "FKGL": 10
        }
      },
      {
        "source": "Emerging self-driving vehicles are vulnerable to different attacks due to the principle and the type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via vehicular ad hoc networks (VANETs). VANETs add new threats to self-driving vehicles that contribute to substantial challenges in autonomous systems. These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for self-driving and semi self-driving cars. The hybrid detection system relies on the Back Propagation neural networks (BP), to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.",
        "prediction": "Self-driving cars are at risk of various attacks because of how they communicate. These cars mainly use a system called vehicular ad hoc networks (VANETs) for communication, which introduces new security threats. Some of the attacks that can affect these vehicles include Sybil attacks, Denial of Service (DoS), black hole, grey hole, and wormhole attacks. This paper presents a smart security system aimed at protecting the external communications of self-driving and semi self-driving cars. The proposed system uses Back Propagation neural networks (BP) to detect Denial-of-Service (DoS) attacks in VANETs. Experiments show that this BP-IDS can successfully identify malicious vehicles in both self-driving and semi self-driving cars",
        "reference": "Self-driving vehicles are vulnerable to different attacks because of their communication systems. These vehicles use external communication via a network. This network add new threats to self-driving vehicles that contribute to the challenges in autonomous systems. With these communication systems, self-driving vehicles are vulnerable to many types of malicious attacks on the network. In this paper, we propose an intelligent security system to secure external communications for self-driving cars. The system will detect a common type of attack in the network: Denial-of-Service (attack trying to make network resource unavailable). The results show that the intrusion detection system is capable of identifying malicious vehicles in self-driving vehicles.",
        "metrics": {
          "SARI": 25.6200564786697,
          "BLEU": 0.0592164472920286,
          "FKGL": 12.6
        }
      },
      {
        "source": "Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications. When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, might become deadlocked, or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols, while ensuring safety.",
        "prediction": "Self-driving vehicles are quickly advancing and will be important for future transport. A key challenge for these vehicles on public roads is safely working together with other vehicles using sensors and communication systems. When self-driving cars enter the same space at the same time, they might crash into each other, get stuck, or have to stop suddenly, which can be uncomfortable or unsafe for passengers. In this study, we look at how self-driving cars can safely navigate merge points where two lanes with different priorities come together. We propose a safe protocol called the Autonomous Vehicle Protocol for Merge Points, which allows self-driving cars to communicate and cooperate with each other and human-driven vehicles. Our simulation results indicate that this protocol improves traffic flow while ensuring safety compared to simpler traffic protocols",
        "reference": "Self-driving vehicle technologies are progressing and are expected to play a significant role in the future of transportation. A main challenge for self-driving vehicles on public roads is the safe cooperation, collaboration and communication with other vehicles. When self-driving vehicles are in the same area, they might collide, become blocked, or suddenly brake and make uncomfortable or unsafe the passengers. We study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other vehicles (self-driving or human-driven). Our simulation results show that our traffic protocol leads to a better traffic flow, while ensuring safety.",
        "metrics": {
          "SARI": 35.4790435781281,
          "BLEU": 0.18346919808265427,
          "FKGL": 13.1
        }
      },
      {
        "source": "This paper provides a holistic study of how stock prices vary in their response to financial disclosures across different topics. Thereby, we specifically shed light into the extensive amount of filings for which no a priori categorization of their content exists. For this purpose, we utilize an approach from data mining - namely, latent Dirichlet allocation - as a means of topic modeling. This technique facilitates our task of automatically categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings from U.S. companies. We then evaluate the subsequent stock market reaction. Our empirical evidence suggests a considerable discrepancy among various types of news stories in terms of their relevance and impact on financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit rating, but also for disclosures regarding business strategy, the health sector, as well as mergers and acquisitions. Our results yield findings that benefit managers, investors and policy-makers by indicating how regulatory filings should be structured and the topics most likely to precede changes in stock valuations.",
        "prediction": "This paper examines how stock prices react to different financial disclosures. We focus on a large number of filings that don't have a clear category. To do this, we use a data mining method called latent Dirichlet allocation for topic modeling. This helps us automatically categorize over 70,000 regulatory 8-K filings from U.S. companies. We then look at the stock market's reaction to these filings. Our findings show that different types of news stories have varying effects on financial markets. For example, we see significant changes in stock prices in response to earnings reports and credit ratings, as well as news about business strategy, the health sector, and mergers. Our results offer insights for managers, investors, and policy-makers on how regulatory filings should be organized and what topics are likely to impact stock values.",
        "reference": "This paper studies how stock prices vary with financial disclosures. We particularly study the many uncategorized filings. We use latent Dirichlet allocation for topic modeling. This helps automatically predict the categories based on the content of 70,000 regulatory 8-K filings from U.S. companies, which announce significant events relevant to shareholders. We then evaluate the stock market reaction. Evidence suggests a large difference among types of news stories in relevance and impact on financial markets. For example, we find unusual returns in response to disclosures about earnings results, credit rating, business strategy, the health sector, mergers and acquisitions. Our results are useful for managers, investors and policy-makers. They show how to structure regulatory filings and which topics influence stock valuations.",
        "metrics": {
          "SARI": 55.284739307052014,
          "BLEU": 0.31880721356082725,
          "FKGL": 11
        }
      },
      {
        "source": "A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.",
        "prediction": "A major challenge of using deep learning is the limited data for training. While there are some datasets for popular tasks like recognizing items and classifying images for self-driving cars, the data is much scarcer for industrial robotics. In past research, we trained a multi-objective Convolutional Neural Network (CNN) to identify robot bodies in images and estimate their joint positions using just a 2D image. This was limited to robots made by Universal Robots (UR). In this study, we expand our method to a new robot arm, the Kuka LBR iiwa, which looks different and has an extra joint. Instead of gathering large datasets again, we collect several smaller datasets with a few hundred images each and apply transfer learning techniques on the CNN trained with UR robots. This helps adapt the model to the new robot with different shapes and visuals. We have shown that transfer learning works well in this area, needs smaller and well-organized training datasets, trains much faster, and achieves similar or better accuracy than our original method.",
        "reference": "Limited data available for training affects the use of deep learning techniques. Datasets are available for problems such as item recognition and classification for self-driving cars, but very limited for industrial robotics. A multi-objective Convolutional Neural Network (CNN) was first trained to identify the robot in the image and find 3D positions of the joints using just a 2D image. But this only used robots made by Universal Robots (UR). Here we work with a new robot arm - Kuka LBR iiwa which looks very different and has an additional joint. We collect a number of smaller datasets and use transfer learning techniques on the CNN trained on UR robots to adapt it to a robot that looks different. Transfer learning requires smaller training datasets, learns faster and reaches similar or even better accuracy.",
        "metrics": {
          "SARI": 45.3657777522197,
          "BLEU": 0.23050919257741062,
          "FKGL": 10.5
        }
      },
      {
        "source": "We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a ResNet-18 variant, which is fed in images from the front of a simulated F1 car, and outputs optimal labels for steering, throttle, braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.",
        "prediction": "We present Ignition, a neural network designed to train self-driving cars in simulation. The model is based on ResNet-18 and takes images from the front of a simulated F1 car to produce the best labels for steering, acceleration, and braking. Notably, we do not specifically teach the model to recognize road features like the track outline or the distance to other cars; rather, we show that these features can be learned by the network on its own.",
        "reference": "Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments. This ResNet-18 variant model receives images from the front of a simulated car and uses them to steer, throttle, or brake appropriately. The model is not explicitly trained to detect road features such as track outline or distance to other cars, as these features can automatically be calculated by the network.",
        "metrics": {
          "SARI": 44.594297888963325,
          "BLEU": 0.1960871992371782,
          "FKGL": 12.1
        }
      },
      {
        "source": "As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) system. When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. Then, the second DNN for primary acoustic modeling employs both the bottleneck features tossed from the first DNN and the acoustics features. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "prediction": "Humanoid robots generate internal noise from their motors and other parts while moving, which affects how well they can recognize speech. This paper presents a new speech recognition system for these robots that is less sensitive to this noise. The system uses the on/off state of the motor as extra information to help identify important input features. We use specific features known as bottleneck features, which are helpful in deep neural network-based speech recognition. Initially, we use both the motor's on/off states and acoustic features as inputs for the first deep neural network for basic acoustic modeling. Then, a second deep neural network uses the bottleneck features from the first network along with the acoustic features. Testing our method on the TIMIT database showed a significant improvement of 11% in phoneme error rate compared to traditional systems.",
        "reference": "Internal noise generated by motors, fans and mechanical components severely decreases speech recognition accuracy, when the humanoid robot is moving or shaking its body. This paper presents a novel speech recognition system robust to ego-noise for humanoid robots. On/off state of the motor is used to find relevant input features. Bottleneck features have been successfully applied to automatic speech recognition (ASR) systems based on deep neural networks (DNN). The motor on/off state data and acoustic features are used as the input of the first deep neural network (DNN) for preliminary acoustic modeling. The second DNN for primary acoustic modeling uses both bottleneck features from the first DNN and acoustic ones. The proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, experimental results show that improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "metrics": {
          "SARI": 30.906135774093713,
          "BLEU": 0.1672182137299556,
          "FKGL": 10.9
        }
      },
      {
        "source": "Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model is made of residual convolutional blocks with hierarchical dilated skip connections joined in steps. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.",
        "prediction": "Using intelligent systems to recognize traffic signs can significantly lower accidents around the world. With self-driving cars becoming more common, automatically recognizing traffic and handheld signs on busy streets is a major challenge. Various machine learning methods, like Random Forest and SVM, along with deep learning techniques, have been developed for this purpose. While some of these methods perform well on specific datasets, they struggle with various traffic sign recognition tests. In this paper, we present a new architecture that excels across multiple benchmarks and outperforms existing methods. Our model uses residual convolutional blocks with hierarchical dilated skip connections. As a result, we achieved 99.33% accuracy on the German sign recognition test and 99.17% accuracy on the Belgian traffic sign classification test. Additionally, we introduce a new dilated residual learning technique that is efficient in both memory and computation.",
        "reference": "If intelligent systems recognize traffic signs, it can reduce car accidents. For self-driving cars, we need automatic recognition of traffic and hand signs. Machine learning techniques  could be used to classify traffic signs. They perform well on a particular dataset, but fail on multiple traffic sign recognition test datasets. We proposed a new and better method. This model is based on enhanced convolutional neural networks (a deep-learning approach often used for image recognition). This model was more than 99% correct on German and Belgian sign recognition datasets. This new model is very low in both memory and computational complexity.",
        "metrics": {
          "SARI": 40.470306520489615,
          "BLEU": 0.05757925121925417,
          "FKGL": 11.1
        }
      },
      {
        "source": "In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",
        "prediction": "This paper presents a method for using transfer learning to control self-driving cars. It allows a convolutional neural network (CNN) trained in one area to work in another area. Normally, a CNN for controlling a car takes an image from a front-facing camera and gives a steering command. For transfer learning, we added a new task that helps the CNN also measure how much a car might leave its lane. This is called lane departure level (LDL). The CNN from the original area, called the source network, helps train another module, called the target network, which makes steering commands for the new area. The steering commands from both networks are combined based on the LDL to control the car in the new area. We tested this method using two simulators, TORCS and GTAV, to represent the original and new areas. Results show that our method is better than others for safely and steadily controlling cars.",
        "reference": "A transfer learning method to control self-driving cars is proposed, in which a convolutional neural network (CNN) is trained on one domain then used for the same task in a different domain. A conventional CNN is designed to map a single front-facing camera image to control a car. CNN produces a steering command and a lane departure level (LDL, mechanism designed to warn the driver when the vehicle begins to move out of its lane). To enable the transfer learning, a new task module takes the output of the last convolutional layer as input. The CNN trained on the source domain is then used to train the target network. It takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are merged and used for car control in the target domain. Two simulators were used to evaluate the proposed method. The proposed method is more stable and safer for car control.",
        "metrics": {
          "SARI": 34.74394749139828,
          "BLEU": 0.15559191142293646,
          "FKGL": 7.6
        }
      },
      {
        "source": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market, based on a Limit Order Book (LOB) such as is common in major stock exchanges. Construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Research aimed at understanding the dynamics of this new style of financial market is hampered by the fact that no operational real-world exchange is ever likely to allow experimental probing of that market while it is open and running live, forcing researchers to work primarily from time-series of past trading data. Similarly, university-level education of the engineers who can create next-generation automated trading systems requires that they have hands-on learning experience in a sufficiently realistic teaching environment. BSE as described here addresses both those needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHuB.",
        "prediction": "This paper explains the design and use of the Bristol Stock Exchange (BSE), a simple simulation of a centralized financial market based on a system called Limit Order Book (LOB), which is used in big stock exchanges. The BSE was created because many main financial markets have become automated, with computers now handling trading instead of human traders. Researchers find it hard to study these new markets since real exchanges do not allow experiments while they are active, making them rely mostly on past trading data. Also, training engineers for future automated trading systems requires practical experience in a realistic setting. The BSE meets both of these needs: it has been used for teaching and research at a leading UK university since 2012, and the program code is available for free on GitHub.",
        "reference": "This paper describes the Bristol Stock Exchange (BSE), a minimal simulation of a centralised financial market based on a Limit Order Book (LOB), a type of order to buy or sell a security at a specific price or better. BSE was built because most of the financial markets became automated. Research tries to understand the dynamics of this automated financial markets but Trainee engineers also need a realistic learning experience to build automated trading systems. BSE has been successfully used for both teaching and research since 2012. BSE code is freely available on GitHuB.",
        "metrics": {
          "SARI": 47.43071276700257,
          "BLEU": 0.18023156409632182,
          "FKGL": 13.7
        }
      },
      {
        "source": "Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict the probabilities of contents to be requested in specific areas. To reduce the car-DC delay, MLP outputs are logged into MEC servers attached to roadside units. Second, in order to cache entertainment contents stylized for car passengers’ features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.",
        "prediction": "As self-driving cars become common, passengers will need new ways to stay entertained. Our research shows we can predict entertainment content needed in the car with over 98% accuracy and can reduce delays. However, getting these contents from the Data Center can slow down service because of the long time it takes to communicate between the car and the Data Center. To solve this, we suggest using deep learning methods in a system called Multi-access Edge Computing. First, we use a technique called Multi-Layer Perceptron to predict what contents people will want in certain areas, which helps to save time in getting content to the car. Next, we use Convolutional Neural Networks to determine the age and gender of passengers, which helps in selecting appropriate entertainment content. Finally, we frame our caching strategy as a math problem aimed at reducing download delays. We apply a method called Block Successive Majorization-Minimization to find the best solution.",
        "reference": "Once self-driving cars become reality and passengers no longer worry about it, they will need to find other forms of entertainment. Prediction accuracy was 98.04% for contents to be cached for self-driving cars. Such an approach can minimize delay. However, high communication delay can slow retrieval of entertainment content from Data Centers (DC). Self-driving cars can use deep learning caching to address these challenges. It is deployed on the Multi-access Edge Computing (MEC) structure that moves services from a centralized cloud to the network edges and closer to the customer. Multi-Layer Perceptron (MLP), a type of artificial neural network, is used to predict the content requested in specific areas. To reduce delay, MLP outputs are sent to MEC servers for roadside units. A Convolutional Neural Network (CNN) is used to predict passengers’ age and gender to cache adapted entertainment content. The self-driving car can therefore identify the contents to be downloaded and cached. Deep learning caching is formulated for the self-driving car to enhance entertainment services and minimize content downloading delay. A Block Successive Majorization-Minimization (BS-MM) technique solves the problem.",
        "metrics": {
          "SARI": 30.361960603467423,
          "BLEU": 0,
          "FKGL": 10.9
        }
      },
      {
        "source": "Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges.",
        "prediction": "Moral responsibility is an important issue in automated decision-making, which can be used in areas like self-driving cars and kidney exchanges",
        "reference": "Moral responsibility is a major concern in automated decision-making, from self-driving cars to kidney transplants.",
        "metrics": {
          "SARI": 30.716490299823633,
          "BLEU": 0.17894177180728454,
          "FKGL": 16.2
        }
      },
      {
        "source": "A key problem for autonomous car navigation is the understanding, at an object level, of the current driving situation. Addressing this issue requires the extraction of meaningful information from on-board stereo imagery by classifying the fundamental elements of urban scenes into semantic categories that can more easily be interpreted and be reflected upon (streets, buildings, pedestrians, vehicles, signs, etc.). A probabilistic method is proposed to fuse a coarse prior 3D map data with stereo imagery classification. A novel fusion architecture based on the Stixel framework is presented for combining semantic pixel-wise segmentation from a convolutional neural network (CNN) with depth information obtained from stereo imagery while integrating coarse prior depth and label information. The proposed approach was tested on a manually labeled data set in urban environments. The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to what is obtained from the semantic pixel-wise segmentation of a CNN alone.",
        "prediction": "A major challenge for self-driving cars is to understand the current driving situation by identifying objects around them. To tackle this, it is important to gather useful information from the car's cameras, categorizing things like streets, buildings, people, vehicles, and signs. This paper suggests a method that combines basic 3D map data with information from the cameras. A new system is introduced that merges pixel-level classifications from a convolutional neural network (CNN) with depth data collected from the cameras, while also considering previous depth and label information. The approach was tested on a dataset with labeled urban scenes. The results show that this method greatly improved the accuracy of identifying the main elements in urban scenes compared to using only the CNN's pixel classification.",
        "reference": "A key problem for autonomous car navigation is the understanding of the environment while driving. It needs to extract information from the sensors and cameras and classify the elements into categories so it can be easily analysed (streets, buildings, pedestrians, vehicles, signs, etc.). A method is to fuse 3D map data with classification of the sensors and cameras. This system combine image segmentation (simplify the representation of an image to facilitate its analysis) and information obtained from sensors and cameras while integrating prior information. This approach was tested on manually entered data in urban environments. The results show that the classification of elements is more accurate with this method than the image segmentation alone.",
        "metrics": {
          "SARI": 40.153654575167494,
          "BLEU": 0.12680303612584007,
          "FKGL": 13.7
        }
      },
      {
        "source": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to the development of self-driving systems is the utilization of ROS which is an open-source middleware framework used in the development of robot applications. On the other hand, the popular approach in the automotive industry is the utilization of MATLAB/Simulink which is software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables to create functionalities of ROS-based robots in MATLAB/Simulink. However, it is not been fully utilized in the development of self-driving systems yet because there are not enough samples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.",
        "prediction": "This paper introduces a MATLAB/Simulink benchmark suite for an open-source self-driving system using the Robot Operating System (ROS). ROS is a tool for creating robot applications. In contrast, MATLAB/Simulink is common in the automotive industry for modeling and analyzing systems. It offers a connection between ROS and MATLAB/Simulink, allowing developers to work on ROS-based robots in MATLAB/Simulink. However, this approach hasn't been widely used for self-driving systems yet due to a lack of examples and challenges in co-development. To address this, we present a MATLAB/Simulink benchmark suite for the ROS-based self-driving system called Autoware, which is well-known open-source software with many self-driving modules. The benchmark includes MATLAB/Simulink samples from Autoware that assist in designing ROS-based self-driving systems using MATLAB/Simulink.",
        "reference": "This paper proposes MATLAB/Simulink tests for an open-source self-driving system based on Robot Operating System. One approach to the development of self-driving systems is the utilization of Robot Operating System (open-source software framework used in the development of robot applications). But the popular approach in the automotive industry is the utilization of MATLAB/Simulink (software for modeling, simulating, and analyzing). MATLAB/Simulink provides an interface that enables to create functionalities of robots (based on robot operating system). However, it is not fully utilized in the development of self-driving systems yet because there are not enough samples, and it is difficult for developers to adopt co-development. We provide MATLAB/Simulink tests for a self-driving system (based on robot operating system) called Autoware. Autoware is a popular open-source software that provides a complete set of self-driving modules. The provided tests contain MATLAB/Simulink samples available in Autoware. They help to design self-driving systems (based on robot operating system) using MATLAB/Simulink.",
        "metrics": {
          "SARI": 23.211565907389875,
          "BLEU": 0.14731291362551863,
          "FKGL": 12.3
        }
      },
      {
        "source": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. One approach to the development of self-driving systems is the use of ROS; however, the system used in the automotive industry is typically designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with ROS-based systems. To allow the two to be used in tandem, it is necessary to rewrite the C++ code and incorporate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, thereby improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "prediction": "Managing self-driving systems is getting more complicated as technology advances. One way to develop these systems is by using ROS, but the automotive industry usually relies on MATLAB/Simulink for modeling and testing. Unfortunately, these models do not work with ROS systems. To make them compatible, C++ code needs to be rewritten, which slows down development. The proposed framework allows models made in MATLAB/Simulink to be used within a ROS-based self-driving system, making development more efficient. Our tests showed that this framework has real potential.",
        "reference": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. Self-driving systems use Robot Operating System (open-source software framework used in the development of robot applications). However, in the automotive industry, the system is designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with systems based on Robot Operating System. To use both, it is necessary to incorporate new code into the system based on Robot Operating System, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a self-driving system, which improve development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "metrics": {
          "SARI": 23.51484889903149,
          "BLEU": 0.09963236305052907,
          "FKGL": 11.1
        }
      },
      {
        "source": "Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common, but costly and risky method, which has already caused fatalities. As a safer alternative, virtual tests, in which self-driving car software is tested in computer simulations, have been proposed. One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for systematically testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car software, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "prediction": "Making sure self-driving cars are safe is very important, but there is no agreed standard for testing them. Testing these cars on real roads is common but can be expensive and dangerous, leading to accidents. A safer option is to use computer simulations to test the software of self-driving cars. It's impossible to create enough tests manually for all the situations these cars might face. To solve this problem, we created a tool called AsFault that automatically generates virtual tests for self-driving car software. We showcase AsFault by testing the lane keeping feature of a self-driving car's software, where AsFault creates scenarios that make it veer off the road. A video showing AsFault in action can be found at: https://youtu.be/lJ1sa42VLDw",
        "reference": "Ensuring the safety of self-driving cars is important, but there is no standard way to test them. Testing self-driving cars in regular traffic is costly and risky, and has already caused fatalities. As a safer alternative, virtual tests (self-driving car software tested in computer simulations) have been proposed. To cover the huge number of possible driving situations, self-driving cars must be tested in manually created tests. We developed AsFault, a tool for automatically generating virtual tests for testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of a self-driving car software with scenarios going off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "metrics": {
          "SARI": 39.08306312934703,
          "BLEU": 0.30205093910737396,
          "FKGL": 10
        }
      },
      {
        "source": "With the developing of the technology on self-driving, more and more L3 driverless vehicles are launched in market, people get opportunities to experience the self-driving cars in their daily life. As a result, there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can not only improve the driving experience, but also accelerate the process of self-driving commercialization. This paper discusses eye-movement interaction, voice interaction and gesture interaction in self-driving car, and analyzes the technology, advantages and disadvantages of the existing interaction modes, and prospect the future development trend of self-driving human-computer interaction.",
        "prediction": "As self-driving technology advances, more L3 driverless cars are available for people to use in their daily lives. This increase has led to a greater interest in the autopilot experience. Effective interaction between humans and computers can enhance the driving experience and speed up the commercialization of self-driving cars. This paper explores three types of interactions: eye movements, voice commands, and gestures. It looks at the technology behind these interactions, their pros and cons, and considers future trends in how humans will interact with self-driving cars.",
        "reference": "Self-driving technology is developing, more and more driverless vehicles are launched in market and accessible for people in their daily life. So there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can improve the driving experience and accelerate the process of self-driving commercialization. This paper discusses interactions with the eye, voice and gesture in self-driving car, analyzes the technology, the interaction modes, and considers the future of self-driving human-computer interaction.",
        "metrics": {
          "SARI": 32.11381425057093,
          "BLEU": 0.1099165723242794,
          "FKGL": 11.2
        }
      },
      {
        "source": "In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on highfidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The state of the art in driving scenario generation, as adopted by some of the front-runners of the self-driving car industry, still relies on human input [1]. In this paper we propose to automate the process using Bayesian optimization to generate adversarial self-driving scenarios that expose poorly-engineered or poorly-trained self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy using vision-based imitation learning we obtain safer self-driving behavior.",
        "prediction": "Self-driving vehicles are becoming more common on roads and are expected to make transportation safer and more efficient. To improve the reliability of these vehicles, extensive software testing is needed, ideally using advanced simulators where self-driving cars interact with other vehicles and pedestrians. It is important to test self-driving software in many difficult driving situations. Currently, some leading companies still depend on humans to create these driving scenarios. This paper suggests automating this process using a technique called Bayesian optimization to generate challenging scenarios that can reveal weaknesses in self-driving systems, increasing the chance of accidents with simulated pedestrians and vehicles. We demonstrate that by adding these generated scenarios to the training data and refining the self-driving policy through vision-based imitation learning, we can achieve safer self-driving actions.",
        "reference": "Self-driving vehicles have become common on public roads, with the promise of a safe and efficient mode of transport. To be reliable, these vehicles need many software tests on simulators with interactions of multiple vehicles and pedestrians. It is essential that self-driving software is evaluated with many different challenging driving scenarios. The driving scenario generation still needs human contribution. We propose to automate the process and generate conflicting self-driving that expose weaknesses of self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. By incorporating the generated scenarios into the parameters of the self-driving policy, and by adjusting the policy with the imitation learning, we obtain safer self-driving behavior.",
        "metrics": {
          "SARI": 38.22183793366272,
          "BLEU": 0.14341666104150252,
          "FKGL": 15.1
        }
      },
      {
        "source": "At the brink of the introduction of self-driving vehicles, only little is known about how potential users perceive them. This is especially true for self-driving vehicles deployed in public transport services. In this study, the relative preferences for a trip with a self-driving bus is assessed compared to a trip with a regular bus, based on a stated preference experiment. Based on the responses of 282 respondents from the Netherlands and Germany, a discrete choice model is estimated as a Mixed Logit model including attitudes towards trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. This is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. Findings from this study further suggest that the popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences towards automated public transport services are expected to evolve along with the transition from demonstration pilots to their deployment in regular operations.",
        "prediction": "As self-driving vehicles are being introduced, we still know little about how users feel about them, especially in public transport. This study looks at how people prefer trips with self-driving buses compared to regular buses by using a preference experiment. Responses from 282 people in the Netherlands and Germany were analyzed using a Mixed Logit model, which included feelings about trust in self-driving vehicles and interest in technology. The results show that passengers prefer self-driving buses for short trips, because the value of time spent traveling is higher for these buses compared to regular ones. However, the appeal of self-driving buses goes down if there is a human steward on board or if they operate with set routes. Those who are very interested in technology or trust automated vehicles tend to have a more positive view of self-driving buses. As self-driving public transport becomes more common, public preferences are likely to change.",
        "reference": "Little is known about how potential users perceive self-driving vehicles. This is especially true for self-driving vehicles in public transport services. In this study, the preferences between a trip in a self-driving bus and a regular bus were compared. The results of 282 participants were studied, including trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. The preference is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. The popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences for automated public transport services are expected to evolve with their deployment in regular operations.",
        "metrics": {
          "SARI": 26.53739836635392,
          "BLEU": 0.15622424017036948,
          "FKGL": 12.9
        }
      },
      {
        "source": "In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. The fundaments of human dignity, autonomous decision-making, and democracies are shaking, but I believe that they need to be vigorously defended, as they are not only core principles of livable societies, but also the basis of greater efficiency and success.",
        "prediction": "The ongoing digital revolution is changing our economy and many social systems. This change can bring great benefits, but it also poses significant risks. After we automate production and create self-driving cars, the next step is the automation of society. We are reaching a crucial point where we must choose between two types of societies: one where actions are controlled from the top down and enforced through coercion or manipulative technologies like targeted ads and nudging, or one where decisions are made freely and cooperatively. Modern information and communication technology (ICT) can support both models, but the second option offers economic and strategic advantages. The foundations of human dignity, individual decision-making, and democracy are under pressure, and it is important to defend them because they are essential for a healthy society and lead to greater efficiency and success.",
        "reference": "The on-going digital revolution will dramatically transform our economy and societal institutions. But this also presents risks for our society. Automation of society is coming. We must choose either between a society in which the actions are determined top-down by coercion or manipulative technologies (such as personalized ads) or in which decisions are taken freely and cooperatively. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. Foundations of human dignity, autonomous decision-making, and democracies are weakening. These core principles of society and the basis of greater efficiency and success need to be defended.",
        "metrics": {
          "SARI": 40.80243395735914,
          "BLEU": 0.1564122612403394,
          "FKGL": 14.6
        }
      },
      {
        "source": "This paper addresses the financial markets problem space with particular emphasis on trading systems seen in banking today. With the advent of modern computing, cross-regional trading transactions can now be executed within milliseconds, a reality that is impossible without the advancements of software systems technology. We then move on and discuss the anatomy of a trading system and how it fits in with the banks ecosystem of vital inter-working components.",
        "prediction": "This paper looks at financial markets, especially trading systems used in banks today. Thanks to modern computing, trading transactions across different regions can happen in milliseconds, which wouldn't be possible without new software technology. We also talk about how a trading system works and its role within the important components of a bank's ecosystem.",
        "reference": "This paper addresses problems of trading systems in banking. Modern computing allows cross-regional trading transactions within milliseconds. We then discuss how a trading system fits in with the bank ecosystem.",
        "metrics": {
          "SARI": 41.85262658992202,
          "BLEU": 0.07405581026799743,
          "FKGL": 11.5
        }
      },
      {
        "source": "In this paper we present queueing-theoretical methods for the modeling, analysis, and control of autonomous mobility-on-demand MOD systems wherein robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first cast an autonomous MOD system within a closed Jackson network model with passenger loss. The theoretical insights are used to design a robust, real-time rebalancing algorithm, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study of New York shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles roughly 70% of the size of the current taxi fleet operating in Manhattan. Finally, we extend our queueing-theoretical setup to include congestion effects, and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to the problem of system-wide coordination of autonomously driving vehicles, and provides one of the first characterizations of the sustainability benefits of robotic transportation networks.",
        "prediction": "This paper discusses methods using queueing theory to model, analyze, and manage on-demand transportation systems where self-driving vehicles take customers around a city and adjust their locations to maintain good service. First, we describe an autonomous transportation system using a closed Jackson network model that accounts for passenger loss. We then create a strong, real-time rebalancing algorithm and apply it to a case study in New York City, using an eight-vehicle robot test system. The study shows that around 8,000 robotic vehicles could meet the taxi demand in Manhattan, which is about 70% of the current number of taxis. Lastly, we enhance our model to consider traffic congestion and examine how rebalancing vehicles affect overall traffic. Overall, this paper offers a solid approach to coordinating self-driving vehicles and discusses the sustainability advantages of robotic transportation systems.",
        "reference": "Queueing models are used for autonomous mobility-on-demand MOD systems. A queueing model is constructed so that queue lengths and waiting time can be predicted. In MOD systems,  robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure quality of service. We first cast an autonomous MOD system within a closed Jackson network model,a class of queueing network, with passenger loss. A real-time rebalancing algorithm is applied to a case study of New York City. It is implemented on an eight-vehicle mobile robot testbed. The current taxi demand in Manhattan can be met with about 8,000 robotic vehicles. It represents roughly 70% of the size of the current taxi fleet in Manhattan. Congestion effects are also included to study the impact of autonomously rebalancing vehicles on overall traffic jams. This paper describes the sustainability benefits of robotic transportation networks.",
        "metrics": {
          "SARI": 33.01801144848716,
          "BLEU": 0.13724161169529758,
          "FKGL": 14.5
        }
      },
      {
        "source": "Many model based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. However sensor based robot control overcomes many of the difficulties of uncertain models and unknown environments which limit the domain of application of the previous methods. Furthermore, for service and manipulation tasks, it is more suitable to study the interaction between the robot and its environment at the contact point using the sensor based control, rather than specifying the joint positions and velocities required to achieve them.",
        "prediction": "Many techniques for using humanoid robots in domestic tasks have been suggested, like teleoperation and learning by imitation. However, controlling robots with sensors is better for dealing with uncertain models and unknown environments, which limits the use of the earlier methods. For tasks involving service and manipulation, it's more effective to focus on the interaction between the robot and its environment at the point of contact, rather than just the robot's joint positions and speeds.",
        "reference": "Model based techniques exist to apply domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. Sensor based robot control overcomes difficulties of uncertain models and unknown environments. Interaction between the robot and its environment using the sensor based control is important.",
        "metrics": {
          "SARI": 36.551938307933376,
          "BLEU": 0.1292813680972524,
          "FKGL": 15.4
        }
      },
      {
        "source": "Inaccurate information, in the field of library and information science, is often regarded as a problem that needs to be corrected or simply understood as either misinformation or disinformation without further consideration. Misinformation and disinformation, however, may cause significant problems for users in online environments, where they are constantly exposed to an abundance of inaccurate and/or misleading information. This paper aims to establish conceptual groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of cues to deception, as means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related yet distinct sub-categories of information. Misinformation is a multifaceted concept, more complex than simply being inaccurate or incomplete, and disinformation does not always entail misinformation.",
        "prediction": "In library and information science, wrong information is often seen as a problem to fix or recognize as misinformation or disinformation. However, misinformation and disinformation can create major issues for users online who face a lot of inaccurate and misleading information. This paper aims to lay the foundation for future research by looking at how information, misinformation, and disinformation are related. We also discuss ways to identify deception to help detect misinformation and disinformation. We believe that misinformation and disinformation are related but different types of information. Misinformation is more complicated than just being wrong or incomplete, and disinformation doesn’t always include misinformation.",
        "reference": "Inaccurate information is often regarded as a problem that needs to be corrected or simply understood as misinformation or disinformation without further consideration. Misinformation and disinformation may cause problems online because users are constantly exposed to inaccurate and/or false information. This paper aims to establish preliminary work for future research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related but distinct sub-categories of information. Misinformation is more complex than simply being inaccurate or incomplete, and disinformation does not always involve misinformation.",
        "metrics": {
          "SARI": 35.69283458825059,
          "BLEU": 0.32013895775614315,
          "FKGL": 15.9
        }
      },
      {
        "source": "The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.",
        "prediction": "Social media makes it easy for people to communicate, but it also spreads a lot of misinformation. This misinformation can harm the public. To tackle this, we created a system that combines human knowledge and technology to identify false information. The main idea is to (1) automatically assess the expertise of users based on their posts and (2) connect those experts with suspicious information. By sharing this suspicious information with the right experts, we can gather their opinions to evaluate its trustworthiness and fight misinformation. In this paper, we emphasize finding experts for spotting misinformation. We introduced a method that uses tags to assess the expertise of microblog users. Tests on real data show our method works well for finding experts who can help identify misinformation in posts.",
        "reference": "The growth of social media leads to better communication between people, but also propagation of misinformation. The wide spread of misinformation over social media have bad consequences for public interest. We design a framework to help identify misinformation. The idea is to index the expertise of users and to match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, they can judge the credibility of information, and help refute misinformation. In this paper, we focus on finding appropriate experts for misinformation identification. We propose a method to index the expertise of users with tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding for misinformation identification.",
        "metrics": {
          "SARI": 29.139089533574047,
          "BLEU": 0.127338823133707,
          "FKGL": 10.7
        }
      },
      {
        "source": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation in support of such research. This paper aims to address the aforementioned challenges by conceptualizing misinformation and enabling the interoperability of misinformation. In particular, a representation and a model of misinformation are proposed through surveying, synthesizing, and explicating existing work in the field. The ontology-supported misinformation model can not only guide future misinformation research but also lay the foundation for building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a formal methodology for managing misinformation in a digital library, and suggest future research directions related to the misinformation model.",
        "prediction": "Research on misinformation is increasingly recognized as important. However, this field faces two main challenges: a lack of theoretical models and not enough examples of misinformation for study. This paper aims to overcome these challenges by defining misinformation and promoting ways to connect different aspects of it. We propose a framework and a model for misinformation by reviewing and explaining existing research. This new model can help guide future studies on misinformation and serve as a basis for creating a digital library of misinformation, improving how we share and manage this information. Additionally, we outline a method for managing misinformation in a digital library and suggest areas for future research on the model.",
        "reference": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the rare misinformation data. This paper aims to address these challenges by conceptualizing misinformation and being able to completely understand and work on misinformation. A representation and a model of misinformation are proposed through existing work in the field. The model can guide future misinformation research and help building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a methodology for managing misinformation in a digital library, and suggest future research directions.",
        "metrics": {
          "SARI": 29.112581641952634,
          "BLEU": 0.18060892281159271,
          "FKGL": 13
        }
      },
      {
        "source": "Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to the event. We define as misinformation any false or inaccurate information that is spread either intentionally or unintentionally. In this paper we study the problem of misinformation identification in OSNs, and we focus in particular on the Twitter social network. Based on user and tweets characteristics, we build a misinformation detection model that identifies suspicious behavioral patterns and exploits supervised learning techniques to detect misinformation. Our extensive experimental results on 80294 unique tweets and 59660 users illustrate that our approach effectively identifies misinformation during emergencies. Furthermore, our model manages to timely identify misinformation, a feature that can be used to limit the spread of the misinformation.",
        "prediction": "Online Social Networks (OSNs) are popular for sharing information among users. News about emergencies often spreads on these networks, but so does misinformation, which is any false or inaccurate information shared intentionally or unintentionally. In this paper, we examine how to identify misinformation on OSNs, focusing on Twitter. We create a model that detects suspicious behavior based on user and tweet characteristics, using supervised learning techniques. Our tests on 80294 unique tweets and 59660 users show that our method effectively identifies misinformation during emergencies. Additionally, our model can quickly identify misinformation, helping to reduce its spread.",
        "reference": "Social media have become popular means of information sharing. The spread of news regarding emergency events is common in social media but so is the spread of misinformation. Misinformation is any false or inaccurate information that is spread intentionally or not. In this paper we study the problem of misinformation identification in social media, and we focus in particular on Twitter. We build a misinformation detection model that identifies suspicious behavioral patterns and detect misinformation. We found 80294 unique tweets and 59660 users. It illustrates that our approach identifies misinformation during emergencies. Our model manages to timely identify misinformation, it can be used to limit the spread of the misinformation.",
        "metrics": {
          "SARI": 29.953312341066162,
          "BLEU": 0.18847856423255907,
          "FKGL": 13.1
        }
      },
      {
        "source": "The advent of WWW changed the way we can produce and access information. Recent studies showed that users tend to select information that is consistent with their system of beliefs, forming polarized groups of like-minded people around shared narratives where dissenting information is ignored. In this environment, users cooperate to frame and reinforce their shared narrative making any attempt at debunking inefficient. Such a configuration occurs even in the consumption of news online, and considering that 63% of users access news directly form social media, one hypothesis is that more polarization allows for further spreading of misinformation. Along this path, we focus on the polarization of users around news outlets on Facebook in different European countries (Italy, France, Spain and Germany). First, we compare the page posting behavior and the users interacting patterns across countries and observe different posting, liking and commenting rates. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities generated around specific pages.",
        "prediction": "The rise of the World Wide Web has changed how we create and find information. Recent studies show that people tend to choose information that matches their beliefs, leading to groups of like-minded individuals who ignore opposing views. In these groups, users work together to strengthen their shared stories, making it hard to challenge those views. This situation is evident in how people consume news online, especially since 63% of users get news from social media. One idea is that increased polarization may contribute to the spread of misinformation. We focus on how users in Italy, France, Spain, and Germany are polarized around news pages on Facebook. First, we look at how often pages post and how users engage with these posts, finding differences in posting, liking, and commenting habits across countries. Second, we investigate how users tend to engage with certain pages, leading to the formation of polarized communities around specific pages.",
        "reference": "The WWW changed the ways we can produce and access information. Users tend to select information that agrees with their beliefs. They form polarized groups of like-minded people sharing narratives that ignore contradicting information. Here, users reinforce their shared narratives and can resist debunking This is the case with online news. Because 63% of users access news from social media, polarization could increase misinfomation spread. We focus on polarisation of Facebook news consumers in different European countries. Posting, liking and commenting rates differ across countries. We look at how users interact with different pages and polarized communities form around specific pages.",
        "metrics": {
          "SARI": 44.10947178183679,
          "BLEU": 0.10015442573453841,
          "FKGL": 9.6
        }
      },
      {
        "source": "We find that Italy is the most polarized country, followed by France, Germany and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the usersu0027 engagement and trust on the news. Our findings suggest that trust in information broadcaster plays a pivotal role against polarization of users online.",
        "prediction": "We found that Italy is the most divided country, followed by France, Germany, and then Spain. We also modified a model to show how these communities form based on users' engagement and trust in the news. Our results indicate that trust in the information source is crucial for reducing online polarization among users.",
        "reference": "Italy is the most polarized country, then France, Germany and Spain. A variant of the Bounded Confidence Model simulates the emergence of these communities by considering user engagement and trust in the news. Trust in an information source plays a major role against polarization of users online.",
        "metrics": {
          "SARI": 37.74114499811923,
          "BLEU": 0.20475739007221863,
          "FKGL": 9
        }
      },
      {
        "source": "Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative strategies to contrast misinformation. Simultaneously, they allow the spread of misinformation by empowering individuals to self-select the narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. A precise theoretical understanding of such trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs unbiasedly based on the arrival of new information, while a fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. We show that this simple confirmation bias mechanism can generate permanent opinion polarisation. Furthermore, the model results in states where unbiased agents behave \"as if\" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. We derive analytic results for the distribution of individual agents’ beliefs, explicitly demonstrating the aforementioned trade-off between confirmation bias and social connectivity, which we further validate against US county-level data on the impact of Internet access on the formation of beliefs about global warming. Our findings indicate that confirmation bias in small doses may actually result in improved accuracy across individuals by preserving information diversity in a social network. However, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups.",
        "prediction": "Social media gives users a chance to see many different opinions but also spreads misinformation. We examine the effects of our model on policy, pointing out the downsides of strategies that aim to debunk false information while suggesting better ways to counter it. Many people choose the news they want to see, either by actively seeking confirming views or through personalized algorithms that reinforce those views. This results in missing a clear understanding of the balance between these issues. We present a basic social learning model where most people adjust their beliefs based on new information, but some stick to their existing beliefs due to confirmation bias. This bias can lead to divided opinions that last a long time. Moreover, unbiased individuals can start acting like they are biased because their biased peers limit the information they can receive. We show the relationship between confirmation bias and social connections using data from US counties on how Internet access affects beliefs about global warming. Our results suggest that a little confirmation bias may help improve belief accuracy by keeping information diverse in a group. However, too much confirmation bias can reduce accuracy because biased individuals block information from others.",
        "reference": "Social media give users opportunities to interact with different opinions. We discuss the meaning of our model for policy and the downsides of myth-busting We suggest alternatives to fight misinformation. But misinformation is spread via social media because individuals can search for information that confirms their beliefs and personalized news algorithms may supply it. Theoretical understanding of the balance between opportunities and misinformation is incomplete. We introduce a stylized social learning model. In this model, most participants in a network update their beliefs based on new information, while others show confirmation bias and can reject news that differs from their beliefs. This simple confirmation bias mechanism can create lasting opinion polarisation. Besides, in the model, unbiased people behave “as if” biased because biased neighbours limit their access to diverse information. With our model, we show the trade-off between confirmation bias and social connectivity. We validated the model using the data on how internet access influences beliefs on global warming. Small confirmation bias may even improve accuracy across individuals by keeping information diversity in a social network. But large confirmation bias lowers accuracy as biased individuals limit information flow to others.",
        "metrics": {
          "SARI": 44.39488276356517,
          "BLEU": 0.13812436431165545,
          "FKGL": 12.2
        }
      },
      {
        "source": "Superintelligence is a potential type of future artificial intelligence (AI) that is significantly more intelligent than humans in all major respects. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. Meanwhile, the prospect of superintelligence is the subject of major ongoing debate, which includes a significant amount of misinformation. Superintelligence misinformation is potentially dangerous, ultimately leading bad decisions by the would-be developers of superintelligence and those who influence them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: strategies to prevent the spread of superintelligence misinformation and strategies to correct it after it has spread. In general, misinformation can be difficult to correct, suggesting a high value of strategies to prevent it. The strategies proposed can be applied to lay public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "prediction": "Superintelligence is a type of artificial intelligence that could be much smarter than humans in almost every way. If created, it could greatly change the world, with either very good or very bad outcomes. There is a lot of debate about superintelligence, and some of it includes false information. This misinformation can be harmful and lead to poor decisions by those trying to develop superintelligence. This paper looks at ways to fight against misinformation about superintelligence. It focuses on two main approaches: stopping misinformation from spreading in the first place and fixing it after it has already spread. Generally, it is hard to correct misinformation, so preventing it is very important. The strategies discussed can be used to inform the public about superintelligence, improve AI education programs, and help experts agree on the topic.",
        "reference": "Superintelligence is a potential future artificial intelligence (AI) significantly more intelligent than humans. Superintelligence could be a major event, with potential consequences that are beneficial or catastrophic. The prospect of superintelligence is the subject of major debate, which includes a lot of misinformation. Superintelligence misinformation could be dangerous, and lead to bad decisions by the people that could work on the project. This paper searches for strategies to counter superintelligence misinformation. Two strategies are examined: to prevent the spread of misinformation and to correct it after it has spread. Misinformation can be difficult to correct, so preventing it seems to be the best strategy. The strategies proposed can be used to bring public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "metrics": {
          "SARI": 33.544024998450425,
          "BLEU": 0.14257088313419192,
          "FKGL": 11
        }
      },
      {
        "source": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\\\\Omega(2^{\\\\log^{1-\\\\epsilon}n^4})$ in polynomial time unless $NP \\\\subseteq DTIME(n^{\\\\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "prediction": "Online misinformation can lead to public panic and economic damage. The misinformation containment problem seeks to limit its spread in social networks by launching counter-campaigns. This paper presents the first analysis of this problem when multiple information spread events, or cascades, can occur. We propose a formal model for how these cascades happen and introduce a key idea called cascade priority. We also establish that the misinformation containment problem is very difficult to approximate accurately in polynomial time unless certain complex conditions are met. Additionally, we describe various types of cascade priority that often appear in real social networks. Lastly, we create new algorithms to tackle the misinformation containment problem, and our experiments show that these algorithms are effective.",
        "reference": "The online misinformation could cause public panic and serious economic damages. The goal of misinformation containment is to limit the spread of misinformation in online social networks by launching campaigns to counter misinformation. We present the first analysis of the misinformation containment problem with a random number of cascades (people observe the actions of others and then make the same choice) allowed. First, we provide a diffusion model and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Ω(2log1−ϵn4) in polynomial time unless $NP \\subseteq DTIME(n^{\\polylog{n}})$ Third, we introduce several types of cascade priority that are frequently seen in social networks. Finally, we design algorithms for solving the misinformation containment problem. The proposed algorithm is effective and shows encouraging results.",
        "metrics": {
          "SARI": 29.708241842382105,
          "BLEU": 0.150907640394713,
          "FKGL": 13.5
        }
      },
      {
        "source": "Yet contemporary work in Philosophy argues provisional belief in conspiracy theories is—at the very—least understandable (because conspiracies occur) and if we take an evidential approach—judging individual conspiracy theories on their particular merits—belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine the kinds of evidence typically associated with conspiracy theories, showing that the evidential problems typically associated with conspiracy theories are not unique to such theories. As such, if there is a problem with the conspiracy theorist’s use of evidence, it is one of principle: is the principle which guides their use of evidence somehow in error? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for a scepticism of conspiracy theories based purely on their use of evidence.",
        "prediction": "Current philosophy suggests that it's at least reasonable to have some belief in conspiracy theories because conspiracies do happen. When we evaluate conspiracy theories based on evidence, we find that believing in them is justified in many situations. I explore the common types of evidence linked to conspiracy theories and highlight that the issues with evidence aren't exclusive to these theories. Therefore, if there is a flaw in how conspiracy theorists use evidence, it relates to the principles guiding their evidence evaluation. I argue that regardless of our views on conspiracy theories, we shouldn't automatically distrust them just based on how they use evidence.",
        "reference": "To believe in conspiracy theories is understandable as conspiracies occur and if we take an evidential approach, belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine evidence associated with conspiracy theories, showing that the evidential problems are not unique to conspiracy theories. The problem with the use of evidence of conspiracy theorist could be the principle which guides their use of it. I argue that there is no ground to be suspicious of conspiracy theories as a whole, because those are based on evidence.",
        "metrics": {
          "SARI": 32.03257050784909,
          "BLEU": 0.07873966789724425,
          "FKGL": 13.8
        }
      },
      {
        "source": "Conspiracy theories have gained much academic and media attention recently, due to their large impact on public events. However, little is known about how conspiracy theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis--the Zika virus outbreak. Using a mixed-methods approach including content analysis and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the Zika crisis, the conditions under which Zika conspiracy theories emerge, and the particular discursive strategies through which Zika conspiracy theories developed in online forums. Our analysis shows that conspiracy talk emerged as people attempted to make sense of a public health crisis, reflecting their emergent information needs and their pervasive distrust in formal sources of Zika information. Practical implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "prediction": "Conspiracy theories have become a popular topic for study because they can significantly affect public events. However, we know little about how these theories form and spread on social media. This study looks at conspiracy theories on Reddit during the Zika virus outbreak. We used different methods, including analyzing content and discussions, to identify the types of conspiracy theories related to Zika and the situations that led to their rise. We also explored the ways these theories developed in online discussions. Our findings suggest that people shared conspiracy theories as they tried to understand the public health crisis, showing their need for information and their distrust of official Zika information sources. We discuss the implications of our findings for researchers, health professionals, and policymakers.",
        "reference": "Conspiracy theories have gained much attention recently, due to their large impact on public events. Little is known about how conspiracy theories are produced and developed on social media. We present a study of conspiracy theory creation on Reddit during a public health crisis. Using content and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the crisis, the conditions of their creation, and the strategies of their development in online forums. Our analysis shows that conspiracy talk come from people trying to make sense of a public health crisis, which reflect their information needs and their lack of confidence in formal sources of information. Implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "metrics": {
          "SARI": 25.975572047628404,
          "BLEU": 0.11873478873768235,
          "FKGL": 12.6
        }
      },
      {
        "source": "Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories evolve, multiply, and interconnect, further complicating efforts to understand them and to limit their propagation. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif such as \\\"governmental agency-controls-communications\\\" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.",
        "prediction": "Conspiracy theories are everywhere in online chats, often used to explain recent events without official reports or to express political dissent. These theories change, grow, and connect in complex ways, making them hard to study and control. It's important to create effective methods to analyze how people discuss conspiracy theories online. What topics do they cover? What common themes can we find? What do these themes reveal about their thinking? This study looks at over ten years of discussions in the Reddit community r/conspiracy, which focuses on conspiracy theories. We examine crucial parts of a conspiracy theory, such as the people involved, their actions, and their targets. For instance, a common pattern might express that \"government agencies control communications,\" showing how different conspiracy claims suggest that the government manipulates information. These patterns reveal similarities among various conspiracy theories, even when they relate to different situations. By doing this, we can better understand how people discuss conspiracy theories and interpret their conversations. Our method allows for a large-scale analysis of conspiracy theories in alternative news and social media, helping us understand why they spread and how to counter them.",
        "reference": "Conspiracy theories are omnipresent in online discussions. Conspiracy theories evolve, multiply, and interconnect, complicating how to understand them and to limit their propagation. It is crucial to develop methods to examine the nature of these conspiratorial discussions. What do users talk about? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in an online community on the social media Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif (an idea) such as \"governmental agency-controls-communications\" represents the various ways in which multiple conspiratorial statements suggest how governmental agencies control information. Narrative-motifs expose similarities between multiple conspiracy theories even when they refer to different events or circumstances. These representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media and ways to understand them and limit their propagation.",
        "metrics": {
          "SARI": 25.209398724362682,
          "BLEU": 0.16935650025035304,
          "FKGL": 11.8
        }
      },
      {
        "source": "Blockchain technology is the underlying enabling technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force. Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make that transaction simpler and easier between entities. We investigate factors that influence an individualu0027s intention to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption grounded in the theory of planned behavior (TPB) to: identify the determinants for the acceptance of cryptocurrency and explore the relative importance of each construct. We offer empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context.",
        "prediction": "Blockchain technology is the foundation for Bitcoin, which is the most popular cryptocurrency. This technology is gaining popularity and has the potential to disrupt many industries. Both individuals and organizations can benefit from blockchain because it allows for secure and easier exchanges of data. We study what influences a person's decision to use a blockchain cryptocurrency. We create a model based on the theory of planned behavior to find out what factors affect the acceptance of cryptocurrency and how important each factor is. Our findings help better understand why people adopt cryptocurrency, especially in the context of e-government.",
        "reference": "Blockchain technology is a technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become popular with the potential to become a powerful disruptive force. People and organizations may use it to increase secure data exchange and make transactions simpler and easier. We investigate what influence people to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption to identify how cryptocurrency is accepted. Our evidence allow a better understanding of cryptocurrency adoption.",
        "metrics": {
          "SARI": 36.42559977006851,
          "BLEU": 0.1273510650659468,
          "FKGL": 12
        }
      },
      {
        "source": "With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. We find that cryptojacking is common, with currently 1 out of 500 sites hosting a mining script. Moreover, we perform several secondary analyses to gain insight into the cryptojacking landscape, including a measurement of code characteristics, an estimate of expected mining revenue, and an evaluation of current blacklist-based countermeasures.",
        "prediction": "With the rise of memory-bound cryptocurrencies like Monero, using JavaScript for mining in web browsers has become a viable option compared to traditional mining hardware. This has led to a new practice known as cryptojacking, where a website uses its visitors' computing power to mine cryptocurrencies without their knowledge. In this study, we investigate this trend by introducing a three-phase analysis method to identify mining scripts and examine how widespread cryptojacking is among the top one million websites. Our findings reveal that cryptojacking is relatively common, with about 1 in 500 sites running a mining script. Additionally, we carry out further analyses to understand the cryptojacking scene better, including looking at the code used, estimating potential mining profits, and assessing current methods to block such activities.",
        "reference": "Memory-bound cryptocurrencies like Monero can be mined  from a browser with JavaScript, better than with a dedicated rig. However, this technology enables cryptojacking, whereby a site covertly mines for cryptocurrencies on its visitors’ computers. We propose a new approach to identify mining scripts. The approach was evaluated on Alexa 1 websites. Cryptojacking is common, as 1 in 500 sites host a mining script. We also measured code characteristics, estimated mining revenue, and evaluated blacklist-based countermeasures.",
        "metrics": {
          "SARI": 47.96569170418062,
          "BLEU": 0.08064911522445366,
          "FKGL": 15.5
        }
      },
      {
        "source": "The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.  contracts :[39],\"are commonly written in a high-level language such as Ethereumu0027s Solidity, and translated to compact low-level bytecode for deployment on the blockchain. Once deployed, the bytecode is autonomously executed, usually by a %Turing-complete virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts :[39],\"are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal consists of an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations. Users of the framework can express security analyses in a declarative fashion: a security analysis is expressed in a logic specification written in the \\souffle language.",
        "prediction": "Modern blockchains have led to the creation of smart contracts, which are self-running programs on the blockchain. These contracts are becoming very popular and are expected to be used in areas like law, business, and governance. Smart contracts are often written in languages like Solidity and then converted into a simplified code for the blockchain. Once on the blockchain, this code runs automatically, usually on a Turing-complete virtual machine. However, like any programs, smart contracts can be vulnerable to attacks because of poor programming and tools. They are also attractive targets since they can hold significant amounts of cryptocurrency. Therefore, developers need security tools that can analyze this low-level code to find weaknesses. This paper introduces Vandal, a security analysis tool for Ethereum smart contracts. Vandal has a process that converts low-level Ethereum Virtual Machine bytecode into logical relations. Users can define security checks through a logic specification language called Souffle.",
        "reference": "Smart contracts have appeared thanks to blockchain technology. These autonomous programs are used in law, business, commerce, and governance. Smart contracts are written in a high-level language such as Ethereum’s Solidity and translated to bytecode. that live and run on the blockchain are predicted in Once deployed on the blockchain, the bytecode is autonomous. Smart contracts are vulnerable to malicious attacks due to poor programming, languages and toolchains Smart contracts can command large amounts of cryptocurrency. Developers and auditors need frameworks to detect security vulnerabilities. Vandal is a security analysis framework for Ethereum smart contracts. Vandal converts Ethereum Virtual Machine bytecode to semantic logic relations. Users can express security analyses in the Soufflé language.",
        "metrics": {
          "SARI": 42.67319195931849,
          "BLEU": 0.17826267574961052,
          "FKGL": 10.4
        }
      },
      {
        "source": "Vandal is both fast and robust, successfully analysing over 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.",
        "prediction": "Vandal is both fast and strong, analyzing over 95% of all 141,000 unique contracts in an average time of 4.15 seconds. It performs better than current leading tools like Oyente, EthIR, Mythril, and Rattle under similar conditions.",
        "reference": "Vandal is both efficient (95% accuracy), fast (141k in 4.15 s) and robust. It outperforms the state of the art tools like Oyente, EthIR, Mythril, and Rattle",
        "metrics": {
          "SARI": 56.10950913965562,
          "BLEU": 0.2757492657184922,
          "FKGL": 9.3
        }
      },
      {
        "source": "As Bitcoinu0027s popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds.",
        "prediction": "As Bitcoin has become more popular over the last ten years, it has attracted more attention from various types of attackers",
        "reference": "One of the most powerful potential adversaries is the country of China, which has expressed adversarial positions regarding the cryptocurrency and demonstrated powerful capabilities to influence it.",
        "metrics": {
          "SARI": 45.86975524475525,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "For decentralised P2P networks, it is very important to have a mechanism in place that allows the nodes to control resource usage and prevent flooding and denial-of-service attacks with spam. In this paper, we discuss and compare the different approaches to fully decentralised resource control that are used by projects in the cryptocurrency space. The introduced methods are then applied to design a decentralised exchange for Namecoin names (or more generally, crypto assets) as an example.",
        "prediction": "In decentralized peer-to-peer networks, it is crucial to have a way for the nodes to manage resource usage and protect against spam attacks and denial-of-service issues. This paper reviews and compares various methods for decentralized resource control used in cryptocurrency projects. We then apply these methods to create a decentralized exchange for Namecoin names, which can also be used for other crypto assets.",
        "reference": "In decentralised P2P networks nodes need a way  to control resource usage and prevent flooding and denial-of-service attacks. We compared different approaches to decentralised resource control in cryptocurrency projects. We designed a decentralised exchange for crypto assets, for  example Namecoin names, with these methods.",
        "metrics": {
          "SARI": 40.69020393568133,
          "BLEU": 0.12492273568443224,
          "FKGL": 12.7
        }
      },
      {
        "source": "Cryptocurrency is a rapid developing financial technology innovation which has attracted a large number of people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have triggered panic and chain reactions towards the application and adoption of cryptocurrency and have caused public security related events. In this paper, we analyze the dynamics and systemic risk of the cryptocurrency market based on the public available price history. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.",
        "prediction": "Cryptocurrency is a rapidly growing financial technology that has gained interest from many people worldwide. Its fast changes, sudden price swings, and varying responses from governments have created confusion and reactions regarding its use and acceptance, leading to concerns about safety. This paper looks at the behavior and risks in the cryptocurrency market using publicly available price data. Our analysis shows, in line with public views, that the cryptocurrency market is quite delicate and unstable.",
        "reference": "Cryptocurrency is a financial technology innovation which has attracted many people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have caused panic. In this paper, we analyze the dynamics and risks of the cryptocurrency market. Consistent with public perception, our analysis reveals that the cryptocurrency market is fragile and unstable.",
        "metrics": {
          "SARI": 29.900276530017102,
          "BLEU": 0.10951018052014812,
          "FKGL": 11.8
        }
      },
      {
        "source": "Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by treating it as a financial asset. We position cryptocurrency by comparing its dynamic characteristics with two traditional and massively adopted financial assets: foreign exchange and stock. Our investigation suggests that the dynamics of cryptocurrency are more similar to stock. As to the robustness and clustering structure, our analysis shows cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "prediction": "Cryptocurrency was first created as a form of digital currency and has since developed into various uses including finance and investment. This paper aims to provide insights into understanding cryptocurrency as a financial asset. We compare its behavior to two widely used financial assets: foreign exchange and stocks. Our findings suggest that cryptocurrency behaves more like stocks. However, the cryptocurrency market is more fragile and less stable than the stock market, making it a high-risk environment for investing.",
        "reference": "Cryptocurrency was initially an implementation of digital currency, then derivatives were created in various fields. This paper aims to help understand cryptocurrency. We compared cryptocurrency with foreign exchange and stock. Our investigation suggests that cryptocurrency is similar to stock. Our analysis shows that cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "metrics": {
          "SARI": 36.946031959125605,
          "BLEU": 0.1512556089324472,
          "FKGL": 11.7
        }
      },
      {
        "source": "The smart device owning rate such as smart phone and smart watch is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time, blockchain-based cryptocurrency is becoming a nonnegligible type of currency and the total value of all types of cryptocurrency has reached USD 200 billion. Therefore, it is a natural demand to support cryptocurrency payment on mobile devices. Considering the poor infrastructure and low penetration of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are the two main obstacles for mobile payment using cryptocurrency. We propose two different schemes for cryptocurrency mobile payment, one involves a centralized bank and the other one does not require any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.",
        "prediction": "The use of smart devices like smartphones and smartwatches is at an all-time high, and mobile payment is now a key method for transactions. At the same time, cryptocurrency, which is based on blockchain technology, is becoming increasingly important, with its total value reaching USD 200 billion. This leads to a clear need for supporting cryptocurrency payments on mobile devices. This is particularly appealing in developing countries that have limited financial services. However, high storage costs and slow payment processing are major challenges for using cryptocurrency in mobile payments. We suggest two approaches for mobile cryptocurrency payments: one that involves a centralized bank and one that operates without a central authority. Additionally, we present a way for banks to comply with KYC (know your customer) and AML (anti-money laundering) regulations when processing cryptocurrency mobile payments.",
        "reference": "The owning rate of smart devices is higher than ever before and mobile payment has become a major payment method. Cryptocurrency is becoming an important type of currency and the total value of cryptocurrencies has reached USD 200 billion. Therefore, support of cryptocurrency payment on mobile devices is a natural demand. Considering the poor infrastructure and the low usage of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are two obstacles for cryptocurrency mobile payment. We propose two solutions, one with a centralized bank and the other one without any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) expectations about cryptocurrency mobile payment.",
        "metrics": {
          "SARI": 29.329181037390462,
          "BLEU": 0.12363579693958372,
          "FKGL": 12
        }
      },
      {
        "source": "Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.",
        "prediction": "Recent financial crises have led to a lot of research on how contagion effects and herding behavior affect financial markets. However, there has been less focus on how financial news influences these markets. We introduce a new way to measure collective behavior in financial news called the News Cohesiveness Index (NCI), which we suggest can help indicate systemic risk. We analyzed the NCI using financial documents from major news websites daily, from October 2011 to July 2013, to study the relationship between financial markets and related news. We believe that when financial news shows strong cohesion, it reflects changes in financial markets. This cohesiveness is a more general and reliable way to measure systemic risk in news compared to simply counting specific terms. Our findings show that cohesiveness in financial news is closely linked to and affected by market volatility.",
        "reference": "Motivated by recent financial crises, research efforts have been put into studying contagion effects (spread of market disturbances) and herding behaviour (individuals acting collectively without centralized direction) in financial markets. Much less has been said about influence of financial news on financial markets. We propose a measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator, which signals probability of financial system crises. We evaluate the NCI on financial documents from Web news sources and analyse the relation between financial markets and financial news. We hypothesized that strong cohesion in financial news reflects variations in the financial markets. Cohesiveness is a better measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results show that cohesiveness in the financial news is highly correlated with variations in the financial markets.",
        "metrics": {
          "SARI": 24.9555446263242,
          "BLEU": 0.16997472573839695,
          "FKGL": 12.3
        }
      },
      {
        "source": "In this paper, I propose a methodology to study the comovement between the entropy of different financial markets. The entropy is derived using singular value decomposition of the components of stock market indices in financial markets from selected developed economies, i.e., France, Germany, the United Kingdom, and the United States. I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using a dynamic factor model and derive a common factor behind the entropy movements in these four markets.",
        "prediction": "In this paper, I suggest a method to examine how the changes in entropy of various financial markets are related. I calculate the entropy by using a technique called singular value decomposition on stock market data from countries like France, Germany, the United Kingdom, and the United States. I analyze how a change in the entropy in the United States influences the entropy in the other markets. Additionally, I create a dynamic model to describe the entropy and identify a common factor that drives the changes in entropy in these four markets.",
        "reference": "In this paper, I propose a methodology to study the comovement (correlation on profits on investment) between the entropy (randomness and uncertainty) of different financial markets. The entropy is based on the components measuring the stock market in financial markets from selected developed economies (France, Germany, the United Kingdom, and the United States). I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using different time variables and obtain a common factor behind the entropy movements in these four markets.",
        "metrics": {
          "SARI": 32.58447822651038,
          "BLEU": 0.3087665989656413,
          "FKGL": 12.3
        }
      },
      {
        "source": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties - none of which passes through company-owned systems, and is to all intents and purposes 'off the grid'. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics. Detecting 'tech-savvy' corporate fraudsters is a constant game of catch-up. It's not only about playing catch-up with the intellect, motives and awareness of the e-fraudster, but also the technologies that can be misused. What's more, the methods through which the technology can be manipulated to secrete, disguise and protect fraudulent activities - all while staying away from corporate networks where they can readily be monitored and detected - are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.",
        "prediction": "Using mobile phone forensics to investigate fraud isn't new. However, smartphones have advanced, and so have the fraudsters using them. Smartphones have many features that can help in stealing data or making inappropriate contacts without going through company systems. Employers should understand these risks when giving out devices and should have procedures ready for when suspicions arise, as noted by Philip Ridley from CCL-Forensics. Finding tech-savvy fraudsters is an ongoing challenge. It involves keeping up with their intelligence, motives, and the technologies they can misuse. Additionally, methods for hiding and protecting fraudulent actions while avoiding detection by company networks are always changing. As a result, a company's important information and sensitive data are at risk of theft or sabotage.",
        "reference": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones have functionalities that can be used for data theft or inappropriate contact with other parties. They are out of the systems of the company and are not monitored or controlled. Employers need to be aware of these risks when devices are delivered and have processes when suspicions are raised. Detecting fraudsters with computer skills is a constant game of catch-up. It's not only about playing catch-up with the cyberfraudster, but also the technologies that can be misused. Moreover, the methods assisting fraudulent activities (still out of reach of company systems) are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or theft.",
        "metrics": {
          "SARI": 28.275625050684855,
          "BLEU": 0.09219964199505679,
          "FKGL": 10.3
        }
      },
      {
        "source": "Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes to phishing campaigns, a cadre of file matching algorithms is implemented to detect phishing websites based on their content, employing a custom data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using a variety of different content-based approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "prediction": "Phishers keep changing the source code of the web pages they use to make them look like real websites and to avoid being caught. These changes can be very small or more obvious, like adding or removing important content. To tackle these changes in phishing campaigns, several file matching algorithms are used to find phishing websites based on their content. This uses a custom data set that includes 17,992 phishing attempts against 159 brands. The experiments show that some methods can detect over 90% of phishing sites while keeping the number of false alarms low.",
        "reference": "Phishers, people who attempt to trick people over the internet to steal their money, continue to modify the web pages used in their attacks to imitate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as changing the source code, which is the programming instructions, or as apparent as adding or removing significant content. To respond to these changes to phishing campaigns, algorithms are used to detect phishing websites based on their content, employing a data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using different approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "metrics": {
          "SARI": 24.154546739276746,
          "BLEU": 0.14485733103260637,
          "FKGL": 8.3
        }
      },
      {
        "source": "Mobile devices are very common in everyone’s day-to- day life. Nowadays such devices come with many features of desktop or laptop. Hence people can use these devices for diverse applications. As the acceptability and usability of such devices are very high, there are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is in hike. So it becomes necessary to digitally analyze such devices requiring cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and an agent based approach for logically acquiring such devices. Also it describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.",
        "prediction": "Mobile devices are now a big part of our daily lives. They have many features similar to desktops and laptops, allowing people to use them for a variety of tasks. Because these devices are so popular and easy to use, they can also be misused for illegal activities. The number of mobile phones involved in cyber crimes is increasing. Therefore, it is important to analyze these devices using cyber forensics tools. This paper talks about different types of digital evidence found in Windows Mobile smartphones and presents a method for retrieving information from these devices. It also describes a tool that has been created to acquire and analyze Windows Mobile devices and WinCE PDAs.",
        "reference": "Mobile devices are very common in everyone’s day-to-day life. Nowadays such devices come with many features of computers. People can use these devices for diverse applications. There are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is increasing. So it becomes necessary to digitally analyze such devices with cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and how to acquire such devices. Also it describes a forensic tool for acquiring and analyzing Windows Mobile devices and personal digital assistants.",
        "metrics": {
          "SARI": 31.494198793343703,
          "BLEU": 0.22476226813048314,
          "FKGL": 9.6
        }
      },
      {
        "source": "Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It gives the information about the processing history of an image, and also can expose forgeries present in an image. There have been a few methods proposed to detect different types of image editing operations in a single framework. However, all the operations have to be known a priori in the training phase. But, in real-forensics scenarios it may not be possible to know about the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.",
        "prediction": "Detecting various types of image editing is an important issue in image forensics. This helps reveal how an image has been processed and can uncover any alterations. Some methods have been suggested to identify different editing types together. However, these methods require prior knowledge of all editing types during training. In real-life forensic situations, it is often impossible to know all the edits made to an image. To address this, we introduce a new deep learning method that can tell apart different image editing types. Our method uses a special deep learning setup called a siamese neural network to classify small parts of images as similarly or differently edited. Once the network learns how to recognize different editing features, it can also identify edits not included in the training. Our experiments demonstrate that this method is effective in detecting and distinguishing various image editing operations.",
        "reference": "Detection of image editing is an important task in image forensics. The edit history of an image can expose forgeries in the image. Various methods have been proposed to detect image editing operations. However all of the possible types of image manipulation must be known in advance. But, in the real world, manipulations on images are sometimes unknowable. A novel deep learning-based method can distinguish between different types of image editing operations. This method uses a deep siamese neural network to compare images two by two. Using deep learning, the computer can learn to see features of edited images as compared to unedited and learn to differentiate between different image editing operations. This method works to find out what editing an image has undergone.",
        "metrics": {
          "SARI": 44.81211576505489,
          "BLEU": 0.14040491450866727,
          "FKGL": 10.7
        }
      },
      {
        "source": "The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices – potentially progressing a wide variety of currently hindered digital investigations.",
        "prediction": "The rise of Internet of Things (IoT) devices means they will be more important for digital forensic investigations in the future. These devices often have limited ways to communicate, like USB ports or WiFi/Bluetooth. As security and privacy for user data become more important, encryption is now common in consumer devices, including IoT. This creates a challenge for digital forensics when trying to analyze data from IoT devices. This work looks at electromagnetic (EM) side-channel analysis to help with digital forensic investigations of IoT devices. EM side-channel analysis uses unintentional electromagnetic signals to listen in on how devices operate and handle data. This method does not change anything on the device, making it a good option for forensic investigations. The paper discusses different EM side-channel attack techniques that could be useful for investigating IoT devices. The knowledge gained from this study helps identify potential future uses of this technique for digital forensics, which could improve many investigations that currently face challenges.",
        "reference": "Internet of Things (IoT) devices (devices connected to the internet, collecting and sharing data) will be used in digital forensic investigations in the future. These devices have limited interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing focus on the security and privacy of user data, built-in encryption (which encode information) is becoming commonplace in devices. This presents a significant challenge to digital forensic investigations, where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where electromagnetic emissions are used for eavesdropping on the operations and data handling of devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected according to their applicability in IoT device investigations. The data obtained from our survey are used to identify promising future applications of the technique for digital forensic analysis on IoT devices.",
        "metrics": {
          "SARI": 30.36482454858503,
          "BLEU": 0.21107852729409993,
          "FKGL": 12.6
        }
      },
      {
        "source": "The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is of paramount importance. However, this can be difficult since the attacker has many opportunities and means to hide such changes. In particular, cloud computing, host and guest machine time can be manipulated in various ways by an attacker. Guest virtual machines are especially vulnerable to attacks coming from their (more privileged) host. As such, it is important to guarantee the timeline integrity of both hosts and guests in a cloud, or at least to ensure that the alteration of such timeline does not go undetected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. Further, we describe a novel architecture for host and guest time alteration detection and correction/resilience with respect to compromised hosts and guests. The proposed framework has been implemented on an especially built simulator.",
        "prediction": "Changing machine time in harmful ways is a major issue in computer forensics. Our results show that our solution is possible. It is very important to detect these changes and recreate the real sequence of events. However, this can be challenging because attackers have many ways to hide their actions. In particular, in cloud computing, the time on host and guest machines can be manipulated by attackers. Guest virtual machines are particularly at risk from attacks by their more privileged host machines. Therefore, it is crucial to maintain the integrity of the timeline for both hosts and guests in the cloud or at least make sure that any changes to it are detected. In this paper, we review the issues concerning the integrity of host and guest machine time in the cloud. Additionally, we explain a new system for detecting and correcting time alterations in compromised hosts and guests. This proposed system has been tested using a specially built simulator.",
        "reference": "Machine timestamp falsification can be used in cyber attacks. The algorithm detects manipulations effectively. That is why it is important to detect such changes and to reconstruct the actual timeline of events. However, the hacker can try to hide their tracks. In cloud computing, hackers can manipulate host and guest machine time. Guest virtual machines use computing resources provided by a physical machine called a host. Guest virtual machines are especially vulnerable to attacks coming from their host. That is why the timeline integrity of both hosts and guests in a cloud is important. Any manipulation of such a timeline should be detected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. A new algorithm is proposed to detect host and guest time manipulations and to correct or reject them. A simulator has been built for this purpose.",
        "metrics": {
          "SARI": 44.7986735619912,
          "BLEU": 0.17742114421251665,
          "FKGL": 9.5
        }
      },
      {
        "source": "Image tampering, being readily facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. As the most popular multimedia data, JPEG images can be easily tampered without leaving any clues; therefore, JPEG-based forensics , including the detection of double compression, interpolation, rotation, etc., has become an active research topic in multimedia forensics. Nevertheless, the interesting issue of detecting image tampering and its related operations by using the same quantization matrix has not been fully investigated. Aiming to detect such forgery manipulations under the same quantization matrix, we propose a detection method by using shift-recompression -based reshuffle characteristic features. The learning classifiers are applied for classification. Our experimental results indicate that the approach is indeed highly effective in detecting image tampering and relevant manipulations with the same quantization matrix.",
        "prediction": "Image tampering is becoming a bigger issue with the rise of digital technology, especially with JPEG images which are popular and can be altered easily without obvious signs. This has made it important to study JPEG forensics, like spotting double compression and rotation. However, detecting tampering using the same quantization matrix has not been explored much. To address this, we suggest a method that detects forgery by analyzing shift-recompression features. We use learning classifiers to help with identification. Our tests show that this method works very well in finding tampering and related changes made with the same quantization matrix.",
        "reference": "Image tampering has become a serious problem with the advancement of digital techniques. JPEG images can be easily manipulated without leaving any clues. This is why researchers are trying to develop methods for JPEG image manipulation detection. However, methods based on image compression techniques, like quantization, are rarely applied. To detect such manipulations, we propose a detection method to identify inconsistencies in the quantization matrix caused by the manipulation. Then machine learning algorithms are applied to detect image manipulation. Experimental results show that the approach is highly effective in detecting image tampering.",
        "metrics": {
          "SARI": 36.28708282241506,
          "BLEU": 0,
          "FKGL": 10.9
        }
      },
      {
        "source": "In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations of its monocular camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets of the whole body including the legs, as well as the camera extrinsic and intrinsic parameters. Furthermore, we developed an approach to automatically select a subset of configurations for the calibration process that yields a good trade-off between the number of observations and accuracy. Further, our approach to configuration selection yields substantially better optimization results compared to randomly chosen viable configurations. Hence, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "prediction": "In this paper, we introduce a new way to accurately adjust the kinematic model of a humanoid robot using data from its monocular camera. Our method estimates the parameters of the entire model, which includes the joint angle offsets for the whole body, such as the legs, along with the camera's external and internal settings. We also created a method to automatically choose a set of configurations for the calibration that balances the number of observations with the accuracy. Our selection method leads to much better optimization results than using randomly chosen configurations. As a result, our system needs fewer configurations to produce accurate results. Our optimization is general, and the implementation can be easily used for different humanoid robots.",
        "reference": "In this paper, we present a novel approach to accurately calibrate a humanoid and its motions (robot with human form or characteristics) based on observations of its camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets and the camera parameters. We developed an approach to automatically select configurations for the calibration process that produces a good compromise between the number of observations and accuracy. Our approach to configuration selection generates better optimization results compared to randomly chosen viable configurations. Thus, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "metrics": {
          "SARI": 28.342940368813867,
          "BLEU": 0.23219220603544263,
          "FKGL": 13.5
        }
      },
      {
        "source": "In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. From the data of an onboard consumer-grade depth camera, our system estimates the robot's pose to compensate for drift of odometry and maintains a heightmap representation of the environment. Based on this model, our system iteratively computes sequences of safe actions including footsteps and whole-body motions, leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system leads to robust navigation in cluttered environments and the robot is able to traverse highly challenging passages.",
        "prediction": "In this paper, we introduce a navigation system for humanoid robots, enabling them to move independently in unfamiliar and messy spaces. Using data from a consumer-level depth camera, our system estimates the robot's position to correct for errors in movement tracking and creates a heightmap of the environment. Based on this information, the system repeatedly plans safe actions like walking steps and body movements to guide the robot to its target. To effectively check for obstacles during planning, we created a new method that considers the shape of the robot and the objects around it. Our experiments with a Nao humanoid show that this system allows for reliable navigation in tough environments, enabling the robot to pass through difficult areas.",
        "reference": "In this paper, we present a navigation system that allows humanoid robots (robots with human form or characteristics) to autonomously navigate in unknown, cluttered environments. From the data of a camera which can judge depth and distance, our system estimates the robot’s position and maintains a height representation of the environment. Our system determines safe actions (including footsteps and body motions) leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments, our system allows the robot to navigate in clustered environments and traverse challenging passages.",
        "metrics": {
          "SARI": 33.5557432172338,
          "BLEU": 0.2010016346791916,
          "FKGL": 13.8
        }
      },
      {
        "source": "In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict traversal costs corresponding to potential obstacles in regions not yet observable by the robot's sensors. By taking into account the predicted costs for path planning, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.",
        "prediction": "In this paper, we present a way for robots to navigate effectively in crowded indoor spaces. We suggest estimating how dense obstacles are based on the objects the robot has already seen. This information helps predict how difficult it will be to move through areas that the robot can't currently see. By using these predictions for planning its path, the robot can navigate more wisely and avoid getting stuck in messy areas. Our experiments show that this method helps robots move efficiently through cluttered environments and finish tasks much faster than traditional methods that don't make predictions.",
        "reference": "In this paper, we introduce an approach for robot to navigate through cluttered indoor environments. We propose to estimate obstacles based on already detected objects and use them to predict potential obstacles in regions not yet observable by the robot. The robot is then able to navigate in a more anticipated manner and reduces the risk of getting stuck. Our method enables the robot to efficiently navigate through cluttered environments and faster than a standard approach not using any prediction.",
        "metrics": {
          "SARI": 28.998758282057462,
          "BLEU": 0.09840950788928005,
          "FKGL": 10.9
        }
      },
      {
        "source": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, step-by-step following of the human might not be the optimal solution since better paths for the robot exist. We propose to perform a prediction about the human's future movements and use this information in a reinforcement learning framework to generate foresighted navigation actions for the robot. Since frequent occlusions of the human will occur due to obstacles and the robot's constrained field of view, the estimate about the humans's position and the prediction of the next destination are affected by uncertainty. Our approach deals with such situations by explicitly considering occlusions in the reward function such that the robot automatically considers to execute actions to get the human in its field of view. We show in simulated and real-world experiments that our technique leads to significantly shorter paths compared to an approach in which the robot always tries to closely follow the user and, additionally, can handle occlusions.",
        "prediction": "In many cases, people walk along usual paths to particular places where a mobile robot is needed. Following the human step-by-step may not be the best option because there might be better routes for the robot. We suggest predicting where the human will go next and using that information to help the robot plan its movements. Since obstacles often block the view of the human, it makes it hard to know exactly where they are heading. Our method addresses this problem by including these obstacles in the robot's decision-making process, making it try to stay in sight of the human. We demonstrate through experiments in both simulations and real-life scenarios that our approach results in much shorter paths compared to when the robot tries to always follow the human closely and can also manage obstacles effectively.",
        "reference": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, following the human might not be the optimal solution because better paths for the robot exist. We propose to predict the human’s future movements and use this information to generate navigation actions for the robot. The estimation of the human’s position and the prediction of the next destination are uncertain because of the occlusions due to obstacles and the robot’s limited field of view. Our approach deals with considering occlusions so the robot automatically considers to execute actions to get the human in its field of view. We show that our technique leads to significantly shorter paths compared to an approach in which the robot follows the user and, additionally, can handle occlusions.",
        "metrics": {
          "SARI": 24.47723823752154,
          "BLEU": 0.11240695739689388,
          "FKGL": 12.1
        }
      },
      {
        "source": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem, especially when this person moves around since the robot’s field of view is constrained and the environment structure typically poses further visibility constraints that influence the perception of the user. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. As the robot needs time to reach the possible search locations, we take this time as well as the visibility constraints into account when computing effective search locations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.",
        "prediction": "An autonomous service robot often needs to find a user to perform a task. This can be hard, especially if the person is moving around, since the robot's view is limited and the environment can block visibility. In this paper, we present a new method that estimates how visible the user is from different locations using Monte Carlo simulations. We consider the time it takes for the robot to reach these locations and the visibility limits when deciding where to search. This helps the robot select the next search spot that is most likely to allow it to see the user. Our tests in different simulated settings show that our method results in much shorter search times compared to a simpler approach that uses background information.",
        "reference": "An autonomous service robot often first has to search for a user to execute a task. This is a challenging problem, especially when this person moves around because the robot’s field of view is limited. The environment also poses visibility limitations that influence the perception of the user. In this paper, we propose a method that estimate the user’s observability in the environment. When estimating effective search locations, we take into account the time for the robot to reach the search location and the visibility limitations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments demonstrate that our approach leads to a significantly shorter search time compared to other approach.",
        "metrics": {
          "SARI": 31.634487035972786,
          "BLEU": 0.19591244113101783,
          "FKGL": 11.5
        }
      },
      {
        "source": "Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Considering dynamic obstacles introduces another level of complexity, since they can lead to necessary replanning or collisions at later stages. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.",
        "prediction": "Mobile robots need to understand their surroundings to move safely, especially in changing environments. Most robots, like humanoids and small wheeled ones, usually focus on flat areas. However, humanoids need more than just a simple 2D map because they can step over things, so they also require height information. When you add moving obstacles, it becomes more complicated, as this can lead to needing to change plans or risk collisions. In this paper, we introduce a system that first identifies flat areas in height maps and detects moving obstacles. Our system then creates prediction maps, allowing for efficient and quick path planning with low CPU usage. We demonstrate through simulations and real-world tests that our system can perform calculations in under 10 milliseconds, enabling smart real-time 3D footstep planning.",
        "reference": "Perception of the environment is necessary for mobile robots to navigate safely in dynamic environments. Most robots (humanoids and wheeled robots) rely on planar regions. For humanoids (robots with human form or characteristics), a 2D map is not sufficient because they can step over and onto objects, so they need height information. Dynamic obstacles bring another level of complexity, because it can lead to necessary replanning or collisions. In this paper, we present a framework that extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be planned in real time at low processor cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms and allows for real-time 3D footstep planning.",
        "metrics": {
          "SARI": 31.773131098587488,
          "BLEU": 0.2058460432569652,
          "FKGL": 10.5
        }
      },
      {
        "source": "In this paper, a lab automation drone notional concept is introduced. Here, a robotic limb is attached to a robotic rotorcraft. The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high throughput systems (HTS). The resulting drone could augment existing HTS operations. The 6 degree-of-freedom (DOF) arm and gripper design are presented. Test-and-evaluation approach and results are also given.",
        "prediction": "This paper introduces a concept for a lab automation drone. The drone has a robotic arm attached to a flying vehicle. The arm's gripper lets the drone handle small items like micro-arrays and test tubes used in high throughput systems. This new drone can improve current high throughput operations. The design of the arm and gripper, as well as the testing methods and results, are discussed.",
        "reference": "In this paper, a concept of lab automation drone is introduced. Here, a robotic arm is attached to a drone. The arm’s gripper allows the drone to manipulate objects such as chips and test tubes often used in scientific experimentations like high throughput systems. The drone could augment existing high throughput systems operations. The 6 degree-of-freedom arm and gripper design are presented. Results of evaluations and tests are also given.",
        "metrics": {
          "SARI": 35.68033834282434,
          "BLEU": 0.1331856055921519,
          "FKGL": 7.3
        }
      },
      {
        "source": "Robot companionship has become more popular in past years. However, humanoid gait might be somewhat unstable for these applications. Even with miniature humanoids, falls occur frequently. Thus, wheel attachments have been added onto a miniature humanoid, so it can move faster and with more stability than walking. In addition, with such attachments a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as an experimentation and performance evaluation platform. This paper discusses preliminary work regarding robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "prediction": "Robot companionship is becoming more common. However, the way humanoid robots walk can be unstable. Even small humanoid robots often fall. To help with this, wheels have been added to a small humanoid robot so it can move faster and more steadily than by walking. The robot can also switch between walking and rolling when needed. DARwIn-OP is a humanoid robot used for testing and performance evaluations. This paper talks about early work on robot companionship with a small humanoid robot that can fetch different toys using voice commands.",
        "reference": "Robot companionship has become more popular in past years. However, humanoid (robots with human form or characteristics) walking might be unstable. Even with miniature humanoids, falls occur frequently. Wheels have been added onto a miniature humanoid, so it can move faster and with more stability than walking. With wheels, a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used for experimentation and evaluation. This paper discusses preliminary work for robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "metrics": {
          "SARI": 28.046571726791598,
          "BLEU": 0.1898359797014441,
          "FKGL": 7.1
        }
      },
      {
        "source": "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds of up to 6m/sec. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning based navigation. With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest. In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot. Derived from the classic image classification algorithms, our classifier has been constructed in the form of a fast 39-layered Inception model, that evaluates the presence of roads using the tomographic reconstructions of the input frames. Based on the Inception-v3 architecture, our system performs better in terms of processing complexity and accuracy than many existing models for imitation learning. The data used for training the system has been captured from the drone, by flying it in and around urban and semi-urban streets, by experts having at least 6-8 years of flying experience. Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process. With the extensive amount of drone data that we collected, we have been able to navigate successfully through roads without crashing or overshooting, with an accuracy of 98.44%.",
        "prediction": "Autonomous vehicles are a key area of research in automation and robotics. MAVNet allows drones to fly quickly, reaching speeds of up to 6 meters per second. This research compares its results with other advanced navigation methods that rely on vision and learning. As more drones are used in civilian and commercial activities, the need for improved autonomy in these systems grows. Following government guidelines on how high drones can operate, there is a rising interest in road-tracking navigation. We propose a solution using imitation learning, where drones navigate city streets by mimicking expert pilots. Our classifier is based on a fast 39-layer Inception model that identifies roads from images captured by the drone. This system outperforms many other imitation learning models in efficiency and accuracy. The data for training was collected by experienced pilots flying drones in urban areas, with permissions obtained to ensure safety. After collecting extensive drone data, we achieved successful navigation through roads with an accuracy of 98.44% without crashes or overshooting.",
        "reference": "Current academic and industrial research is interested in autonomous vehicles. MAVNet computational efficiency enables the drone to fly up to 6m/sec. The solution is compared with other recent methods. Drones are increasingly used in the civilian and commercial domain and need to be autonomous. Governments set guidelines on the operation ceiling of civil drones. So, road-tracking based navigation is attracting interest. Researchers propose data-driven solutions allowing drones to autonomously navigate city streets, learning to fly by imitating an expert pilot. The algorithm, based on the Inception model, detects roads using tomographic reconstructions. The Inception model is an extension of classic image classification algorithms. The Inception-v3 architecture has better accuracy than many existing models of imitation learning. The data used for training the system was captured from the drone over urban streets, navigated by an expert pilot. Data collection requires special permission to ensure pedestrian security. The drone can navigate successfully through roads without crashing or overshooting; its accuracy is 98.44%.",
        "metrics": {
          "SARI": 35.45608102616302,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "Advances in electronics and sensor technology have widened the scopes of networked drones to include applications as diverse as surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording for their operations. Large drones are used singly in missions while small ones are used in formations or swarms. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "prediction": "New technologies in electronics and sensors have expanded the use of networked drones for various tasks like surveillance, video recording, advertising, signal transmission, and delivery. These tasks often involve video recording. Large drones are typically used for individual missions, while small drones operate in groups or swarms and are particularly beneficial for civilian uses. The interest in small drones for activities like group flights and entertainment has led to the use of networked drone systems. A system for controlling drone formations in real-time has been proposed to create group displays. Simulations indicate that these formations can effectively communicate messages.",
        "reference": "Advances in technology allowed drones to be used for surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording. Large drones are used individually while small ones are used in groups. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "metrics": {
          "SARI": 21.071742731815082,
          "BLEU": 0.172205900736689,
          "FKGL": 12.1
        }
      },
      {
        "source": "This study focuses on the digital marketing capabilities of tourism SMEs. The study addresses the question of how the use of ICT-based tools benefit the organisational capabilities of a company. By adopting marketing as a set-of-skills approach, the study provides new insights into the existing tourism literature on e-marketing. Initial findings indicate that the digital marketing capabilities of companies are transformed through ICT-based uptake. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that the use of ICT-based tools transforms digital marketing capabilities from a set of abilities that enables tourism SMEs not only to float in a web-marketing stream, but also to lead such a stream.",
        "prediction": "This study looks at how tourism small and medium-sized enterprises (SMEs) use digital marketing tools. It explores how these tools help improve the skills of a company. By viewing marketing as a set of skills, the study adds new information to the existing research on online marketing in tourism. The early results show that using these digital tools changes the marketing abilities of companies. Four main capabilities were found, each growing through the use of these tools. One important result is that these digital marketing tools help tourism SMEs not just keep up but also take charge in online marketing.",
        "reference": "This study focuses on the digital marketing capabilities of tourism small and medium-sized enterprises (SMEs). The study questions on how information and communication technology (ICT) tools can help the organisational capabilities of a company. The study provides new insights from tourism studies on digital marketing. Findings show that digital marketing capabilities of companies are transformed by ICT tools. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that ICT tools transform these capabilities in a set of abilities for SMEs in web-marketing.",
        "metrics": {
          "SARI": 36.00375779918074,
          "BLEU": 0.1610933805981095,
          "FKGL": 8.6
        }
      },
      {
        "source": "The prevalence and rapid development of the Internet and mobile technology in recent decades has revamped our living styles and daily habits. To ride on the digital trend, more business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. The rise of Key Opinion Leaders (KOLs), social media platforms, and Omni-channel retailing have attracted countless business entities to consider the adoption of digital marketing tools for promoting and advertising their brands and products. However, with the increasing diversity of the types of digital marketing tools, they must be carefully selected based on a multiple number of criterion. In this paper, a fuzzy-AHP method is proposed and developed for assisting industry practitioners in systematically and effectively evaluate and select proper digital marketing tool(s) for adoption. The developed method not only streamlines the internal business process of digital marketing tool selection, but it also increases the practitioner's effectiveness of achieving the pre-defined strategic marketing objectives.",
        "prediction": "The growth of the Internet and mobile technology has changed how we live and work. Businesses are increasingly going digital, especially in marketing and advertising. Key Opinion Leaders, social media, and multi-channel selling have encouraged many companies to use digital marketing tools to promote their brands and products. However, with many different digital marketing tools available, businesses need to choose carefully based on several factors. This paper introduces a fuzzy-AHP method to help industry professionals assess and select the right digital marketing tools. This method not only makes the selection process smoother but also helps achieve their marketing goals more effectively.",
        "reference": "The development of the Internet and mobile technology in recent decades has modified our living styles and daily habits. More business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. Many businesses consider the adoption of digital marketing tools for their promotion and advertisement, due to the rise of Key Opinion Leaders (person with expert status and influence), social media platforms, and Omni-channel retailing (multiple channels but a uniform experience for the customer). However, with the increasing diversity of digital marketing tools, they must be carefully selected. In this paper, a method to organize and analyze complex decisions is proposed and developed for assisting businesses to evaluate and select appropriate digital marketing tools. The developed method rationalizes and simplifies the process of digital marketing tool selection. It also increases effectiveness of achieving strategic marketing objectives.",
        "metrics": {
          "SARI": 26.801949964151156,
          "BLEU": 0.09681675220189309,
          "FKGL": 12.2
        }
      },
      {
        "source": "In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be primarily contingent on changes in muscle size. Given this assumption, there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength. However, the conclusion that a change in muscle size affects a change in strength is surprisingly based on little evidence. We suggest that these changes may be completely separate phenomena based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with detraining, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load resistance training, yet divergent results in strength.",
        "prediction": "This paper looks at a topic first discussed in 1955: we still lack strong evidence that increasing muscle size from exercise is crucial for gaining strength. Many believe that long-lasting increases in strength mainly depend on muscle size changes. Because of this belief, a lot of focus has been on programs that aim to boost both muscle size and strength. However, the idea that bigger muscles lead to more strength is surprisingly not well supported by evidence. We propose that these two changes might be different processes because of: (1) the weak link between increases in muscle size and strength after training; (2) the fact that losing muscle mass during rest doesn’t always lead to a loss of strength; and (3) similar muscle growth from both light and heavy weight training, but different strength outcomes.",
        "reference": "In this paper we revisit a topic originally discussed in 1955, namely the lack of evidence that muscle growth plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be related to changes in muscle size. Given this assumption, training programs made for increasing both muscle size and strength were popular. However, the conclusion that a change in muscle size has an influence on strength is surprisingly based on little evidence. We suggest that these changes may be based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with the interruption of training, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load strength training, yet different results in strength.",
        "metrics": {
          "SARI": 27.795506208385778,
          "BLEU": 0.22588430159096454,
          "FKGL": 11.5
        }
      },
      {
        "source": "Low-intensity occlusion (50-100 mm hg) training provides a unique beneficial training mode for promoting muscle hypertrophy. Training at intensities as low as 20% 1 repetition maximum with moderate vascular occlusion results in muscle hypertrophy in as little as 3 weeks. A typical exercise prescription calls for 3 to 5 sets to volitional fatigue with short rest periods. The metabolic buildup causes positive physiologic reactions, specifically a rise in growth hormone that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "prediction": "Low-intensity occlusion training uses lower weights (50-100 mm Hg) to help muscles grow bigger. Training with weights as light as 20% of the maximum you can lift, along with some restriction of blood flow, can lead to muscle growth in just 3 weeks. Typically, this kind of workout involves doing 3 to 5 sets until you can't continue, with short breaks in between. This method increases certain hormones, like growth hormone, more than lifting heavier weights. Occlusion training is suitable for people who can't lift heavy weights due to joint pain, those recovering from surgery, patients in cardiac rehab, athletes needing to reduce loads, and astronauts.",
        "reference": "Low-intensity occlusion training, which is a training method with blood flow restriction, provides a unique beneficial training mode for muscle growth. Training at intensities as low as 20% of maximum strength with moderate occlusion results in muscle growth in 3 weeks. A typical exercise consists of 3 to 5 sets to muscle failure with short rest periods. The metabolic accumulation causes positive physiologic reactions, specifically a rise in growth hormone, which is one of the main hormone responsible of muscle growth, that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "metrics": {
          "SARI": 25.79681674782306,
          "BLEU": 0.15795272160509408,
          "FKGL": 10.4
        }
      },
      {
        "source": "Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. We and others have demonstrated that aerobic exercise acutely and chronically alters protein metabolism and induces skeletal muscle hypertrophy. These findings promote an antithesis to the status quo by providing novel perspective on skeletal muscle mass regulation and insight into exercise-countermeasures for populations prone to muscle loss.",
        "prediction": "Recent beliefs say that aerobic exercise training doesn't significantly change muscle size. However, our research and that of others show that aerobic exercise can both quickly and over time change how proteins are used in the body and can lead to an increase in muscle size. These results challenge traditional views and offer new ideas about how muscle mass is regulated and how exercise can help people at risk of losing muscle.",
        "reference": "Current belief suggests cardio exercise training has minimal effect on muscle size. We and others have demonstrated that cardio exercise alters protein metabolism (responsible for protein synthesis and muscle growth) and induces muscle growth. These findings against the current belief provide a novel perspective on muscle mass regulation and insight into exercise to prevent muscle loss.",
        "metrics": {
          "SARI": 28.741465219793387,
          "BLEU": 0,
          "FKGL": 12.7
        }
      },
      {
        "source": "Stretch training is widely used in a variety of fitness-related capacities such as increasing joint range of motion, preventing contractures and alleviating injuries. Moreover, some researches indicate that stretch training may induce muscle hypertrophy; however, studies on the topic have been primarily relegated to animal and in vitro models. The purpose of this brief review was to evaluate whether stretch training is a viable strategy to induce muscle hypertrophy in humans. Of the 10 studies identified, 3 observed some significantly positive effects of stretch training on muscle structure. Intriguingly, in these studies, the stretching was carried out with an apparatus that aided in its performance, or with an external overload. Of the 5 available studies that integrated stretching into a resistance training programme, 2 applied the stretching in the interset rest period and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to confer beneficial changes in muscle size and architecture; alternatively, albeit limited evidence suggests that when stretching is done with a certain degree of tensile strain (particularly when loaded, or added between active muscle contractions) may elicit muscle hypertrophy.",
        "prediction": "Stretch training is often used for fitness purposes like improving joint movement, preventing stiffness, and reducing injuries. Some research suggests that stretch training can also help muscles grow, but most studies have been done on animals or in lab settings. This review looks at whether stretch training can promote muscle growth in humans. Out of 10 studies reviewed, 3 showed notable positive effects on muscle structure from stretching. Interestingly, these studies used machines to help with stretching or added extra weight. Among 5 studies that included stretching in resistance training, 2 used stretching during rest periods and showed increased muscle growth. In summary, low-intensity passive stretching does not seem to change muscle size or structure beneficially, but limited evidence suggests that stretching with some tension, especially when combined with weight or done between muscle contractions, may promote muscle growth.",
        "reference": "Stretch training is used in a variety of fitness capacities such as increasing flexibility, preventing contractures (shortening of muscle) and reduce injuries. Moreover, some researches indicate that stretch training may induce muscle growth. The purpose of this review was to evaluate if stretch training is a viable strategy to induce muscle growth in humans. Of the 10 studies identified, 3 observed positive effects of stretch training on muscles. In these studies, the stretching was performed with external help like equipment or load. Of the 5 available studies that integrated stretching into strength training, 2 applied the stretching in the rest period between the sets and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to produce beneficial changes in muscles but evidence suggests that when stretching is done with a certain intensity (particularly when loaded, or added between active muscle contractions), it may induce muscle growth.",
        "metrics": {
          "SARI": 28.46926502943912,
          "BLEU": 0.09764815774813172,
          "FKGL": 11.1
        }
      },
      {
        "source": "Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. Therefore, the purpose of this review is to discuss the effects of cycle training on muscle size and strength of the lower extremity and the possible mechanisms for increasing muscle size with cycle training. It is plausible that cycle training requires a longer period to significantly increase muscle size compared to typical resistance training due to a much slower hypertrophy rate. Cycle training induces muscle hypertrophy similarly between young and older age groups, while strength gain seems to favor older adults, which suggests that the probability for improving in muscle quality appears to be higher in older adults compared to young adults. For young adults, higher-intensity intermittent cycling may be required to achieve strength gains. It also appears that muscle hypertrophy induced by cycle training results from the positive changes in muscle protein net balance.",
        "prediction": "Cycle training is a common part of exercise programs aimed at improving aerobic fitness and heart health. However, we still need more information about how cycle training affects muscle size and strength. It is known that professional cyclists have bigger muscles than those who don’t cycle. This review looks at how cycle training influences muscle size and strength in the legs and the possible reasons for muscle growth from this training. It seems that cycle training takes longer to noticeably increase muscle size than regular strength training because muscle growth happens more slowly. Both young and older people can see muscle growth from cycle training, but older adults may gain strength more effectively, indicating that they might improve muscle quality more than younger adults. Younger adults might need to do more intense cycling to see strength gains. Additionally, muscle growth from cycle training appears to be linked to better overall muscle protein balance.",
        "reference": "Cycling training is performed as a major part of any training program which objective is to improve cardiovascular capacities and health. However, the effect of cycling training on muscle size and strength still requires a better comprehension. Therefore, the purpose of this review is to discuss the effects of cycling training on muscle size and strength of the legs and the possible mechanisms for increasing muscle size with cycling training. It is plausible that cycling training requires a longer period to significantly increase muscle size compared to strength training due to a much slower muscle growth rate. Cycling training induces muscle growth similarly between young and older people, while strength gain seems to favor older adults. For young adults, series of high-intensity cycling may be required to achieve strength gains. It also appears that muscle growth induced by cycling training results from the positive changes in protein balance, which is the balance between protein intake which leads to muscle growth and protein loss which leads to muscle loss).",
        "metrics": {
          "SARI": 29.370160440653788,
          "BLEU": 0.12716569725778118,
          "FKGL": 9.6
        }
      },
      {
        "source": "Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship between training dose/volume and physiological response has been hypothesized to exist. However, the majority of available evidence supports a clear dose-response relationship between resistance training volume and physiological responses, such as muscle hypertrophy and health outcomes. Additionally, there is a paucity of data to support the inverted U-shaped response. The overarching principle argued herein is that volume is the most easily modifiable variable that has the most evidenced-based response with important repercussions, be these muscle hypertrophy or health-related outcomes.",
        "prediction": "Resistance training is the best way to build muscle mass and has many health benefits. It is considered safe and helpful in treating and preventing various diseases. Many research studies have focused on finding a quick and minimal amount of exercise that is still effective. There is a theory that suggests a relationship between the amount of training and how the body responds, which may look like an inverted U-shape. However, most evidence clearly shows that there is a direct relationship between how much resistance training you do and the physiological effects, like muscle growth and health improvements. There is not enough data to strongly support the inverted U-shaped idea. The main point made here is that the training volume is the easiest thing to change and has the strongest evidence for how it affects muscle growth and health outcomes.",
        "reference": "Strength training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is helpful for treating and preventing diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship (which means that the efficiency increases until a point, and then starts decreasing) between training volume and physiological response has been hypothesized to exist. However, evidence supports a clear correlation between resistance training volume (number of exercises, sets, etc.) and physiological responses, such as muscle growth and health outcomes. Additionally, there is a lack of data to support the inverted U-shaped response. The principle suggested is that volume is the most easily modifiable variable that has the most important response, for muscle growth or health outcomes.",
        "metrics": {
          "SARI": 26.884379307100847,
          "BLEU": 0.1328795208853072,
          "FKGL": 9.9
        }
      },
      {
        "source": "Although the effects of short versus long inter-set rest intervals in resistance training on measures of muscle hypertrophy have been investigated in several studies, the findings are equivocal and the practical implications remain unclear. Current evidence indicates that both short and long inter-set rest intervals may be useful when training for achieving gains in muscle hypertrophy. Novel findings involving trained participants using measures sensitive to detect changes in muscle hypertrophy suggest a possible advantage for the use of long rest intervals to elicit hypertrophic effects. However, due to the paucity of studies with similar designs, further research is needed to provide a clear differentiation between these two approaches.",
        "prediction": "The impact of short and long rest times between sets in weight training on muscle growth has been studied, but the results are mixed and not very clear. Evidence shows that both short and long rest intervals can help increase muscle size. New research with trained individuals suggests that longer rest periods might be better for muscle growth. However, there are not enough studies with similar methods, so more research is needed to clearly distinguish between the two types of rest intervals.",
        "reference": "The effects of short versus long rest intervals between sets in strength training on muscle growth have been investigated in several studies but the findings and the practical implications are unclear. Current evidence indicates that both short and long rest intervals may be useful for muscle growth. Novel findings suggest for trained people a possible advantage for the use of long rest intervals to induce muscle growth. However, due to the lack of studies with similar methodologies, further research is needed to provide a clear differentiation between these two approaches.",
        "metrics": {
          "SARI": 34.71906630786908,
          "BLEU": 0.2151941196390645,
          "FKGL": 8.9
        }
      },
      {
        "source": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review describes a cellular memory in skeletal muscle in which hypertrophy is 'remembered' such that a fibre that has previously been large, but subsequently lost its mass, can regain mass faster than naive fibres. A new cell biological model based on the literature, with the most reliable methods for identifying myonuclei, can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells before hypertrophic growth. Even if subsequently subjected to grave atrophy, the higher number of myonuclei is retained, and the myonuclei seem to be protected against the elevated apoptotic activity observed in atrophying muscle tissue. Fibres that have acquired a higher number of myonuclei grow faster when subjected to overload exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit in the elderly, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "prediction": "Memory involves encoding, storing, and retrieving information. While it is generally thought to happen only in the brain for vertebrates, this review highlights that skeletal muscle also has a form of memory. When muscle fibers grow larger, they can remember that growth and regain size more quickly after losing mass compared to fibers that have never been enlarged. A new model explains this by showing that untrained muscle fibers can gather myonuclei from satellite cells before they grow. Even if they lose mass later, they keep more myonuclei, which seem to be protected from cell death during muscle wasting. Fibers with more myonuclei grow faster with stress or exercise, indicating that these nuclei are important for storing a \"memory\" of past strength. This memory may last a long time in humans, with myonuclei remaining stable for at least 15 years and possibly even longer. However, older individuals may find it harder to gain myonuclei. If this long-lasting muscle memory does exist in people, starting strength training early in life could be beneficial for public health.",
        "reference": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, it has been said that it occurs only in the brain. This review describes a cellular memory in muscles in which muscle growth is 'remembered' such that a fibre that has lost its mass, can regain it faster than untrained fibres. A new model, with the most reliable methods for identifying myonuclei (nuclei in muscle fiber), can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells (quiescent cells acting as a reserve population of cells for regeneration of muscle) before muscle growth. Even after grave muscle loss, the higher number of myonuclei is retained, and the myonuclei seem to be protected against programmed cell death observed during muscle loss. Fibres that have acquired a higher number of myonuclei grow faster during strength exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit for old people, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "metrics": {
          "SARI": 23.256496484961716,
          "BLEU": 0.1200239453028856,
          "FKGL": 10.9
        }
      },
      {
        "source": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several observational studies lend support to this hypothesis, with an inverse relationship noted between the frequency of eating and adiposity. The purpose of this narrative review is to present and discuss a meta-analysis with regression that evaluated experimental research on meal frequency with respect to changes in fat mass and lean mass. Feeding frequency was positively associated with reductions in fat mass and body fat percentage as well as an increase in fat-free mass. However, sensitivity analysis of the data showed that the positive findings were the product of a single study, casting doubt as to whether more frequent meals confer beneficial effects on body composition. In conclusion, although the initial results of this meta-analysis suggest a potential benefit of increased feeding frequencies for enhancing body composition, these findings need to be interpreted with circumspection.",
        "prediction": "Some studies suggest that eating small meals more often can help with fat loss and weight management. These studies often show that people who eat more frequently tend to have less body fat. This review looks at a detailed analysis of research on meal frequency and its effects on body fat and muscle mass. The analysis found that eating more often was linked to lower fat mass and body fat percentage, while also increasing muscle mass. However, further analysis indicated that these positive results mainly came from one study, raising questions about whether eating more meals really improves body composition. In summary, while initial results hint at benefits from eating more frequently, we should be cautious in interpreting these findings.",
        "reference": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps for better weight maintenance. Several studies support this hypothesis, with a link between the frequency of eating and being overweight. This review aims to present and discuss a research on meal frequency with respect to changes in body mass. Eating frequency was positively associated with reductions in fat mass and increasing in fat-free mass. However, the positive findings were the product of a single study, leaving a doubt about the beneficial effect of frequent meals on body composition. In conclusion, although the initial results suggest a potential benefit of increased eating frequencies for enhancing body composition, these findings need to be carefully interpreted.",
        "metrics": {
          "SARI": 28.431781230069525,
          "BLEU": 0.10893569650320167,
          "FKGL": 11.1
        }
      },
      {
        "source": "Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by resting metabolic rate reduction and fat accumulation. Ten weeks of resistance training may increase lean weight by 1.4 kg, increase resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of resistance training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Resistance training may assist prevention and management of type 2 diabetes by decreasing visceral fat, reducing HbA1c, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Resistance training may enhance cardiovascular health, by reducing resting blood pressure, decreasing low-density lipoprotein cholesterol and triglycerides, and increasing high-density lipoprotein cholesterol. Resistance training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Resistance training may be effective for reducing low back pain and easing discomfort associated with arthritis and fibromyalgia and has been shown to reverse specific aging factors in skeletal muscle.",
        "prediction": "Inactive adults lose 3% to 8% of their muscle mass every decade, which also leads to lower resting metabolic rates and more fat. After ten weeks of resistance training, people can gain 1.4 kg of lean weight, raise their resting metabolic rate by 7%, and lose 1.8 kg of fat. Resistance training helps with better physical performance, movement control, walking speed, independence, thinking skills, and self-confidence. It can also help prevent and manage type 2 diabetes by decreasing belly fat, lowering HbA1c levels, increasing the glucose transporter type 4 density, and enhancing insulin sensitivity. Additionally, resistance training can improve heart health by lowering resting blood pressure, reducing bad cholesterol and triglycerides, and raising good cholesterol levels. It may also help build stronger bones, with some studies showing a 1% to 3% increase in bone density. Resistance training is also effective in reducing lower back pain and relieving discomfort from arthritis and fibromyalgia, and it can help reverse some aging effects in muscles.",
        "reference": "Per decade, inactive adults experience a muscle loss of 3% to 8%, a reduction of resting metabolic rate (energy consumed by the body at rest) and fat accumulation. Ten weeks of strength training may increase fat-free mass by 1.4 kg, resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of strength training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Strength training may assist prevention and management of type 2 diabetes by decreasing abdominal fat, reducing HbA1c which is a form of hemoglobin linked to a sugar, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Strength training may enhance cardiovascular health, by reducing resting blood pressure, decreasing 'bad' cholesterol and triglycerides (main constituents of body fat), and increasing 'good' cholesterol. Strength training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Strength training may be effective for reducing low back pain, joint and muscle pain, and reverse specific aging factors in muscles.",
        "metrics": {
          "SARI": 31.816428462048947,
          "BLEU": 0.23471437776155302,
          "FKGL": 10.3
        }
      },
      {
        "source": "We hypothesized that compared to post-exercise protein intake, co-ingestion of alcohol would down-regulate translation initiation signaling and decrease rates of MPS.",
        "prediction": "We believed that drinking alcohol along with protein right after exercise would lower the signaling that starts protein building and reduce the amount of muscle protein synthesis.",
        "reference": "We assumed that consumption of protein and alcohol (compared to protein only) would reduce protein synthesis.",
        "metrics": {
          "SARI": 57.51336586709904,
          "BLEU": 0,
          "FKGL": 15
        }
      },
      {
        "source": "Eight healthy physically active male subjects (age 21.4±4.8 yr, body mass (BM) 79.3±11.9 kg, peak oxygen uptake (VO2peak) 48.1±4.8 mL·kg−1·min−1, leg extension one repetition maximum (1RM) 104±20 kg; values are mean ± SD) who had been participating in regular exercise (3 times wk−1 for >6 months) volunteered for this study.",
        "prediction": "Eight healthy active male subjects, average age 21.4 years and weight 79.3 kg, participated in this study. They had a peak oxygen uptake of 48.1 mL·kg−1·min−1 and could lift an average of 104 kg in leg extensions. All had been exercising regularly, three times a week for more than six months.",
        "reference": "Eight healthy physically active men (3 trainings per week for at least 6 months) volunteered for this study.",
        "metrics": {
          "SARI": 37.50121555449424,
          "BLEU": 0,
          "FKGL": 4.2
        }
      },
      {
        "source": "The study employed a randomized counter-balanced, cross-over design in which each subject completed bouts of consecutive resistance, continuous and intermittent high-intensity exercise with either post-exercise ingestion of alcohol-carbohydrate (ALC-CHO), alcohol-protein (ALC-PRO) or protein only (PRO) beverages on three separate occasions.",
        "prediction": "The study used a random and balanced design where each participant did sessions of resistance exercise and high-intensity exercise in two ways: with drinks that had alcohol and carbohydrates, alcohol and protein, or only protein on three different occasions",
        "reference": "Subjects did high intensity exercise and just after consumed an alcohol-carbohydrate, an alcohol-protein or a protein drink in three separate occasions.",
        "metrics": {
          "SARI": 36.156356779839776,
          "BLEU": 0,
          "FKGL": 20.9
        }
      },
      {
        "source": "Resistance exercise consisted of eight sets of five repetitions at ∼80% of 1RM.",
        "prediction": "The resistance exercise included eight sets of five repetitions using about 80% of the maximum weight a person can lift once.",
        "reference": "Training consisted of eight sets of five repetitions of leg extension at 80% of their maximum strength.",
        "metrics": {
          "SARI": 50.32368620647202,
          "BLEU": 0.21800193956058223,
          "FKGL": 11.5
        }
      },
      {
        "source": "After completion of the final set, subjects rested for 5 min before commencing 30 min of continuous cycling at ∼63% PPO (∼70% VO2peak).",
        "prediction": "After finishing the last set, the participants took a 5-minute break before starting 30 minutes of continuous cycling at about 63% of their peak power output or around 70% of their peak oxygen consumption.",
        "reference": "After 5 minutes rest, subjects cycled for 30 minutes at medium intensity,",
        "metrics": {
          "SARI": 36.89054001554002,
          "BLEU": 0,
          "FKGL": 15.4
        }
      },
      {
        "source": "Upon completion, subjects rested on the bike for 2 min before undertaking 10×30 s high intensity intervals at ∼110% of PPO, with 30 s active recovery (∼50% PPO) between each work bout.",
        "prediction": "After finishing, participants rested on the bike for 2 minutes before doing 10 intervals of 30 seconds each at about 110% of their peak performance output, with 30 seconds of active recovery at around 50% of their peak performance output between each interval",
        "reference": "then 5 minutes at high intensity.",
        "metrics": {
          "SARI": 33.19989479045206,
          "BLEU": 0,
          "FKGL": 20.1
        }
      },
      {
        "source": "Immediately following exercise and after 4 h recovery, subjects ingested a 500 mL solution of either protein (PRO, 25 g whey protein powder; ISO8, Musashi, Melbourne, VIC Australia) or an energy-match in the form of CHO (25 g maltodextrin, International Health Investments, Helensvale, QLD Australia).",
        "prediction": "After exercising and resting for 4 hours, participants drank a 500 mL solution containing either 25 g of whey protein or 25 g of maltodextrin, which is a carbohydrate.",
        "reference": "Immediately following exercise and after 4 h recovery, subjects consumed a 500mL drink of protein or carbohydrates.",
        "metrics": {
          "SARI": 32.48630304880305,
          "BLEU": 0,
          "FKGL": 15.8
        }
      },
      {
        "source": "Furthermore, a CHO-based meal (1.5 g·kg−1 BM) was consumed ∼2 h post-exercise, immediately after the muscle biopsy, according to recommendations for post-exercise glycogen recovery [24].",
        "prediction": "A meal with carbohydrates from CHO (1.5 g per kg of body mass) was eaten about 2 hours after exercise, right after taking a muscle biopsy, following guidelines for recovering glycogen after exercise",
        "reference": "A carbohydrate meal was consumed 2 h after exercise, according to recommendation for glycogen recovery.",
        "metrics": {
          "SARI": 44.140036729218664,
          "BLEU": 0,
          "FKGL": 9.7
        }
      },
      {
        "source": "The 8 h time frame represents an important phase of post-exercise recovery [1] as well as the period during which blood alcohol concentrations are likely to be elevated by a post-event drinking binge [14].",
        "prediction": "The 8-hour period is a crucial time for recovery after exercise and when blood alcohol levels may rise due to drinking after an event",
        "reference": "The 8 h period after exercise is an important phase for recovery but also the period during which blood alcohol level may be the highest after excessive consumption of alcohol.",
        "metrics": {
          "SARI": 38.10936694270028,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "The alcohol ingestion protocol (1.5 g·kg−1 BM; 12±2 standard drinks) began 1 h post-exercise and was consumed in 6 equal volumes of 1 part vodka (∼60 mL) to four parts orange juice (∼240 mL, 1.8 g CHO·kg−1 BM) during a 3 h period.",
        "prediction": "The alcohol consumption plan was to take 1.5 grams of alcohol per kilogram of body mass, which is about 12 standard drinks. This started one hour after exercise and was taken in six equal parts, mixing one part vodka with four parts orange juice over three hours.",
        "reference": "The alcohol consumption started 1 h after exercise with 6 drinks (60mL of vodka and 240mL of orange juice) and during 3 h.",
        "metrics": {
          "SARI": 41.78903471333049,
          "BLEU": 0,
          "FKGL": 5.9
        }
      },
      {
        "source": "The focus of the early post-exercise period (i.e., 1–8 h) is to enhance physiological processes that are critical for reversing the exercise-induced disturbances to homeostasis and physiological function and for promoting adaptations to training [1].",
        "prediction": "The early period after exercise, which lasts from 1 to 8 hours, aims to improve body processes that help restore balance and normal function disrupted by exercise. This period is also important for promoting adaptations to training.",
        "reference": "The period after exercise (1-8h) enhances the body's processes that reverse the effects of the exercise and promotes adaptation to the training.",
        "metrics": {
          "SARI": 39.93430976945404,
          "BLEU": 0,
          "FKGL": 11.7
        }
      },
      {
        "source": "For the PRO condition, orange juice was consumed with a matched volume of water in place of the alcohol.",
        "prediction": "In the PRO condition, orange juice was drunk along with an equal amount of water instead of alcohol",
        "reference": "For the protein drink, orange juice was consumed with water instead of alcohol.",
        "metrics": {
          "SARI": 46.959706959706956,
          "BLEU": 0.2680165156355778,
          "FKGL": 8
        }
      },
      {
        "source": "Subjects ingested the beverages within 5 min every 30 min.",
        "prediction": "Participants drank the beverages within 5 minutes, every half hour.",
        "reference": "Subjects consumed the drinks in 5 minutes every 30 minutes.",
        "metrics": {
          "SARI": 36.178804855275445,
          "BLEU": 0,
          "FKGL": 9.6
        }
      },
      {
        "source": "Blood, cell signaling and mRNA data were analyzed by two-way ANOVA (two factor: time × treatment) with repeated measures and myofibrillar protein synthesis was analyzed by one-way ANOVA with repeated measures.",
        "prediction": "Blood cell signaling and mRNA data were studied using two-way ANOVA, which looks at two factors: time and treatment, with repeated measures. Myofibrillar protein synthesis was studied using one-way ANOVA with repeated measures.",
        "reference": "Data (blood, protein synthesis, responses, etc.) were analyzed several times for the 3 different consumptions of drinks.",
        "metrics": {
          "SARI": 33.341998415527826,
          "BLEU": 0,
          "FKGL": 10.9
        }
      },
      {
        "source": "The first novel finding of this study was that mTOR signaling and rates of myofibrillar protein synthesis (MPS) following concurrent resistance, continuous and intermittent high-intensity exercise, designed to mimic the metabolic profile of many team sports, were impaired during the early (8 h) recovery phase by the ingestion of large amounts (1.5 g•kg−1 BM) of alcohol.",
        "prediction": "This study found that drinking a lot of alcohol (1.5 g per kg of body mass) after high-intensity exercise affected the mTOR signaling and the production of myofibrillar protein synthesis during the first 8 hours of recovery.",
        "reference": "The first finding of this study was that the mTOR enzyme (responsible for muscle growth) and protein synthesis (after exercise) were reduced by an excessive alcohol consumption during the 8 h period of recovery.",
        "metrics": {
          "SARI": 40.384653620935794,
          "BLEU": 0,
          "FKGL": 11.7
        }
      },
      {
        "source": "These outcomes were most evident (37% reduction in rates of MPS) when alcohol was consumed in the absence of post-exercise protein intake, as is likely to occur when intoxication reduces the athlete's compliance to sound recovery practices.",
        "prediction": "The results showed a 37% decrease in MPS rates when alcohol was consumed without protein intake after exercise. This situation likely happens when being intoxicated makes athletes less likely to follow good recovery habits.",
        "reference": "Consequences were most evident when alcohol was consumed without protein (37 % reduction of protein synthesis).",
        "metrics": {
          "SARI": 51.56112891730192,
          "BLEU": 0.14219389639501667,
          "FKGL": 12.3
        }
      },
      {
        "source": "However, a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate MPS [8] during post-exercise recovery, the intake of alcohol reduced MPS by ∼24%, representing only a partial ‘rescue’ of the anabolic response compared with protein alone.",
        "prediction": "A second finding showed that even when people consumed the right amount of protein after exercise to help muscle repair, drinking alcohol still lowered this muscle repair response by about 24% compared to just having protein alone.",
        "reference": "A second finding was that even with protein consumption, alcohol reduced protein synthesis by around 24%.",
        "metrics": {
          "SARI": 43.44475532805641,
          "BLEU": 0,
          "FKGL": 16.5
        }
      },
      {
        "source": "The mechanistic target of rapamycin complex 1 (mTORC1) is a central node for integrating nutrient (i.e. amino acid) and exercise/contraction signal transduction [31], [32].",
        "prediction": "The mTORC1 complex plays a key role in combining signals from nutrients like amino acids and from exercise.",
        "reference": "mTOR is important in absorption of nutrients (like amino acids) and in sending signals to the body to produce an effort.",
        "metrics": {
          "SARI": 57.558479532163744,
          "BLEU": 0,
          "FKGL": 9.1
        }
      },
      {
        "source": "In conclusion, the current data provide the novel observation that alcohol impairs the response of MPS in exercise recovery in human skeletal muscle despite optimal nutrient provision.",
        "prediction": "In conclusion, new data show that alcohol weakens the muscle recovery process after exercise in humans, even when nutrients are provided adequately.",
        "reference": "The data from this study show that alcohol reduce the protein synthesis after exercise even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 53.14733352690669,
          "BLEU": 0,
          "FKGL": 15.4
        }
      },
      {
        "source": "The quantity of alcohol consumed in the current study was based on amounts reported during binge drinking by athletes.",
        "prediction": "The amount of alcohol used in this study was based on what athletes said they drank during binge drinking.",
        "reference": "The quantity of alcohol consumed in this study was based on consumption by athletes during binge drinking.",
        "metrics": {
          "SARI": 51.6871447708095,
          "BLEU": 0.40157733283424196,
          "FKGL": 7.2
        }
      },
      {
        "source": "However, published reports suggest intakes of some individuals can be significantly greater [9], [50], which is of concern for many reasons related to health and safety [13].",
        "prediction": "Some studies show that certain people may consume much more than others which raises concerns about health and safety",
        "reference": "However, reports show that some individuals can consume higher quantities, which is a concern for health and safety issues.",
        "metrics": {
          "SARI": 51.3947618754921,
          "BLEU": 0,
          "FKGL": 8.3
        }
      },
      {
        "source": "Recommended nutritional strategies to maximize recovery in skeletal muscle include protein for enhancing rates of protein synthesis and carbohydrate for replenishing glycogen stores [2],[3].",
        "prediction": "To help muscles recover better, it is suggested to consume protein to boost protein production and carbohydrates to restore glycogen levels",
        "reference": "Nutritional strategies to optimize muscle recovery include proteins which are recommended to allow protein synthesis (production of new proteins which improve, for example, muscle mass) and carbohydrates to recharge glycogen stores (energy storage).",
        "metrics": {
          "SARI": 42.20478944212269,
          "BLEU": 0,
          "FKGL": 13.8
        }
      },
      {
        "source": "Regrettably, there has been difficulty in finding an educational message with alcohol consumption related to sports performance that has resonance with athletes.",
        "prediction": "It has been challenging to communicate a clear message about how alcohol use affects sports performance that resonates with athletes",
        "reference": "Regrettably, it is difficult to find an educational message about alcohol consumption and sport performances that speaks to athletes.",
        "metrics": {
          "SARI": 41.78362573099415,
          "BLEU": 0,
          "FKGL": 12.3
        }
      },
      {
        "source": "Given the need to promote protein synthesis that underpins adaptation, repair and regeneration of skeletal muscle the results of the current study provide clear evidence of impaired recovery when alcohol is consumed after concurrent (resistance, continuous and intermittent high-intensity) exercise even in the presence of optimal nutritional conditions.",
        "prediction": "This study shows that drinking alcohol after doing high-intensity exercise can hinder muscle recovery. This is true even when nutrition is optimal.",
        "reference": "Considering the importance of protein synthesis (adaptation, repair and regeneration of muscles), the results of this study provide evidence of reduced recovery when alcohol is consumed after exercise, even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 30.016648982192756,
          "BLEU": 0,
          "FKGL": 11.1
        }
      },
      {
        "source": "We propose our data is of paramount interest to athletes and coaches.",
        "prediction": "We believe our data is very important for athletes and coaches",
        "reference": "We consider our data crucial for athletes and coaches.",
        "metrics": {
          "SARI": 53.07167369667369,
          "BLEU": 0.28997844147152074,
          "FKGL": 6.4
        }
      },
      {
        "source": "Our findings provide an evidence-base for a message of moderation in alcohol intake to promote recovery after exercise with the potential to alter current sports culture and athlete practices.",
        "prediction": "Our findings suggest that drinking alcohol in moderation can help recovery after exercise and may change sports culture and how athletes behave.",
        "reference": "Our findings provide evidence for a message of moderation in alcohol consumption to promote recovery after exercise, with the potential to change sport cultures and athlete practices.",
        "metrics": {
          "SARI": 25.015952993361346,
          "BLEU": 0,
          "FKGL": 13.1
        }
      },
      {
        "source": "Muscle contraction and the intake of leucine-rich protein sources activate independent but complimentary signaling responses that converge at the mechanistic target of rapamycin (mTOR) to stimulate protein translation enhancing rates of muscle protein synthesis [4]–[6].",
        "prediction": "Muscle contraction and eating protein sources high in leucine trigger different but supportive signaling responses that come together at a point called mTOR. This activation helps increase muscle protein synthesis.",
        "reference": "Muscle contraction and consumption of leucine-rich protein (essential amino acid composing proteins) induce reactions which increase the protein synthesis.",
        "metrics": {
          "SARI": 52.326899259622195,
          "BLEU": 0,
          "FKGL": 11.5
        }
      },
      {
        "source": "The ingestion of ∼20–25 g of high quality protein soon after exercise [7], repeated every 4 h [8] has been shown to maximise the anabolic response in skeletal muscle.",
        "prediction": "Eating about 20 to 25 grams of high-quality protein shortly after exercise, and then every 4 hours, helps to get the best muscle-building results.",
        "reference": "Consumption of 20-25 g of high quality protein after exercise and repeated every 4 h showed an optimal anabolic response (increasing muscle mass) of the muscles.",
        "metrics": {
          "SARI": 35.59764091919889,
          "BLEU": 0,
          "FKGL": 12.7
        }
      },
      {
        "source": "The cultural environment surrounding some sports often involves the intake of large amounts of alcohol after training and competition, with athletes in several team sports being particularly at risk of “binge drinking” practices [9]–[11].",
        "prediction": "The cultural environment of some sports often includes drinking a lot of alcohol after training and competitions, and athletes in team sports are especially at risk for binge drinking.",
        "reference": "Several studies showed that athletes are more likely to drink alcohol to excess (50-65% drink above dangerous level).",
        "metrics": {
          "SARI": 35.80667789001122,
          "BLEU": 0,
          "FKGL": 15.8
        }
      },
      {
        "source": "The outcomes of binge drinking after exercise are likely to include the direct effect of alcohol on physiological processes as well as the indirect effect on the athlete's recovery due to not eating or resting adequately as a result of intoxication.",
        "prediction": "Binge drinking after exercise can affect the body directly due to alcohol's impact on physiological functions. It can also hinder an athlete's recovery because they may not eat or rest properly when intoxicated.",
        "reference": "Consequences of an excessive consumption after exercise are direct effect on the body but also indirect effect on the recovery due to an inappropriate eating and rest.",
        "metrics": {
          "SARI": 36.98491609694116,
          "BLEU": 0,
          "FKGL": 12.1
        }
      },
      {
        "source": "Although the concurrent consumption of carbohydrate can partially offset the deleterious effects of alcohol intake on post-exercise glycogen resynthesis [14], the effect of alcohol consumption on muscle protein synthesis is unknown.",
        "prediction": "Drinking carbohydrates while consuming alcohol may help reduce some negative effects on glycogen replenishment after exercise, but it is unclear how alcohol affects muscle protein building.",
        "reference": "Consumption of carbohydrates can partially counter balance the effects of alcohol on the recharge of glycogen stores but the effect on muscle protein synthesis are unknown.",
        "metrics": {
          "SARI": 33.74973749803736,
          "BLEU": 0,
          "FKGL": 18.2
        }
      },
      {
        "source": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis (MPS) in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.",
        "prediction": "This study aimed to find out how alcohol consumption affects cell signaling and the production of myofibrillar protein in humans after intense exercise similar to what athletes face in training and competition for team sports like football, rugby, and court sports",
        "reference": "The aim of this study was to determine the effect of alcohol on anabolism and protein synthesis in humans during recovery following an effort an athlete could do (in rugby or football for example).",
        "metrics": {
          "SARI": 31.017722731698015,
          "BLEU": 0,
          "FKGL": 20.5
        }
      },
      {
        "source": "Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the age-associated loss of muscle mass and is one of the main contributors to musculoskeletal impairments in the elderly. Previous research has demonstrated that resistance training can attenuate skeletal muscle function deficits in older adults, however few articles have focused on the effects of resistance training on functional mobility. The purpose of this systematic review was to 1) present the current state of literature regarding the effects of resistance training on functional mobility outcomes for older adults with skeletal muscle function deficits and 2) provide clinicians with practical guidelines that can be used with seniors during resistance training, or to encourage exercise. We set forth evidence that resistance training can attenuate age-related changes in functional mobility, including improvements in gait speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in progressive resistance training activities, and should be admonished to move along a continuum of exercise from immobility, toward the recommended daily amounts of activity.",
        "prediction": "As people age, their skeletal muscle changes in various ways. Sarcopenia, which is the loss of muscle mass due to aging, is a major cause of movement problems in older adults. Research has shown that resistance training can help improve muscle function in these individuals, but there are few studies that look specifically at how it affects their ability to move. This review aims to share what is currently known about the impact of resistance training on the movement abilities of older adults with muscle function issues and to provide useful guidelines for doctors working with seniors during these training sessions. We found evidence that resistance training can help counteract age-related declines in mobility, leading to better walking speed, balance, and reduced risk of falls. It is important for older adults to engage in progressive resistance training and to gradually increase their physical activity levels from inactivity to recommended amounts of exercise.",
        "reference": "Human aging results in a variety of changes to muscles. Sarcopenia is the age-associated muscle loss and is one of the main contributors to muscle disorders in older adults. Previous research has demonstrated that strength training can attenuate muscle function deficits in older adults, however few articles have focused on the effects of strength training on mobility. The purpose of this review was to present the effects of strength training on mobility for older adults with muscle function deficits and to provide guidelines that can be used with seniors during strength training. We present evidence that strength training can attenuate age-related changes in mobility, including improvements in walking speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in strength training activities, and should avoid immobility.",
        "metrics": {
          "SARI": 28.651082033451818,
          "BLEU": 0.10174026825599836,
          "FKGL": 14.3
        }
      },
      {
        "source": "Maximizing the hypertrophic response to resistance training (RT) is thought to be best achieved by proper manipulation of exercise program variables including exercise selection, exercise order, length of rest intervals, intensity of maximal load, and training volume. An often overlooked variable that also may impact muscle growth is repetition duration. Duration amounts to the sum total of the concentric, eccentric, and isometric components of a repetition, and is predicated on the tempo at which the repetition is performed. We conducted a systematic review and meta-analysis to determine whether alterations in repetition duration can amplify the hypertrophic response to RT. Results indicate that hypertrophic outcomes are similar when training with repetition durations ranging from 0.5 to 8 s. From a practical standpoint it would seem that a fairly wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at volitionally very slow durations (>10s per repetition) is inferior from a hypertrophy standpoint, although a lack of controlled studies on the topic makes it difficult to draw definitive conclusions.",
        "prediction": "Maximizing muscle growth from resistance training is often linked to how we plan workouts, including what exercises we choose, the order we do them in, how long we rest, how heavy we lift, and the overall volume of training. One factor that is often missed is how long each repetition takes. This duration includes the time spent on the concentric (lifting), eccentric (lowering), and isometric (holding) parts of each repetition, and depends on the speed of the movement. We did a review and analysis to see if changing repetition duration can enhance muscle growth. The results show that muscle growth is similar for repetition durations between 0.5 to 8 seconds. This means you can use a wide range of repetition durations to maximize muscle growth. However, training with very slow repetitions (over 10 seconds each) seems less effective for muscle growth, but there aren't many controlled studies to confirm this.",
        "reference": "Maximizing the muscle growth following strength training can be done by manipulating variables like exercise selection, exercise order, rest intervals, intensity of maximal load, and training volume (number of exercises, sets, etc.). A neglected variable that also may impact muscle growth is repetition duration. Total duration of a repetition is the sum of the contraction, the lengthening and isometric (static contraction) and is based on the tempo. We conducted a review and analysis of multiple studies to determine if modification of repetition duration can amplify the muscle growth. Results indicate that muscle growth is similar when training with repetition durations from 0.5 to 8 s. Using a wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at very slow durations (>10s per repetition) is less efficient for muscle growth, although a lack of studies on the topic makes it difficult to draw definitive conclusions.",
        "metrics": {
          "SARI": 32.29286742350605,
          "BLEU": 0.15619792789793044,
          "FKGL": 10.6
        }
      },
      {
        "source": "Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also refers to specific physiological processes or states occurring after exercise that are distinct from the physiology of either the exercising or the resting states. In this context, recovery of the cardiovascular system after exercise occurs across a period of minutes to hours, during which many characteristics of the system, even how it is controlled, change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. Furthermore, some of these changes may provide insight into when the cardiovascular system has recovered from prior training and is physiologically ready for additional training stress. This review focuses on the most consistently observed hemodynamic adjustments and the underlying causes that drive cardiovascular recovery and will highlight how they differ following resistance and aerobic exercise. Primary emphasis will be placed on the hypotensive effect of aerobic and resistance exercise and associated mechanisms that have clinical relevance, but if left unchecked, can progress to symptomatic hypotension and syncope. Finally, we focus on the practical application of this information to strategies to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities of this state. We will explore appropriate field measures, and discuss to what extent these can guide an athlete's training.",
        "prediction": "Recovery from exercise is the time it takes for the body to return to a resting state after physical activity. This process involves specific changes in the body's systems that are different from the exercise and rest phases. The cardiovascular system, which includes the heart and blood vessels, undergoes changes over minutes to hours post-exercise. Some of these changes are important for long-term adaptation to training, while others can lead to instability. Understanding these changes can help determine when the cardiovascular system is ready for more training. This review looks at common blood flow changes during recovery and their causes, emphasizing differences between resistance and aerobic exercises. It will highlight the lowering of blood pressure after these exercises and the related mechanisms, which can have significant implications if not managed properly. Lastly, we will discuss practical strategies for improving cardiovascular recovery and guiding athletes in their training approaches.",
        "reference": "Recovery from exercise refers to the time period between the end of exercise and the return to a resting or recovered state. It also refers to specific physiological processes occurring after exercise. Recovery of the cardiovascular system occurs across a period of minutes to hours, during which many characteristics of the system change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. These changes may provide insight into when the cardiovascular system has recovered and is physiologically ready for additional training. This review focuses on the most consistently observed blood flow adjustments and the causes that drive cardiovascular recovery and how they differ following strength and cardio exercise. First we focus on the effect leading to low blood pressure of cardio and strength exercise and associated mechanisms, which can progress to symptomatic hypotension (low blood pressure) and fainting. Finally, we focus on the practical application to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities. We will explore measures, and discuss how these can guide an athlete's training.",
        "metrics": {
          "SARI": 28.159822033464017,
          "BLEU": 0.12538531234872483,
          "FKGL": 12.9
        }
      },
      {
        "source": "Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed \"exercise as medicine\". Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often overlooked as a target for exercise-based therapy. Despite the vast range of severity in kidney disease (e.g., pre-dialysis, dialysis, transplant), exercise has a potential role in all patients suffering from the condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'.",
        "prediction": "Exercise and physical activity are becoming important in treating and preventing various medical conditions like arthritis and diabetes, a concept known as \"exercise as medicine.\" Exercise helps lower risks for heart disease, reduce inflammation, combat muscle wasting, and manage high blood pressure. It also improves physical ability, strength, and heart and lung function. Chronic kidney disease affects about 10% of people and is often overlooked for exercise therapy. Regardless of how severe the kidney disease is, exercise can benefit all patients. This review discusses the significant role of exercise in managing kidney disease and how it can be effectively prescribed.",
        "reference": "Exercise and physical activity are key tools in the treatment and prevention of several medical conditions including joint disorders and diabetes. Exercise can reduce cardiovascular risk, inflammation, illness causing muscle loss, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often neglected as a target for exercise-based therapy. Despite the vast range of severity in kidney disease, exercise has a potential role in all patients suffering from the condition. In this review, we summarise the potential important role of exercise in the management of kidney disease and how this form of 'medicine' should be used.",
        "metrics": {
          "SARI": 26.982260591964387,
          "BLEU": 0.1518831999718744,
          "FKGL": 12.2
        }
      },
      {
        "source": "Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The pooled standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% CI=-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.",
        "prediction": "Blood flow restriction (BFR) is a treatment that helps healthy people get stronger. Its impact on pain and function in people with knee pain is not well known. This study aimed to see if adding BFR to strength training helps relieve pain and improve function in patients with knee pain. We looked at multiple studies that compared strength exercises with and without BFR for knee pain in adults over 18. The analysis showed that strength exercises with BFR did not reduce pain more than those without BFR and did not improve knee function significantly either.",
        "reference": "Exercise with blood flow restriction (BFR) is used to increase strength in healthy individuals. However, its effects on individuals with knee pain are unknown. The objective is to determine the effectiveness of adding BFR to strength exercise for pain relief and improvement of function in patients with knee pain. We conducted a review and analysis of multiple studies. Studies that compared strength exercise with or without BFR to treat knee pain and function. The results showed that resistance exercises with BFR was not more effective than strength exercises for reducing pain and improving knee function in patients with knee pain.",
        "metrics": {
          "SARI": 39.751298151210015,
          "BLEU": 0.2173515534218095,
          "FKGL": 8.3
        }
      },
      {
        "source": "In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports). In this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review recommends safe and effective whole-body aerobic exercise at moderate intensity (i.e., 50-65% of maximum oxygen intake, 30-60 min per session, 3-4 times a week) that primarily focuses on the major muscle groups for the prevention and treatment of hypertension. Resistance exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but resistance exercise is contraindicated in patients with hypertension who have chest symptoms such as chest pain.",
        "prediction": "In Japan, about 43 million people had high blood pressure in 2010. Managing this condition is very important, and making lifestyle changes is recognized as key to preventing and treating high blood pressure. There is a focus on increasing daily activities and exercising. This review looks at suitable exercise plans for preventing and treating high blood pressure based on Japanese and foreign studies. It recommends safe and effective whole-body aerobic exercise at moderate intensity, which means exercising 50-65% of your maximum oxygen use for 30-60 minutes, 3-4 times a week, targeting major muscle groups. Resistance exercise should be done at low intensity without holding your breath and is recommended as an additional exercise. However, resistance exercise is not suitable for patients with high blood pressure who experience chest symptoms like pain.",
        "reference": "In Japan, 43 million patients had high blood pressure in 2010. The management of this condition and the importance of lifestyle changes for the prevention and treatment of high blood pressure has been recognized in Japan. In particular, increasing the levels of activities of daily living and physical exercise. In this review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of high blood pressure. This review recommends full body cardio exercise at moderate intensity (i.e., 50-65% of maximum intensity, 30-60 min per session, 3-4 times a week) that focuses on the major muscle groups. Strength exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but it is contraindicated in patients with high blood pressure who have chest symptoms such as chest pain.",
        "metrics": {
          "SARI": 27.812728368707667,
          "BLEU": 0.2182976958953707,
          "FKGL": 11.8
        }
      },
      {
        "source": "Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common sense advices call for late inception of intense, strength training-related activities, like weight lifting and plyometrics, which are usually postponed at the end of the growth age, even among sport practitioners. However, such advices seem to have a mainly anecdotal nature. Current literature does not seem to have any particular aversion against the practice of strength training by children and adolescents, provided that some safety rules are followed, like medical clearance, proper instruction from a qualified professional and progressive overload. At the same time, several studies provide consistent findings supporting the benefits of repeated, intense physical efforts in young subjects. Improved motor skills and body composition, in terms of increased fat free mass, reduced fat mass and enhanced bone health, have been extensively documented, especially if sport practice began early, when the subjects were pubescent. It can be therefore concluded that strength training is a relatively safe and healthy practice for children and adolescents.",
        "prediction": "Physical activity is a proven way to prevent diseases and improve health. People often suggest delaying intense activities like weight lifting until after growth periods, especially for young athletes. However, this advice seems mostly based on personal experiences. Current research does not oppose strength training for children and teenagers, as long as safety guidelines are followed, such as getting medical approval, receiving proper training from a qualified instructor, and gradually increasing the intensity. Many studies show that young people can benefit from regular intense exercise. These benefits include better motor skills, healthier body composition with more lean mass and less fat, and improved bone health, especially if they start practicing sports during puberty. Thus, strength training is considered a safe and beneficial activity for children and adolescents.",
        "reference": "Physical activity has many benefits like preventing diseases and improving general health. In popular opinion, it is recommended to start intense physical activities like weight lifting and plyometrics (exercises like sprint and jumps) after adolescence. But this advice does not seem to be based on facts. We did not find any recommendation against strength training at a young age if it is well supervised with safety rules, medical clearance, proper instruction from a professional and progressive overload (gradual increase of stress on muscle and nervous system). Several studies showed many benefits of repeated, intense physical efforts in young people. Improved motor skills and body composition (muscle mass increased, fat mass decreased, healthier bone) were found in studies, especially if sport practice began during puberty. We can conclude that strength training is a safe and healthy practice for children and adolescents.",
        "metrics": {
          "SARI": 40.325818711559265,
          "BLEU": 0.09754313085440197,
          "FKGL": 12.7
        }
      },
      {
        "source": "recently, there has been a renewed public interest in IFast. Given the importance of nutrition in optimizing athletic performance, there is a concern about the effects of IFast on athletics. Looking at high-intensity, endurance, and resistance exercises, studies have been varied but are uniform in showing that there is no benefit to athletic performance while fasting.",
        "prediction": "There has been a recent rise in public interest in intermittent fasting (IFast). Since nutrition plays a key role in improving athletic performance, some people are worried about how IFast affects athletes. Research on high-intensity, endurance, and resistance exercises shows mixed results, but all studies agree that fasting does not improve athletic performance.",
        "reference": "Nowadays, people are interested in intermittent fasting (an eating pattern where you cycle between periods of eating and fasting) Nutrition is important for the optimization of sport performance so there is a concern about the effects of intermittent fasting on performance. Studies showed that there are no benefit in high-intensity, endurance or strength training while fasting.",
        "metrics": {
          "SARI": 36.15430892705948,
          "BLEU": 0.07664420031307786,
          "FKGL": 11.4
        }
      },
      {
        "source": "Nearly every physically active person encounters periods in which the time available for exercise is limited (e.g., personal, family, or business conflicts). During such periods, the goal of physical training may be to simply maintain (rather than improve) physical performance. Similarly, certain special populations may desire to maintain performance for prolonged periods, namely athletes (during the competitive season and off-season) and military personnel (during deployment). In general populations, endurance performance can be maintained for up to 15 weeks when training frequency is reduced to as little as 2 sessions per week or when exercise volume is reduced by 33–66% (as low as 13–26 minutes per session), as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size (at least in younger populations) can be maintained for up to 32 weeks with as little as 1 session of strength training per week and 1 set per exercise, as long as exercise intensity (relative load) is maintained; whereas, in older populations, maintaining muscle size may require up to 2 sessions per week and 2–3 sets per exercise, while maintaining exercise intensity. Our primary conclusion is that exercise intensity seems to be the key variable for maintaining physical performance over time, despite relatively large reductions in exercise frequency and volume.",
        "prediction": "Many physically active people find it hard to exercise due to personal or work issues. During these times, the aim is to maintain, not improve, physical performance. Some groups, like athletes and military personnel, often need to keep their performance steady for long periods. Research shows that general endurance can be preserved for up to 15 weeks by cutting training to just 2 sessions a week or reducing workout volume by 33–66% as long as the exercise intensity stays high. For younger people, strength and muscle size can be maintained for up to 32 weeks with just 1 weekly strength training session of 1 set per exercise if the intensity is consistent. In older adults, maintaining muscle size usually needs 2 sessions a week and 2–3 sets per exercise, at the right intensity. The main takeaway is that keeping exercise intensity high is crucial for sustaining physical performance, even with fewer workouts.",
        "reference": "Physically active people encounter periods with limited time available for exercise. During such periods, the goal of training may be to simply maintain physical performance. Some people may also desire to maintain performance for prolonged periods, like athletes during competitive season or militaries during deployment. Endurance performance can be maintained for up to 15 weeks with 2 trainings per week, or with a reduction of 13–26 minutes per training, as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size can be maintained for up to 32 weeks with 1 strength training per week and 1 set per exercise, as long as exercise intensity (load) is maintained. In older populations, maintaining muscle size may require up to 2 trainings per week and 2–3 sets per exercise, while maintaining exercise intensity. Exercise intensity seems to be the key variable for maintaining physical performance, even with reduction of frequency and volume (number of exercises, sets, etc.)",
        "metrics": {
          "SARI": 39.44180334453013,
          "BLEU": 0.2316402409710262,
          "FKGL": 11.8
        }
      },
      {
        "source": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients--primarily protein and carbohydrate--in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been postulated that the timing of nutritional consumption may be more important than the absolute daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. Theoretically, consuming the proper ratio of nutrients during this time not only initiates the rebuilding of damaged muscle tissue and restoration of energy reserves, but it does so in a supercompensated fashion that enhances both body composition and exercise performance. Several researchers have made reference to an anabolic “window of opportunity” whereby a limited time exists after training to optimize training-related muscular adaptations. However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Not only is nutrient timing research open to question in terms of applicability, but recent evidence has directly challenged the classical view of the relevance of post-exercise nutritional intake with respect to anabolism.",
        "prediction": "Nutrient timing is a popular strategy that involves eating certain nutrients, mainly protein and carbohydrates, around workout times. Some people believe this can significantly improve body composition. It is suggested that when you eat matters more than how much you eat overall. The time after exercise is seen as the most important for nutrient timing. Eating the right mix of nutrients during this time can help repair muscles and restore energy, potentially leading to better body composition and performance. Some researchers talk about an \"anabolic window of opportunity,\" a limited time after training to make the most of muscle growth. However, how important this post-exercise window is can depend on different factors. Recent evidence also questions whether post-exercise nutrition is as crucial for muscle growth as previously thought.",
        "reference": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients around an exercise session. Some people say that this approach can produce dramatic improvements in body composition. It has even been hypothesized that the timing may be more important than the total daily intake of nutrients. The post-exercise period is considered the most important part of nutrient timing. In theory, consuming the proper ratio of nutrients during this time allows the rebuilding of damaged muscle tissue and restoration of energy reserves, but also enhances both body composition and exercise performance. Researchers have made reference to an anabolic “window of opportunity” (a limited time after training to optimize muscular adaptations). However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Research is uncertain about the applicability of nutrient timing. Besides, the relevance of post-exercise nutritional intake concerning anabolism (increasing muscle mass) has been challenged by recent evidence.",
        "metrics": {
          "SARI": 27.873163556708256,
          "BLEU": 0.1330386927042638,
          "FKGL": 11.9
        }
      },
      {
        "source": "Lack of time is among the more commonly reported barriers for abstention from exercise programs. The aim of this review was to determine how strength training can be most effectively carried out in a time-efficient manner by critically evaluating research on acute training variables, advanced training techniques, and the need for warm-up and stretching. When programming strength training for optimum time-efficiency we recommend prioritizing bilateral, multi-joint exercises that include full dynamic movements (i.e. both eccentric and concentric muscle actions), and to perform a minimum of one leg pressing exercise (e.g. squats), one upper-body pulling exercise (e.g. pull-up) and one upper-body pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights based on training goals, availability, and personal preferences. Weekly training volume is more important than training frequency and we recommend performing a minimum of 4 weekly sets per muscle group using a 6–15 RM loading range (15–40 repetitions can be used if training is performed to volitional failure). Advanced training techniques, such as supersets, drop sets and rest-pause training roughly halves training time compared to traditional training, while maintaining training volume. However, these methods are probably better at inducing hypertrophy than muscular strength, and more research is needed on longitudinal training effects. Finally, we advise restricting the warm-up to exercise-specific warm-ups, and only prioritize stretching if the goal of training is to increase flexibility.",
        "prediction": "Finding time is often seen as a major reason people skip exercise. This review looks at how strength training can be done efficiently. We recommend focusing on exercises that use both sides of the body and involve multiple joints, like squats for legs, pull-ups for the upper body, and bench presses for pushing. You can use machines or free weights, depending on what you prefer. It’s more important to do enough sets each week than to train often. Aim for at least 4 sets per muscle group, using weights that allow for 6 to 15 repetitions. Advanced techniques like supersets and drop sets can cut training time in half while keeping the same volume, but they may be better for muscle growth than building strength, so more studies are needed. Lastly, keep warm-ups specific to the exercises and only stretch if you want to improve flexibility.",
        "reference": "Lack of time is one of the most common reason for not exercising. The aim of this review was to determine how strength training can be time-efficient by evaluating research on training variables, advanced training techniques, and the need for warm-up and stretching. We recommend choosing bilateral (with the two arms or legs), multi-joint (multiple joints involved) exercises that include full dynamic movements and to perform a minimum of one leg pressing exercise (e.g. squats), one pulling exercise (e.g. pull-up) and one pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights. Weekly training volume (number of exercises, sets, etc.) is more important than training frequency and we recommend a minimum of 4 weekly sets per muscle group using a load that permits to do 6 to 15 repetitions. Advanced training techniques like combining exercises with no rest or shortening the rest time while using lighter weights can cuts the training time in half, while maintaining training volume. However, these methods are probably better to increase muscle mass than strength. Finally, we advise exercise-specific warm-ups, and stretching only if the goal of training is to increase flexibility.",
        "metrics": {
          "SARI": 29.81675730719544,
          "BLEU": 0.08939520511369513,
          "FKGL": 8.1
        }
      },
      {
        "source": "Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequency on a volume-equated basis. Meta-regression analysis of non-volume-equated studies showed a significant effect favoring higher frequencies, although the overall difference in magnitude of effect between frequencies of 1 and 3+ days per week was modest. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "prediction": "Training frequency is an important factor in how muscles grow from resistance exercise. This paper reviewed various studies to see how often people should train each week for muscle growth. The results showed no big difference in muscle growth between training more often or less, as long as the total workout volume was the same. When looking at studies that didn't match workout volume, training more often seemed to help a little more, but the difference between training once a week and three or more times was small. In summary, evidence suggests that how often you train doesn't significantly affect muscle growth if the total volume is the same. Therefore, people can choose how often to train each muscle group based on their own preference.",
        "reference": "Training frequency is an important variable for hypertrophy (increasing muscle mass) in strength training. The purpose of this paper was to conduct a review and analysis of multiple studies investigating the effects of weekly training frequency on hypertrophy. Results showed no significant difference between higher and lower frequency with the same training volume (number of exercises, sets, etc.). Analysis of studies with different training volumes showed better results for higher frequencies, although the difference between frequencies of 1 and 3+ days per week was small. In conclusion, there is strong evidence that resistance training frequency does not significantly impact muscle hypertrophy when training volume is the same. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "metrics": {
          "SARI": 25.928081835170484,
          "BLEU": 0.10817869832865634,
          "FKGL": 10.2
        }
      },
      {
        "source": "A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Forced repetitions/drop sets, supersets, and heavy negatives, in particular, have been purported to enhance the hypertrophic response to resistance exercise. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to resistance training programs.",
        "prediction": "Different training methods are suggested to increase muscle growth. Techniques like forced repetitions, drop sets, supersets, and heavy negatives are believed to improve muscle development during resistance training. This article will look at how these methods might help build muscle and how they can be used in training programs.",
        "reference": "A variety of training techniques have been suggested to support muscle growth. Forced repetitions/drop sets (use lighter weight or receiving help to continue the set), supersets (combining exercises with no rest), and heavy negatives (lowering phase of the lift), are pretended to enhance hypertrophy (increasing muscle mass). This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to strength training.",
        "metrics": {
          "SARI": 29.75200026447023,
          "BLEU": 0.08093498372449986,
          "FKGL": 10.8
        }
      },
      {
        "source": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. Although both groups are known to display impressive muscularity, it is not clear which method is superior for hypertrophic gains. It has been shown that many factors mediate the hypertrophic process and that mechanical tension, muscle damage, and metabolic stress all can play a role in exercise-induced muscle growth. Therefore, the purpose of this paper is twofold: (a) to extensively review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and (b) to draw conclusions from the research as to the optimal protocol for maximizing muscle growth.",
        "prediction": "Many people who lift weights want to increase their lean body mass but there is not enough research on the best way to achieve this goal Bodybuilders usually train with moderate weights and take short breaks which creates a lot of metabolic stress Powerlifters train with heavier weights and longer rest periods Both types of athletes have impressive muscles but it is unclear which training method leads to better muscle growth Several factors affect muscle growth including mechanical tension muscle damage and metabolic stress This paper aims to review the research on how muscles grow and to suggest the best training method for maximizing muscle growth",
        "reference": "Increasing muscle mass is the goal of many people who lift weights. Research is lacking, however, as to the optimal way to induce muscle growth. Bodybuilders train with moderate loads and short rest intervals. Powerlifters, on the other hand, train with high-intensity loads and lengthy rest periods. Although both groups are known to display impressive muscularity, it is not clear which method is superior for increasing muscle mass. It has been shown that many factors mediate the process of muscle growth and that mechanical tension, muscle damage, and metabolic stress all can play a role in muscle growth. Therefore, the purpose of this paper is to review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and to draw conclusions from the research as to the optimal way to induce muscle growth.",
        "metrics": {
          "SARI": 23.358849862160767,
          "BLEU": 0.05536432601885344,
          "FKGL": 43.5
        }
      }
    ]
  ],
  "aiir_mistral_prompt": [
    {
      "SARI": 42.126106450585326,
      "BLEU": 0.22788502802718158,
      "FKGL": 15.104571428571427
    },
    [
      {
        "source": "Abstract   Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. This work describes an innovative mobile application for wildfire information management that operates on Windows Phone devices and acts as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management. Several tasks can be accomplished from the AEGIS App, such as routing, spatial search for closest facilities and firefighting support infrastructures, access to weather data and visualization of fire management data (water sources, gas refill stations, evacuation sites etc.). An innovative feature of AEGIS App is the support of these tasks by a digital assistant for artificial intelligence named Cortana (developed by Microsoft for Windows Phone devices), that allows information utilization through voice commands. The application is to be used by firefighting personnel in Greece and is potentially expected to contribute towards a more sophisticated transferring of information and knowledge between wildfire confrontation operation centers and firefighting units in the field.",
        "prediction": "Abstract Novel technological advances in mobile devices and applications can be used in fighting wildfires allowing users to easily perform several everyday tasks such as accessing data and information sharing intelligence and coordinating personnel and vehicles This work describes an innovative mobile application for managing wildfire information that works on Windows Phone devices and serves as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management Several tasks can be accomplished from the AEGIS App such as routing finding the closest facilities and firefighting support infrastructures accessing weather data and visualizing fire management data like water sources gas refill stations and evacuation sites An innovative feature of the AEGIS App is that it offers support for these tasks through a digital assistant for artificial intelligence named Cortana developed by Microsoft for Windows Phone devices which allows users to utilize information through voice commands The application is intended for use by firefighting personnel in Greece and is expected to help improve the transfer of information and knowledge between wildfire operation centers and firefighting teams in the field",
        "reference": "Novel mobile devices and applications can be used in wildfire confrontation by helping users to access data and information and coordinate personnel and vehicles. This paper describes a new Windows Phone application for wildfire information management which is complementary to the web version of the AEGIS platform for wildfire prevention and management. The AEGIS App can be used for searching for the closest firefighting infrastructures, as well as access to weather data, and visualization of water sources, gas refill stations, evacuation sites, etc. The AEGIS App supports these tasks through voice commands with Microsoft Cortana (AI digital assistant for Windows Phone).  The application is designed for firefighters in Greece but it can help communication between wildfire coordination and local firefighting units.",
        "metrics": {
          "SARI": 42.805476435426854,
          "BLEU": 0.18207005980626542,
          "FKGL": 77.8
        }
      },
      {
        "source": "In recent years, mobile technologies have developed and applied in education fields, and some mobile emerging carriers with mobile technologies include a personal digital assistant (PDA), smart phone, and e-book. Some of the mobile carriers combines context-aware technologies or involve into wireless network environments to make effective use through the combination of learning scenarios and technologies, and then provide students new learning experiences differed from the past learning. In view of the application of mobile technology in education, some previous studies have addressed that mobile learning is a meaningful learning that can improve the interaction between students and the situations and reach the purposes of learning. In the main stream of mobile learning, using mobile carriers with suitable learning methods or strategies in mobile learning activities for different students to enhance learning have gradually become a important and concern issue. The purpose of this study is to investigate the learning achievement and learning attitude of elementary school students on a campus plant learning activity when they use mobile carriers and competitive learning strategies. The experimental results show that the competitive learning group of students have better learning performance than non-competitive learning group of students. After completing the learning activity, the two groups of students presented high positive attitudes towards learning.",
        "prediction": "In recent years, mobile technologies have been developed and used in education. Some examples of these mobile technologies include a personal digital assistant (PDA), smartphone, and e-book. These technologies often work together with context-aware technologies, which adapt to a user's environment, and operate in wireless network environments. This combination allows for new and effective learning experiences that are different from traditional methods. Previous studies have shown that mobile learning enhances interaction between students and their learning situations, achieving educational goals more effectively. In the field of mobile learning, using mobile technologies along with appropriate learning methods or strategies has become increasingly important for enhancing student learning. The aim of this study is to examine the learning achievements and attitudes of elementary school students participating in a campus plant learning activity while using mobile technologies and competitive learning strategies. The results indicate that students in the competitive learning group performed better than those in the non-competitive learning group. After the activity, both groups of students exhibited high positive attitudes toward learning.",
        "reference": "Mobile technologies, such as personal digital assistants (PDA), smartphones, and e-books, are increasingly used in the educational field. Some mobile carriers combine learning scenarios and context-aware technologies or wireless network environments providing students with new learning experiences. Mobile learning is effective as it can improve the interaction between students and situations. An important question in the mobile learning field is how to enhance learning through the use of mobile carriers with suitable learning methods in student activities. This study investigates learning achievement and attitude of elementary school students when they use mobile carriers and competitive learning strategies during campus plant learning activities. The competitive learning group of students has better learning performance than the non-competitive one. After the activity, both groups of students were positive about learning.",
        "metrics": {
          "SARI": 44.836055343668875,
          "BLEU": 0.15726674488369444,
          "FKGL": 14.2
        }
      },
      {
        "source": "This paper proposes an electrophysiological wireless patient monitoring system which integrates a Wireless ECG signal transmitter, GPS device and a mobile phone to acquire physiological signals and transmit them to a local server via Bluetooth wireless technology. Four kinds of monitor units were specially designed for a wireless communication, including a control center , a local monitor unit, mobile devices (personal digital assistant; PDA), and a Web page (for both patient and doctor). The use of various monitor units is created to fulfill different medical personnel requirements and wishes. This application was developed to promote the mobility and flexibility for the patients and also for the medical personnel, which further will improve both the quality of health care and lifestyle of the patient.",
        "prediction": "This paper proposes a wireless system for monitoring patients' physiological signals. It uses a device that transmits Electrocardiogram (ECG) signals, a Global Positioning System (GPS) device, and a mobile phone to collect these signals and send them to a local server using Bluetooth technology. Four types of monitoring units were specially designed for wireless communication, including a control center, a local monitor unit, mobile devices known as Personal Digital Assistants (PDAs), and a web page accessible to both patients and doctors. These different monitoring units are created to meet the various needs and preferences of medical personnel. This application aims to enhance the mobility and flexibility of both patients and medical staff, ultimately improving the quality of health care and the patient's lifestyle.",
        "reference": "This paper suggests a system to monitor patients wirelessly using a device that transmits ECG signals, a GPS device, and a mobile phone. The signals are sent to a local server via Bluetooth technology. Four types of monitor units were created for wireless communication: a control center, a local monitor unit, mobile devices such as personal digital assistants (PDA), and a web page accessible to both patients and doctors. Various monitor units are used to satisfy different medical personnel needs. This application was developed to boost the quality of healthcare and patient lifestyle by increasing mobility and flexibility for both patients and medical personnel.",
        "metrics": {
          "SARI": 59.46413516522181,
          "BLEU": 0.3462607889571775,
          "FKGL": 16.4
        }
      },
      {
        "source": "As various kinds of output devices emerged, such as highresolution printers or a display of PDA(Personal Digital Assistant), the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging image with high quality. One of the largest problems on image enlargement is the exaggeration of the jaggy edges. To remedy this problem, we propose a new interpolation method, which uses artificial neural network to determine the optimal values of interpolated pixels. The experimental results are shown and evaluated. The effectiveness of our methods is discussed by comparing with the conventional methods.",
        "prediction": "As different types of output devices appeared, like high-resolution printers or displays of Personal Digital Assistants (PDAs), the need for high-quality image resolution conversion has grown. This paper suggests a new way to enlarge images while maintaining high quality. One major issue with image enlargement is the exaggeration of jagged edges. To solve this problem, we introduce a new interpolation method, which uses artificial neural networks to find the best values for the pixels that are being created. We present and evaluate the experimental results. The effectiveness of our method is discussed in comparison to traditional methods.",
        "reference": "The rise of output devices like high-resolution printers and PDA displays has increased the need for high-quality resolution conversion. The paper proposes a new method to make images bigger while maintaining high quality. The main issue with enlarging images is that jagged edges can become exaggerated. To solve this problem, we suggest a new method that helps us to estimate the value of the newly generated pixels using a neural network. The experiment's results are presented and analyzed. We evaluate the effectiveness of our methods by comparing them to traditional approaches.",
        "metrics": {
          "SARI": 49.44152835418657,
          "BLEU": 0.1869432873237128,
          "FKGL": 12
        }
      },
      {
        "source": "A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours. For them, everyday tasks like navigating through a train or metro network map becomes demanding. We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant, non-invasive sound. This technique was implemented inside a Personal Digital Assistant (PDA) portable device. In this implementation, colour information is extracted from the input image and categorised according to how human observers segment the colour space. This information is subsequently converted into sound and sent to the user via speakers or headphones. In the original implementation, it is possible for the user to send its feedback to reconfigure the system, however several features such as these were not implemented because the current technology is limited.We are confident that the full implementation will be possible in the near future as PDA technology improves.",
        "prediction": "A significant percentage of the human population suffers from difficulties in distinguishing or even seeing colors. For these individuals, everyday tasks like navigating a train or metro network map become challenging. We present a new method for extracting color information from everyday images and presenting it to visually impaired users as pleasant and non-intrusive sounds. This method was integrated into a Personal Digital Assistant, which is a small portable device. In this setup, color information is extracted from input images and categorized based on how human observers typically divide the color space. This information is then converted into sound and delivered to the user through speakers or headphones. In the initial implementation, users can send their feedback to adjust the system, but several features like this were not included due to current technological limitations. We are confident that a complete implementation will be possible soon as Personal Digital Assistant technology advances.",
        "reference": "A significant percentage of people have trouble differentiating or seeing colors. Simple tasks like using a train or subway map can be difficult for them. We introduce a new method to help visually impaired users perceive color information using non-invasive, pleasant sounds generated from everyday stimuli. We put this technique into a portable device called a Personal Digital Assistant (PDA). In this implementation, we extract color information from the input image and organize it based on how people perceive color. Then, the information is changed into sound and sent to the user through speakers or headphones. In the first version, the user could give feedback to change the system, but we couldn't add all the features due to technology limitations. We believe that we can add all the features soon as PDA technology advances.",
        "metrics": {
          "SARI": 48.21236036182992,
          "BLEU": 0.20059797885935823,
          "FKGL": 14.2
        }
      },
      {
        "source": "Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients but also incurs a great financial cost to the society. A medicine decision support system in a Personal Digital Assistant (PDA), with a barcode reader, can provide an overview of the patientsu0027 complete medicine use, and detect unsuitable drugs and drug combinations. Focusing on the elderly, our aim was to evaluate if a mobile medicine decision support system with a barcode reader is useful and user-friendly for nurses in home care. The participants received a comprehensive overview from the patientsu0027 medicine use and noted drug-drug interactions, therapeutic duplications and warnings for drugs unsuitable for elderly people. The nurses regarded that the decision support system increased prevention and safety, was useful and user-friendly. Our findings suggest that most of the content and functions were regarded as important. Therefore, this decision support system might be a useful tool for district nurses.",
        "prediction": "Inappropriate use of medicines increases the risk of hospital admissions for elderly people. This not only leads to unnecessary suffering for patients but also creates significant financial costs for society. A medicine decision support system in a Personal Digital Assistant (PDA), which is a portable electronic device, with a barcode reader can give an overview of the patients' complete medicine use and identify unsuitable drugs and drug combinations. Our aim was to evaluate if a mobile medicine decision support system with a barcode reader is useful and easy to use for nurses providing home care to the elderly. The participants received a comprehensive overview of the patients' medicine use and noted drug-drug interactions, which are when two or more drugs negatively affect each other, therapeutic duplications, which occur when a patient is prescribed the same medication more than once, and warnings for drugs that are unsuitable for elderly people. The nurses felt that the decision support system improved prevention and safety, was helpful, and easy to use. Our findings indicate that most of the content and features were considered important. Thus, this decision support system could be a useful tool for district nurses.",
        "reference": "Misusing medicines can make older people more likely to end up in the hospital. This causes patient suffering and financial cost to society. A support system for medicine decision, with a bar code reader, in a PDA can help identify inappropriate medication use and combinations by providing a complete overview of the patient's medication The goal was to evaluate if a mobile medicine decision support system with a barcode reader is helpful and easy to use for home care nurses working specially with older patients. The participants were provided with a complete overview of the patient's medication usage, and identified any drug interactions, duplicated medications, and any warnings about drugs that are not suitable for elderly patients. The nurses found the decision support system to be helpful, and easy to use, and improving the prevention and safety measures. Our results indicate that the majority of the content and features were considered significant. So, this system could be helpful for district nurses.",
        "metrics": {
          "SARI": 53.167072959145564,
          "BLEU": 0.24715465698176722,
          "FKGL": 13.9
        }
      },
      {
        "source": " :[0],\"access to patient appointment schedules can help clinicians manage time and problems better. Many large health care organizations manage clinical appointment scheduling via enterprise resource scheduling systems which are poorly accessible by clinicians. Also, staff other than the clinicianu0027s personal assistant(s) may manage scheduling, making it difficult for clinicians to stay informed of changes. Many a clinician today uses a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician.",
        "prediction": "Access to patient appointment schedules can help doctors manage their time and issues better. Many large healthcare organizations use enterprise resource scheduling systems to manage clinical appointment scheduling, but these systems are often not easily accessible to doctors. Additionally, other staff members, besides the doctor’s personal assistant(s), may handle scheduling, which can make it hard for doctors to stay updated on changes. Many doctors today use a personal digital assistant (PDA) that has basic calendar features. Our “PalmOversite” project shows that it is possible to combine information from the enterprise appointment schedule into a PDA calendar, making schedule information much more available to the doctor.",
        "reference": "Knowing patients' appointment schedules can help clinicians manage time and problems more effectively. Healthcare organizations often use enterprise resource scheduling systems for clinical appointment scheduling, but these systems may not be easily accessible by clinicians. Moreover, scheduling may not only be handled by the clinician's assistant(s), creating challenges for clinicians to stay updated on any changes. Many clinicians today use a personal digital assistant (PDA) with a basic calendar. Our project called 'PalmOversite' shows the practicability of integrating enterprise appointment schedule information into a PDA calendar. This makes it easier for clinicians to access their schedule information.",
        "metrics": {
          "SARI": 53.39030087225976,
          "BLEU": 0.2830281279750332,
          "FKGL": 13.8
        }
      },
      {
        "source": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. Currently, a SES employee visits each health establishment to collect this information by hand, process it and type it into an electronic medical record system  this :[81],\"paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify and upload monthly bacteriology data into the PIH-EMR. After an initial implementation period, we performed a pilot study to test the use of this system. We completed a baseline assessment in two health districts and then implemented the electronic system in one of the districts while the control site continued to use the paper-based system during the same period. The PDA-based system had a processing time of 6.2 days, significantly lower than measurements for both the baseline [54.8] and control sites [64.4] (both pu003c0.0001). It was also able to reduce the frequency of discrepancy from 10.1% to 2.8% (pu003c0.0001) and receive positive feedback from the users. Finally, the system’s cost would be recuperated in three months from time savings due to increased work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors and costs.",
        "prediction": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat most patients with multidrug-resistant tuberculosis (MDR-TB) in Peru, along with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments across the city, are an important part of this treatment. Currently, a SES employee visits each health establishment to manually collect this information, process it, and enter it into an electronic medical record system. In this paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify, and upload monthly bacteriology data into the PIH electronic medical record (PIH-EMR). After an initial implementation period, we conducted a pilot study to test this system. We completed a baseline assessment in two health districts and then introduced the electronic system in one district while the control site continued using the paper-based system during the same period. The PDA-based system processed data in 6.2 days, notably faster than the measurements for both the baseline (54.8 days) and control sites (64.4 days) which were both statistically significant (p < 0.0001). It also reduced the frequency of discrepancies from 10.1% to 2.8% (p < 0.0001) and received positive feedback from users. Finally, the system's costs would be recovered in three months due to time savings from increased work efficiency. This system will be studied further to assess its effects on delays, errors, and costs.",
        "reference": "PIH (Partners In Health) and SES (Socios En Salud) provide treatment for most of the MDR-TB (multidrug-resistant tuberculosis) patients in Peru along with the Peruvian NTP (National TB Program). Monthly bacteriology tests are necessary for this treatment and must be collected from various health establishments located throughout the city. An employee from SES is currently going to each health establishment and collecting monthly bacteriology test results by hand, processing them, and typing them into an electronic medical record system. In this paper, we explain how we made a digital system using a personal digital assistant (PDA) to gather, check, and upload monthly bacteriology data into the PIH-EMR (Partners In Health Electronic Medical Record).  After an initial period of putting the system into action, we conducted a test study to evaluate the effectiveness of using this system. We first evaluated the current system in two health districts, then introduced the electronic system in one district while the other continued using the paper-based system during the same time. The PDA-based system took only 6.2 days to process, which was significantly lower than both the baseline (54.8) and control sites (64.4) (both less than 0.0001). The PDA-based system reduced differences from 10.1% to 2.8% and received positive feedback from users. The system would pay for itself in three months due to increased efficiency and time savings. A larger study will be conducted to assess the impact of this system on delays, errors, and costs.",
        "metrics": {
          "SARI": 54.366299646006624,
          "BLEU": 0.36732470250919236,
          "FKGL": 10.2
        }
      },
      {
        "source": "In order to facilitate knowledge transfer between specialists and generalists and between experts and novices, and to promote interdisciplinary communication, there is a need to provide methods and tools for doing so. This interdisciplinary research team developed and evaluated a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage/tele-consultation when the presenting problem was chest pain. The combined human factors methods of cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase were used to develop the DST. A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period. During this time, the DST was used by the nine nursing coordinators who provide tele-triage/tele-consultation 24/7. This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses, its potential for use by nurses less experienced in cardiac care, and for its potential use in an interdisciplinary team environment.",
        "prediction": "To help share knowledge between specialists and generalists and between experts and beginners, as well as to encourage communication across different fields, it is important to offer methods and tools for this purpose. An interdisciplinary research team created and tested a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage and tele-consultation when the main issue was chest pain. They used human factors methods called cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase to develop the DST. A pilot clinical trial took place at a high-level cardiac care hospital over three months. During this trial, the DST was used by nine nursing coordinators who provide tele-triage and tele-consultation around the clock. This clinical trial confirmed the design's effectiveness and showed its value to experienced cardiac care nurses, its potential usefulness for nurses with less experience in cardiac care, and its applicability in an interdisciplinary team setting.",
        "reference": "To improve communication between specialists, generalists, experts, and novices and encourage interdisciplinary collaboration, we need to develop methods and tools. An interdisciplinary research team created and tested a decision support tool (DST) on a personal digital assistant (PDA) to aid cardiac tele-triage/tele-consultation for patients with chest pain. To create the DST (Decision Support Tool), we used both cognitive work analysis and ecological interface design methods during the requirements-gathering and design phases. A trial was done for 3 months at a quaternary cardiac care hospital During the trial, nine nursing coordinators used DST for 24/7 tele-triage/tele-consultation. This clinical trial confirmed that the design is useful for advanced cardiac care nurses, as well as for less experienced nurses and interdisciplinary teams.",
        "metrics": {
          "SARI": 52.66593137115668,
          "BLEU": 0.28330127375343966,
          "FKGL": 15.9
        }
      },
      {
        "source": "This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Since conventional computer-assisted language learning is often boring, it exploits a cyber pet game to increase the learner’s interest. M-CALL runs on a personal digital assistant with public wireless LAN for mobile learning. It grows a cyber pet by solving problems of Korean language learning. Korean Proficiency Test (KPT), a nationally certified Korean language test, was used as problem sets. It consists of cyber pet game, mobile learning courseware, mobile learning system, and mobile tutoring. It provides various functions for Korean language learning. Currently, the prototype M-CALL was designed and partly implemented between mobile PDA and personal computer.",
        "prediction": "This paper presents M-CALL, a mobile computer-assisted language learning course for people learning the Korean language. Traditional computer-assisted language learning can often be uninteresting, so M-CALL uses a cyber pet game to make learning more engaging. M-CALL operates on a personal digital assistant, which is a portable handheld device, connected to public wireless local area networks for mobile learning. Learners grow a virtual pet by solving Korean language learning problems. The Korean Proficiency Test, known as KPT, is a nationally recognized test for measuring Korean language skills and was used as the problem sets. M-CALL includes a cyber pet game, mobile learning course materials, a mobile learning system, and mobile tutoring. It offers various tools for learning Korean. Currently, the prototype of M-CALL has been designed and partially implemented to work between a mobile PDA and a personal computer.",
        "reference": "M-CALL is a mobile courseware for Korean language learners. It incorporates a cyber pet game to make it more engaging for learners than traditional computer-assisted language learning methods. M-CALL is a mobile learning system that uses a personal digital assistant with public wireless LAN (Computer network in small geographic areas). A cyber pet is raised while solving problems related to Korean language learning. The KPT, a nationally recognized exam for Korean language proficiency, served as the source material for the problem sets. It includes a virtual pet game, a mobile course for learning, a mobile learning platform, and mobile tutoring. It offers multiple features for learning Korean. Right now, a preliminary version of M-CALL has been developed and partially implemented for use on both mobile PDAs and personal computers.",
        "metrics": {
          "SARI": 51.27721290139748,
          "BLEU": 0.242311155558273,
          "FKGL": 12.4
        }
      },
      {
        "source": "This study compared text entry performance of two stylus-driven soft keyboards for use in hand-held computing devices: the QWERTY and the T9. Participants transcribed text presented on a computer screen into a personal digital assistant (PDA) using a stylus and one of these two keyboards. We introduced a new psychophysical technique for measuring transcription rate that provides a composite measure of speed and accuracy. Using this technique, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparent superior physical characteristics. An ancillary experiment demonstrated that the poorer performance of the T9 layout may have resulted from an increase in visual scanning time due to perceptual grouping of the letters on the keys. Together these findings imply that the QWERTY keyboard layout remains the most effective of the currently available designs for stylus tapping on soft keyboards.",
        "prediction": "This study compared how well people can enter text using two types of soft keyboards designed for stylus use in handheld devices: the QWERTY keyboard and the T9 keyboard. Participants typed text shown on a computer screen into a personal digital assistant, also known as a PDA, with a stylus by using one of these two keyboards. We used a new method for measuring typing speed that takes into account both how fast and how accurately people type. With this method, we found the highest typing speed for each keyboard. The results showed that people typed faster using the QWERTY keyboard compared to the T9 keyboard, even though the T9 keyboard has better physical design features. An additional experiment suggested that the T9 keyboard's slower performance might be due to longer time spent looking for letters because of how the letters are grouped on the keys. Overall, these findings suggest that the QWERTY keyboard layout is still the best option available for tapping on soft keyboards with a stylus.",
        "reference": "This study compared two onscreen keyboards, the QWERTY and the T9, for text entry performance on hand-held computing devices. Participants used a stylus to input text on a PDA using either a QWERTY or T9 keyboard in this study. We created a new way to measure transcription rate that considers both speed and accuracy together. We used this method to determine the fastest possible typing speed for each keyboard. The study found that the QWERTY keyboard had a higher transcription rate than the T9 keyboard, even though the T9 keyboard was physically better. Another test showed that the T9 keyboard's lower performance might be because the grouped letters on its keys require more visual scanning time. Overall, these results suggest that, among the current options, the QWERTY keyboard layout is still the most efficient for typing on touch screens with a stylus.",
        "metrics": {
          "SARI": 51.39561785093131,
          "BLEU": 0.2259954158350428,
          "FKGL": 11.5
        }
      },
      {
        "source": "The proliferation of mobile devices such as smart phones and Personal Digital Assistant (PDA) opens new ways for developing mobile E-commerce systems (so called Mobile-commerce or M-commerce system). In Mcommerce systems, we still see many common components that we have in standard E-commerce applications, such as web servers and database servers. However, these new applications raise some unique challenges. For example, the limitations of mobile devices (e.g., small screen size and reduced CPU performance) implies that software development is partly different from desktop applications. Questions which are posed when designing M-commerce system include: “What kind of information do the users want to access via mobile devices?”, “How can we provide useful applications with limited user input facility?”, “How can we test a distributed M-commerce system?” In this paper, we discuss about some software development issues for mobile commerce systems from our experiences while developing an M-commerce system for tourists. Our prototype, easyHotel, is an useful software that allows booking hotel rooms via mobile phones.",
        "prediction": "The growth of mobile devices like smartphones and Personal Digital Assistants (PDAs) creates new opportunities for building mobile E-commerce systems, often referred to as Mobile-commerce or M-commerce systems. In M-commerce systems, many components resemble those found in standard E-commerce applications, such as web servers (computers that host websites) and database servers (computers that store and manage data). Nevertheless, these new applications present unique challenges. For instance, the limitations of mobile devices, such as small screens and lower processing power (CPU performance), mean that software development differs somewhat from desktop applications. Designers of M-commerce systems must consider questions like: \"What information do users want to access through mobile devices?\" \"How can we create useful applications with limited user input options?\" \"How can we test a distributed (spread out across multiple locations) M-commerce system?\" In this paper, we discuss software development issues for mobile commerce systems based on our experiences while creating an M-commerce system for tourists. Our prototype, easyHotel, is a useful software tool that allows users to book hotel rooms using mobile phones.",
        "reference": "The increasing use of mobile devices like smartphones and PDAs provides opportunities for the development of new mobile e-commerce systems, also known as M-commerce (Mobile-commerce) systems. M-commerce systems share several components with traditional E-commerce applications, such as web servers and database servers. However, these new applications present some distinct challenges. For example, due to mobile devices' limitations, such as smaller screens and weaker processors (also called CPUs), software development for mobile applications is partially different from desktop applications. Designing an M-commerce system involves asking questions such as: \"What information do users want on mobile devices?\", \"How can we create useful apps with limited user input?\", and \"How do we test a distributed M-commerce system?\" In this paper, we share our experiences developing an M-commerce system for tourists and discuss software development issues for such systems. Our software prototype, easyHotel, is a helpful application enabling users to book hotel rooms through their mobile phones.",
        "metrics": {
          "SARI": 57.63417047570992,
          "BLEU": 0.3277912866130272,
          "FKGL": 13.1
        }
      },
      {
        "source": "It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As emerging mobile platforms such as different kinds of Personal Digital Assistant (PDA) tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common approaches for content adaptation are automated conversion and explicit specification of adapted content, with a trade-off between quality and development/maintenance effort. As alternative avoiding this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework in the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development/maintenance effort.",
        "prediction": "It is widely recognized that information such as web content must be adjusted for mobile devices because of limitations in mobile environments. As new mobile platforms like different types of Personal Digital Assistants (PDAs) have varying capabilities, we suggest that these adjustments should be specific to each platform. Common methods for adapting content include automated conversion and clearly defining adapted content, which involves a trade-off between quality and the effort needed for development and maintenance. To avoid this trade-off, we propose a straightforward object-oriented framework for content adaptation. To make it easier to use this framework on the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation called WCML. We showcase our object-oriented approach using an example application to illustrate how this method of specifying platform-adapted content can reduce the effort required for development and maintenance.",
        "reference": "It is widely recognized that information, such as web content, should be adjusted for mobile platforms to accommodate the limitations of mobile environments. As new mobile platforms, such as various types of PDAs, differ significantly in their capabilities, we propose that adaptation should be tailored to each platform. Common methods for content adaptation involve automated conversion or explicitly specifying adapted content, balancing the trade-off between quality and the effort required for development and maintenance. Instead of making this trade-off, we suggest using a straightforward object-oriented framework for content adaptation. To make this framework more accessible for use on the web, we build our approach on the object-oriented WebComposition model and its implementation, WCML, which is an application of XML. We use our object-oriented framework in an example application to show how specifying platform-adapted content using an object-oriented approach can lower development and maintenance costs.",
        "metrics": {
          "SARI": 63.69172534001787,
          "BLEU": 0.40645479571810483,
          "FKGL": 16
        }
      },
      {
        "source": "In monitoring a patientu0027s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in timely-context aware manner. In this paper a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device — PSD) used in the Healthcare systems are bulky in design and are not flexible and comfortable for the elderly patients. Presented is a unique flexible, as well as detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage and processing of patient related data, which ensures periodic logging of patient data without saturating communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smart-phone and the ECG and temperature acquisition apparatus is implemented Bluetooth. The presented Data Management System — Periodic Model (DMS — PM) is designed to manage wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a Smart phone making use of the existing GSM network.",
        "prediction": "In monitoring a patient’s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in a timely and context-aware manner. In this paper, a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device - PSD) used in healthcare systems are bulky in design and are not flexible and comfortable for elderly patients. Presented is a unique flexible and detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage, and processing of patient-related data, which ensures periodic logging of patient data without overwhelming communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition device is implemented using Bluetooth. The presented Data Management System - Periodic Model (DMS - PM) is designed to manage the wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a smartphone, making use of the existing Global System for Mobile Communications (GSM) network.",
        "reference": "Medical practitioners can access a wealth of data sources by using Body Area Networks (BAN) to monitor a patient's real-time vital signs. A BAN refers to a network of wearable or implantable devices worn on or attached to the human body for various purposes. Timely and context-aware delivery of data is crucial. This paper presents a system specifically designed for patients with cardiac disorders, focusing on the design of the sensing device and communication scheme selected. Many of the current wearable physiological devices used in healthcare systems, known as Patient Sensing Devices (PSD), are bulky and uncomfortable for elderly patients due to their inflexible design. We introduce a unique Patient Sensing Device (PSD) that is both flexible and detachable, designed for patient comfort. We also propose a model for transmitting, storing, and processing patient-related data that ensures regular logging of data without overwhelming communication networks. We have developed and implemented a proof-of-concept prototype that can transmit a patient's Electrocardiogram (ECG) signal and body temperature, potentially expanding to other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition apparatus is achieved via Bluetooth. The Data Management System - Periodic Model (DMS - PM) introduced here is created to facilitate the wireless connection between sensor units and the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a smartphone, utilizing the existing GSM network.",
        "metrics": {
          "SARI": 54.6488997588228,
          "BLEU": 0.3907427873939354,
          "FKGL": 15.2
        }
      },
      {
        "source": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. Current trend integrates usage of mobile terminals such as smart PDA (personal digital assistant) in a learning system. Such devices can communicate with wireless mobile terminal and realize the intelligence exchange with its surroundings [1]. A LOCH (Language Learning Outside the Classroom) system named One Day Trip is developed to assist overseas students to learn Japanese in real life situations with the help of a PDA. This paper proposes the implementation with Knowledge Management (KM) concepts in the system to aid the teacher and learners in the learning environment.",
        "prediction": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. The current trend integrates the use of mobile devices like smart PDAs, which stands for personal digital assistants, into a learning system. Such devices can communicate with wireless mobile terminals and allow for intelligent exchanges with their surroundings. A LOCH, which stands for Language Learning Outside the Classroom, system named One Day Trip has been developed to help overseas students learn Japanese in real-life situations using a PDA. This paper proposes the implementation of Knowledge Management, or KM, concepts in the system to assist both teachers and learners in the learning environment.",
        "reference": "Wireless and mobile technology progress has allowed more platforms to facilitate learning within a unified space. The current trend is to use mobile devices, such as smart PDAs, in learning systems. These devices can communicate wirelessly with mobile terminals and exchange information with the surrounding environment One Day Trip is a language learning system called LOCH (Language Learning Outside the Classroom), designed to help foreign students learn Japanese in real-life situations using a PDA. This paper suggests incorporating Knowledge Management (KM) concepts in the One Day Trip system to support both the teacher and learners in the learning environment.",
        "metrics": {
          "SARI": 48.83990062026911,
          "BLEU": 0.29552220975916055,
          "FKGL": 13.1
        }
      },
      {
        "source": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko personal digital assistant, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Instead of porting each application to individual platforms, which would prove to be extremely time consuming because of other APIu0027s and networking protocols, we created the Vassar College Messaging Layer. This layer abstracts the Cybiko networking structure and interface, allowing Cybiko applications to be ported to other platforms with minimal code alteration.",
        "prediction": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications were designed for the Cybiko personal digital assistant, which is a low-cost alternative to other hardware devices. Unfortunately, the Cybiko PDA has recently been discontinued. As a result, these applications needed to be converted to work on other platforms to remain useful. Instead of converting each application to individual platforms, which would be very time-consuming due to different application programming interfaces (APIs) and networking protocols, we created the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, allowing Cybiko applications to be adapted to other platforms with minimal changes to the code.",
        "reference": "In recent years, we have created applications for teaching both wired and wireless networking. These applications are designed for the Cybiko personal digital assistant, which offers a cost-effective alternative to other hardware options. The Cybiko PDA was discontinued recently. Consequently, these applications needed to be adapted to other platforms to maintain their usefulness. To avoid the time-consuming task of porting each application to various platforms due to different APIs and networking protocols, we developed the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, making it easier to adapt Cybiko applications to different platforms without needing to make significant changes to the code.",
        "metrics": {
          "SARI": 61.39233943230872,
          "BLEU": 0.48380976987791685,
          "FKGL": 14.1
        }
      },
      {
        "source": "Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful (and thus more useful), machines. On the other side of this border we find the Personal Digital Assistant  :[66],\"our view, to be useful as an extension of the usersu0027 private sphere, a machine must at least have enough functionality and resources to create trustworthy digital signatures (to speak for the user, as it were). A less resourceful machine can merely act as a memory prothesis, helping the owner remembering addresses and phone  are :[121],\"designed to be tamper resistant, and as such they seem ideal as a minimal machine. However, trustworthy digital signatures can not be created by smartcards alone, simply because the user does not know what is given to the card for  :[163],\"order to be trusted--that is, being able to make trustworthy digital signatures--a smartcard must be supported by some infrastructure outside the card proper. We explore what must be included in such an infrastructure, and demonstrate that trustworthy digital signatures can in fact be made using a standard  :[211],\"argue that based on this fact, nontrivial distributed systems can be constructed by utilizing smartcards; a nontrivial system is one where holders of smartcards are \"first class citizens\". Asserting that a smartcard can act as a trusted machine gives new opportunities for designers of user-centric  that smartcards are :[257],\"here to stay, finding ways to apply them in constructive manners is prudent.",
        "prediction": "Unlike many other types of hardware, smartcards cannot communicate securely with the user. Without the means to keep the owner informed, it is hard to take advantage of the positive features of smartcards. We look at the area between smartcards and other, more powerful machines. On the other side of this line, we find the Personal Digital Assistant (PDA), which in our view must have enough functionality and resources to create trustworthy digital signatures, which means it can act on behalf of the user. A less capable machine can only help the owner remember addresses and phone numbers. Smartcards are made to be tamper-resistant, which makes them seem ideal as a minimal machine. However, smartcards cannot create trustworthy digital signatures on their own because the user does not know what information is given to the card for verification. For a smartcard to be trusted and to create reliable digital signatures, it must be supported by some infrastructure outside the card itself. We explore what must be included in this infrastructure and show that trustworthy digital signatures can indeed be created using a standard approach. We argue that based on this fact, nontrivial distributed systems can be built using smartcards; a nontrivial system is one where smartcard holders are treated as important participants. Claiming that a smartcard can serve as a trusted machine provides new opportunities for designers of user-centered technology. As smartcards are likely to remain relevant, finding ways to use them constructively is wise.",
        "reference": "Smartcards lack the capability to communicate with the user in a secure manner, unlike other hardware classes. (Smart cards are small, portable devices that have a special chip inside. They can store and process information securely.) Without a way to keep the owner updated, it becomes challenging to take advantage of the useful features of smart cards. We look at how smart cards and powerful machines can be combined to make even better devices. We believe that Personal Digital Assistants (PDAs) are more useful than smartcards because they can extend the user's private sphere. PDAs have enough functionality and resources to create trustworthy digital signatures, which can speak for the user. A less resourceful machine can serve as a memory aid, assisting the owner in remembering addresses and phone numbers, such as [121]. These machines are designed to be tamper-resistant, making them suitable for a minimalistic purpose. To create trustworthy digital signatures, a smartcard needs support from infrastructure outside the card itself. This is because the user is not aware of what information is given to the card, making it impossible for the card to be trusted solely on its own. We investigate the necessary components of this infrastructure and show that reliable digital signatures can indeed be created using a standard method [211]. We propose that smartcards can be utilized to build nontrivial distributed systems, where smartcard holders are considered important participants or \"first class citizens\". Claiming that smartcards can function as trusted machines opens up fresh possibilities for designers focused on user-centric approaches. It is evident that smartcards are here to stay, so it is wise for designers to explore and implement them in meaningful ways.",
        "metrics": {
          "SARI": 47.794135218638175,
          "BLEU": 0.2393160971156971,
          "FKGL": 12.4
        }
      },
      {
        "source": "In this paper we present the main features of a system that we have developed and that provides a new kind of tele-assistance service to elderly people.apart from the functionalities offered by the current tele-assistance services, also provides a high quality, anywhere and at any time assistance. We rate the assistance as high quality because allows the monitoring of vital signs and a local detection of anomalous situations by the device that the person carries in real time. Therefore, it sends alarms autonomously when  this paper we :[84],\"explain the three main functionalities offered by AINGERU: universal assistance anywhere and at any time, monitoring of vital signs irrespective of location or time and remote monitoring by allowing authorized external persons to consult data about monitored persons using the Internet. Moreover we show how those functionalities are achieved by making use of PDAs (Personal Digital Assistant), wireless communication, Semantic Web, Web services and agent technologies.",
        "prediction": "In this paper we present the main features of a system that we have developed which provides a new type of tele-assistance service for elderly people. In addition to the functions offered by current tele-assistance services, it also provides high quality assistance anytime and anywhere. We define the assistance as high quality because it allows the monitoring of vital signs, which are important indicators of health, and the local detection of unusual situations by the device that the person carries in real time. Therefore, it can send alerts on its own when necessary. We explain the three main functionalities offered by AINGERU: universal assistance anytime and anywhere, monitoring of vital signs regardless of location or time, and remote monitoring by allowing authorized people to access data about the monitored individuals using the Internet. Moreover, we demonstrate how these functionalities are achieved by using PDAs (Personal Digital Assistants), wireless communication, Semantic Web (a system that gives meaning to data on the internet so that it can be more easily shared and understood), Web services (software systems designed to support machine-to-machine interaction over a network), and agent technologies (software that acts on behalf of a user).",
        "reference": "This paper introduces a new tele-assistance service for elderly people that goes beyond existing services by offering high-quality assistance anywhere and anytime. Our system provides high-quality assistance because it allows real-time monitoring of vital signs and local detection of anomalous situations through the device carried by the person. We explain the three main functions of AINGERU in this paper: universal assistance anywhere and at any time, monitoring vital signs regardless of location or time, and remote monitoring that allows authorized external individuals to access data about monitored persons via the internet. We use PDAs, wireless communication, Semantic Web (aims to enhance the World Wide Web by adding meaning to the information available online), Web services, and agent technologies to achieve the aforementioned functionalities.",
        "metrics": {
          "SARI": 47.47946301565597,
          "BLEU": 0.20172203338944494,
          "FKGL": 18.2
        }
      },
      {
        "source": "Objective: To evaluate the effectiveness of a personal digital assistant (PDA)–based clinical decision support system (CDSS) on nonsteroidal anti-inflammatory drug (NSAID) prescribing safety in the outpatient  :[26],\"The design was a randomized, controlled trial conducted in a university-based resident clinic. Internal medicine residents received a PDA-based CDSS suite. For intervention residents, the CDSS included a prediction rule for NSAID-related gastrointestinal risk assessment and treatment recommendations. Unannounced standardized patients (SPs) trained to portray musculoskeletal symptoms presented to study physicians. Safety outcomes were assessed from the prescriptions given to the SPs. Each prescription was reviewed by a committee of clinicians blinded to participant, intervention group assignment, and baseline or follow-up  :[108],\"Prescriptions were judged as safe or unsafe. :[26],\"The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control  :[135],\"At baseline, the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar (0.27 vs. 0.29, p u003e 0.05). Controlling for baseline performance, intervention participants prescribed more safely than controls after receiving the CDSS (0.23 vs. 0.45 [F = 4.24, p u003c 0.05]). With the CDSS, intervention participants documented more complete assessment of patient gastrointestinal risk from  :[198],\"Participants provided with a PDA-based CDSS for NSAID prescribing made fewer unsafe treatment decisions than participants without the CDSS.",
        "prediction": "Objective To evaluate the effectiveness of a personal digital assistant PDA based clinical decision support system CDSS on nonsteroidal anti inflammatory drug NSAID prescribing safety in the outpatient setting The design was a randomized controlled trial conducted in a university based resident clinic Internal medicine residents received a PDA based CDSS suite For intervention residents the CDSS included a prediction rule for NSAID related gastrointestinal risk assessment and treatment recommendations Unannounced standardized patients SPs trained to portray musculoskeletal symptoms presented to study physicians Safety outcomes were assessed from the prescriptions given to the SPs Each prescription was reviewed by a committee of clinicians blinded to participant intervention group assignment and baseline or follow up prescriptions were judged as safe or unsafe The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control group At baseline the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar 0.27 vs 0.29 p greater than 0.05 Controlling for baseline performance intervention participants prescribed more safely than controls after receiving the CDSS 0.23 vs 0.45 F equals 4.24 p less than 0.05 With the CDSS intervention participants documented more complete assessment of patient gastrointestinal risk from Participants provided with a PDA based CDSS for NSAID prescribing made fewer unsafe treatment decisions than participants without the CDSS",
        "reference": "We did a study to test if a clinical decision support system (CDSS) on a personal digital assistant (PDA) could improve the safety of prescribing nonsteroidal anti-inflammatory drugs (NSAIDs) in outpatient clinics. We conducted a randomized, controlled trial in a university-based resident clinic. Residents in internal medicine were given a CDSS suite based on PDA. Intervention residents were provided with a CDSS that had a prediction rule for assessing gastrointestinal risk related to NSAIDs and recommendations for treatment. Trained individuals, known as standardized patients (SPs), who were equipped with musculoskeletal symptoms, interacted with the physicians participating in the study without prior notification. The study evaluated the safety outcomes of prescriptions given to standardized patients portraying musculoskeletal symptoms. A committee of clinicians who didn't know about the participant, intervention group assignment, or the baseline or follow-up data reviewed each prescription. They determined whether a prescription was safe or unsafe. The study aimed to see if the PDA-based CDSS would result in a reduction of unsafe prescribing of NSAIDs. The main outcome measured the difference in unsafe prescribing between the intervention and control groups. At the beginning of the study, both groups had a similar proportion of unsafe prescriptions. (0.27 vs 0.29) After controlling for the baseline performance, it was found that those who received the CDSS suite prescribed nonsteroidal anti-inflammatory drugs more safely than those who didn't (0.23 vs 0.45, with statistical significance). Participants who had access to the CDSS through a PDA documented a more comprehensive assessment of gastrointestinal risk in patients. Moreover, the presence of the CDSS resulted in fewer unsafe treatment decisions compared to participants who did not have access to the CDSS.",
        "metrics": {
          "SARI": 39.378935089865166,
          "BLEU": 0.09292944250692237,
          "FKGL": 16.6
        }
      },
      {
        "source": "Recently pedestrian navigation has been studied by many researchers, which provides pedestrians with a similar function of car navigation. On the other hand, a 3dimensional (3D) map, which is one of major themes in machine vision research, has been utilized as a simulation tool in city and landscape planning, and other engineering fields. The 3D maps will give more intuitive information compared to conventional 2-dimensional (2D) ones. In this paper we first propose pedestrian navigation based on 3D maps, and describe technologies required and its use situations. Secondly we compare effectiveness of 2D and 3D maps for navigation by object search experiments under conditions: 3D maps with and without texture; display sizes corresponding to a mobile phone and PDA (Personal Digital Assistant). The experimental results show that 3D maps with texture on mobile phone display size, and 3D maps without texture on PDA display size are superior to 2D maps in search time and error rate. From these results, we can say the effectiveness of 3D maps in pedestrian navigation.",
        "prediction": "Recently, pedestrian navigation has been studied by many researchers, providing pedestrians with a similar function to car navigation. On the other hand, a three-dimensional (3D) map, which is one of the major themes in machine vision research, has been used as a simulation tool in city and landscape planning, as well as other engineering fields. 3D maps provide more intuitive information compared to traditional two-dimensional (2D) maps. In this paper, we first propose pedestrian navigation based on 3D maps and describe the technologies required and their use situations. Secondly, we compare the effectiveness of 2D and 3D maps for navigation through object search experiments under certain conditions: 3D maps with and without texture and display sizes corresponding to a mobile phone and Personal Digital Assistant (PDA). The experimental results show that 3D maps with texture on mobile phone display sizes and 3D maps without texture on PDA display sizes perform better than 2D maps in terms of search time and error rate. From these results, we conclude about the effectiveness of 3D maps in pedestrian navigation.",
        "reference": "Recently many researchers have studied pedestrian navigation, which is similar to car navigation but designed for pedestrians. A 3D map is widely used in machine vision research and various engineering fields, such as city and landscape planning, as a simulation tool. 3D maps are more intuitive than 2D maps. In this paper, we suggest using 3D maps for pedestrian navigation and explain the necessary technology and possible scenarios for its application. We will conduct object search experiments to compare the effectiveness of 2D and 3D maps for navigation. The experiments will consider the following conditions: 3D maps with and without texture, and display sizes equivalent to a mobile phone and a Personal Digital Assistant (PDA). The results of the experiment suggest that 3D maps with texture on a mobile phone display and 3D maps without texture on a PDA display are better than 2D maps in terms of search time and error rate. Based on these results, we can conclude that 3D maps are effective for pedestrian navigation.",
        "metrics": {
          "SARI": 57.58736529763978,
          "BLEU": 0.3976315207205826,
          "FKGL": 14.3
        }
      },
      {
        "source": "Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments, that are characterized by low bandwidth communication facilities and ad hoc connection/disconnection to stationary systems. They can be used for information retrieval and information filtering, in which case they evaluate replies and return only the relevant data. Mobile agents as a metaphor of active objects are created on a mobile device such as a Personal Digital Assistant (PDA), will be launched into the information galaxy and are fulfilling the mobile user ‘s task on the services available on networked stationary systems. One transmission channel for these itinerant agents is email. This paper introduces Active M 3 as an example of an active mail framework, which can be regarded as a first approach in authoring mobile agents in a graphical interactive manner. Active M 3 integrates two known concepts: active mail and multimedia mail.",
        "prediction": "Mobile agents are a competitive idea in client-server computing and are especially useful in mobile environments. Mobile environments are characterized by communication that has low bandwidth, meaning it can handle only small amounts of data at a time, and by connections that may frequently start and stop with stationary systems, which are fixed devices like computers. Mobile agents can be used for information retrieval and information filtering. In these cases, they evaluate responses and return only the relevant information. Mobile agents act like active objects that are created on a mobile device, such as a Personal Digital Assistant (PDA). They are sent out into the information space to perform tasks for the mobile user using services available on networked stationary systems. One way to send these traveling agents is through email. This paper introduces Active M 3 as an example of an active mail framework. Active M 3 can be seen as an early attempt to create mobile agents in a graphical and interactive way. Active M 3 combines two established ideas: active mail, which refers to email that can perform tasks automatically, and multimedia mail, which refers to email that includes various forms of media like images or videos.",
        "reference": "Mobile agents are a competitive concept in client-server computing, particularly well-suited for mobile environments. These environments are characterized by limited bandwidth communication and the frequent connection and disconnection to stationary systems. They can be utilized for information retrieval and filtering, where they assess responses and provide only the pertinent data. Mobile agents act like active objects that can be created on a mobile device like a PDA. They are sent out to perform tasks on the networked stationary systems and fulfill the user's needs. Email serves as a transmission channel for these mobile agents. This paper presents Active M 3, which is an active mail framework that allows users to create mobile agents using a graphical interface. Active M3 is a framework that combines two concepts: active mail and multimedia mail.",
        "metrics": {
          "SARI": 47.88940238096913,
          "BLEU": 0.1918338161111804,
          "FKGL": 11.1
        }
      },
      {
        "source": "Digital Video Broadcasting — Handheld (DVB-H) is a technology developed as an extension of Digital Video Broadcasting — Terrestrial (DVB-T) with additional features that make it suitable for delivery to mobile devices such as phones and Personal Digital Assistant (PDAs). This paper deals with the conditions of receiving digital video broadcast type services in mobile handheld devices. The DVB-H standard is introduced and the key technology elements on data link layer and physical layer are discussed in some details. Finally the DVB-H terminal and network design is described.",
        "prediction": "Digital Video Broadcasting — Handheld (DVB-H) is a technology that builds on Digital Video Broadcasting — Terrestrial (DVB-T) to add features that make it better for mobile devices like phones and Personal Digital Assistants (PDAs). This paper talks about how to receive digital video broadcast services on mobile handheld devices. The DVB-H standard is explained and important technology aspects related to the data link layer and physical layer are discussed in detail. Finally, the design of the DVB-H terminal and network is described.",
        "reference": "DVB-H is a technology based on DVB-T that is designed for delivering content to mobile devices like phones and PDAs. This paper discusses how digital video broadcast services can be received on mobile handheld devices. The introduction of the DVB-H standard includes a discussion of key technology elements on the data link layer and physical layer, providing some detailed information. This paper also covers the design of DVB-H terminals and networks.",
        "metrics": {
          "SARI": 51.222597325756695,
          "BLEU": 0.23883506115211198,
          "FKGL": 12.4
        }
      },
      {
        "source": "Summary form only given, as follows. We present a design for a low cost but powerful and high speed communications device powered by IDTu0027s RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus compatible IDT ATM NIC. The device could be used as a two way pager, a set-top box, an Internet terminal, a personal digital assistant or as a video phone. The RV4640 is a very affordable 64 bit RISC processor that executes 175 dhrystone MIPS at 133 MHz. The RV4640 can be connected to a System Interface Chip which will provide I/O and memory control. The IDT77903 ATM card is a full duplexed 25 Mbps NIC with a PCI bus interface and it costs less than $100 (US). Integrating this card into our device as a network interface will make highspeed videoconferencing and multimedia applications easily implementable.",
        "prediction": "We present a design for a low-cost but powerful and high-speed communications device powered by IDT's RV4640 as its processing engine. The device can achieve very high speed, thanks to the Peripheral Component Interconnect (PCI) bus compatible IDT ATM Network Interface Card (NIC). The device could be used as a two-way pager, a set-top box, an Internet terminal, a personal digital assistant, or as a video phone. The RV4640 is a very affordable 64-bit Reduced Instruction Set Computer (RISC) processor that executes 175 dhrystone Million Instructions Per Second (MIPS) at a clock speed of 133 Megahertz (MHz). The RV4640 can be connected to a System Interface Chip which will provide Input/Output (I/O) and memory control. The IDT77903 ATM card is a full-duplex 25 Megabits per second (Mbps) NIC with a PCI bus interface, and it costs less than $100 (US). Integrating this card into our device as a network interface will make high-speed videoconferencing and multimedia applications easily implementable.",
        "reference": "Summary provided in the following. We propose a low-cost and high-speed communication device that uses IDT's RV4640 as the processing engine. This device is designed with IDT's RV4640 as the processing engine and can achieve high speed due to the PCI bus (a type of computer bus that simplifies the communication between computer components by providing a standardized interface) compatible IDT ATM NIC. This device can serve multiple purposes such as a two-way pager, set-top box, Internet terminal, personal digital assistant, or video phone. The RV4640 is a low-cost 64-bit RISC processor that runs at 133 MHz and can execute 175 dhrystone MIPS. The RV4640 can connect to a System Interface Chip for I/O (a specialized integrated circuit designed to facilitate communication between a computer system and its external devices) and memory control. The IDT77903 ATM card, which costs less than $100 (US), is a NIC with a PCI bus interface that is capable of full duplexing at 25 Mbps. Using the IDT77903 ATM card as a network interface in our device will make it easy to implement high-speed videoconferencing and multimedia applications.",
        "metrics": {
          "SARI": 50.118432850534354,
          "BLEU": 0.28646585604276165,
          "FKGL": 12.1
        }
      },
      {
        "source": "Dr. Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework, which serves as the foundation for the NET natural user interface. The paper discusses MiPadu0027s design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates compelling user advantages for wireless personal digital assistant (PDA) devices, MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in todayu0027s PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built in roller to navigate, the tapping action narrows the number of possible instructions for spoken word understanding. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition is performed. The Dr Who CSR engine uses a unified CFG and n-gram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager.",
        "prediction": "Dr. Who is a Microsoft research project that aims to create a speech-focused system allowing multiple types of interactions. This system forms the base for the NET natural user interface. The paper discusses the design of MiPad, the ongoing implementation work, and an early user study compared to the existing pen-based personal digital assistant (PDA) interface. MiPad is the application prototype showing significant benefits for wireless PDA devices. It fully combines continuous speech recognition (CSR) and spoken language understanding (SLU) so that users can perform many common tasks using a multimodal interface and wireless technologies. MiPad addresses the issue of using small styluses or tiny keyboards in today's PDAs. Unlike a cellular phone, MiPad does not rely solely on speech. It has a built-in microphone that activates when a user selects a field. When navigating by tapping the screen or using a built-in roller, the tapping action helps narrow down the possible spoken instructions. MiPad operates on a Windows CE Pocket PC with a Windows 2000 computer where the speech recognition takes place. The Dr. Who CSR engine uses a combined context-free grammar (CFG) and n-gram language model. The Dr. Who SLU engine is based on a strong chart parser and a plan-based dialog manager.",
        "reference": "Dr. Who is a Microsoft research project that aims to create a speech-focused system for interacting with technology. This system will serve as the foundation for a natural user interface called NET. The paper discusses MiPad's design and implementation, which are still in progress, as well as a preliminary user study comparing it to existing pen-based PDA interfaces. MiPad is a prototype application that shows how wireless personal digital assistants (PDAs) can benefit from integrating continuous speech recognition (CSR) and spoken language understanding (SLU) technologies. This allows users to perform common tasks using a multimodal interface and wireless technology. It aims to solve the issue of having to use small styluses or type on tiny keyboards on modern PDAs. MiPad does not rely solely on speech interaction, unlike a cellular phone. MiPad has a built-in microphone that turns on when a field is selected. When a user taps the screen or utilizes a built-in roller for navigation, this action reduces the range of potential instructions for spoken word comprehension. MiPad is currently operating on a Windows CE Pocket PC, with speech recognition being carried out on a Windows 2000 machine. The Windows CE Pocket PC is a PDA that runs the Windows Embedded Compact operating system. The Dr Who CSR engine combines a CFG and an n-gram language model. The Dr Who SLU engine uses a chart parser and a plan-based dialog manager.",
        "metrics": {
          "SARI": 55.9960608252014,
          "BLEU": 0.33508930499994105,
          "FKGL": 11.1
        }
      },
      {
        "source": "Little is known about how viable new mobile applications emerge in complex commercial environments. Research in the sociology of technology suggests that the evolution of new applications is a process of social interaction between multiple constituencies, aiming to create a common definition of mobile technology problems and solutions that is obvious only in retrospect. This theory is illustrated by the early evolution of the personal digital assistant (PDA). According to this view, established technological communities attempt to impose their own definitions of key problems and solutions on new mobile technology, thereby forcing newer players to provide a \"total system\" that completely redefines a new application area.",
        "prediction": "Little is known about how new mobile applications become successful in complicated business settings. Research in the sociology of technology, which studies the social aspects of technology, suggests that developing new applications involves interactions among various groups. These groups work together to establish a shared understanding of the issues and solutions related to mobile technology, which may seem clear only after the fact. This idea is shown through the early development of the personal digital assistant, or PDA, which is a handheld device that combines various functions like a calendar, contacts, and tasks. According to this view, established technology groups try to enforce their definitions of important problems and solutions on new mobile technologies. This pressure leads newer companies to create a comprehensive system that completely changes the understanding of a particular application area.",
        "reference": "We don't know much about how new mobile apps succeed in complicated business environments. Research in the sociology of technology indicates that the development of new applications involves social interaction among different groups. These groups work together to establish a shared understanding of mobile technology challenges and solutions, which often becomes clear only in hindsight. The PDA's early evolution exemplifies this theory. This idea suggests that existing technology groups try to define the problems and solutions for new mobile tech, which puts pressure on new players to create a total system that redefines the application area.",
        "metrics": {
          "SARI": 55.077025062244445,
          "BLEU": 0.22818857605139967,
          "FKGL": 15.5
        }
      },
      {
        "source": "In the ubiquitous environment, mobile terminals like personal digital assistant or cellular phone are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. Extracting images and text from Web content, u-PaV generates a Macromedia Flash file, which is a popular multimedia format. Although the presentation of Web content depends on the terminal type, the Web interface of mobile terminals is designed almost the same as that of personal computers. However, comparing mobile and desktop terminals, some difference in device characteristics, such as screen resolution or sound faithfulness, affect the presentation on the terminal. Moreover, when changing the access style between interactive and passive, the presentation should be adjusted for the terminal type because the lacking information is usually not perceived after transformation. To adjust the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content by the logotype of the keyword. Third, we segment the picture of the content into pieces and present one after another using visual effects. These procedures can be applied to any type of terminal.",
        "prediction": "In the everywhere environment, mobile devices like personal digital assistants or cell phones are used to access the Web. We created procedures in a ubiquitous/universal passive viewer system (u-PaV) that changes Web content into a form that is easy to watch, especially for mobile devices. By extracting images and text from Web content, u-PaV makes a Macromedia Flash file, which is a common multimedia format. Although the way Web content looks depends on the type of device, the Web interface for mobile devices is designed almost the same as for personal computers. However, when we compare mobile and desktop devices, some differences in device characteristics, like screen resolution or sound clarity, impact how content is displayed on the device. Additionally, when switching from an interactive to a passive viewing style, the display must be adjusted for the device type because missing information is often not noticed after conversion. To modify the viewing style of Web content to fit the device type, we developed a method for visual presentation. First, using keyword analysis, we show the emotional aspect of a Web page by changing the background color of the screen. Second, we highlight the main subject of the content using the logo of the keyword. Third, we break the images from the content into parts and show them one after another using visual effects. These methods can be used on any type of device.",
        "reference": "In the ubiquitous environment, devices like personal digital assistants or cell phones are used to access the Internet. We created a system called ubiquitous/universal passive viewer (u-PaV) that converts Web content into a format suitable for easy viewing on mobile devices. u-PaV takes text and images from the Web and creates a multimedia file in Macromedia Flash format, a widely used multimedia format. Mobile devices and personal computers have similar Web interfaces, although they may look different depending on the device. Mobile and desktop terminals have some differences in device characteristics, such as screen resolution and sound quality, which can affect how content is presented on the device. When switching between interactive and passive access styles, the presentation needs to be adjusted for the device type because some information may be lost during transformation and not noticed by the user. We developed a visual presentation method to customize how Web content is displayed based on the device type. First, we analyze keywords to capture the emotional tone of a web page and adjust the screen's background color accordingly. Then, we use the keyword's logo to highlight the main subject of the content. Finally, we divide the content's image into sections and present them one by one using visual effects. These steps can be used on any type of device.",
        "metrics": {
          "SARI": 51.64757720719787,
          "BLEU": 0.2735173888634322,
          "FKGL": 11.6
        }
      },
      {
        "source": "We have developed two-way location aware system, named as “The Beijing Explorer”, which exchanged positioning information and users’ situation to one another using a PDA (Personal Digital Assistant) with built-in wireless LAN and a GPS (Global Positioning System) receiver in real time. Users can see their position and their chats on the screen of a PDA using the system real-timely. The system was used for the guidance of the Palace Museum (Forbidden City) in Beijing, China. We carried on experiments two times using the system. The results of experiments show that the service using positioning data and sharing contents were valuable and interesting. The correct positioning information is important for the guidance system.",
        "prediction": "We have developed a two-way location-aware system named “The Beijing Explorer” which exchanges positioning information and users’ situations with one another using a PDA (Personal Digital Assistant) that has a built-in wireless LAN (Local Area Network) and a GPS (Global Positioning System) receiver in real time. Users can see their position and their chats on the screen of a PDA in real time. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system. The results show that the service using positioning data and sharing content was valuable and interesting. Correct positioning information is important for the guidance system.",
        "reference": "We created a location-aware system called \"The Beijing Explorer\" that uses a PDA with wireless LAN and GPS to exchange positioning and user information in real-time. Users can view their position and chats in real-time on the PDA screen when using the system. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system. The experiment results indicate that the service was useful and engaging because it shared positioning data and content. Accurate positioning information is crucial for the guidance system.",
        "metrics": {
          "SARI": 54.25425372606563,
          "BLEU": 0.3564138099594458,
          "FKGL": 11.7
        }
      },
      {
        "source": "A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. It is personal and private in an unprecedented manner, and its modus operandi is such that network and power failures will not be rare. Designing distributed systems where PDAs are treated as \"first class citizens\" is a  private :[59],\"assets (electronic money, keys for authentication and opening doors) will be stored in PDAs. Ownership and control of these :[59],\"assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private  :[105],\"introduce the \"open-ended argument\" to describe the design strategy we used for designing a system that is designed to reveal information to the user (as opposed to hide it). We argue and show that when systems are designed this way, the user (a human) is better able to control the system and his personal data, as he can make better decisions than the system itself based on qualitative assessment of the provided information. The system we have designed and implemented under this design guidelines is presented and discussed.",
        "prediction": "A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. A PDA is a handheld device that helps individuals manage personal information and tasks, functioning like a small computer. It is personal and private in an unprecedented manner, and its way of working means that failures in network connections and power supply will not be uncommon. Designing distributed systems where PDAs are treated as important components means that electronic assets, like digital money and keys for authentication and unlocking doors, will be stored in PDAs. Ownership and control of these assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private use. We introduce the \"open-ended argument\" to describe the design strategy we used for creating a system that reveals information to the user instead of hiding it. We argue and show that when systems are designed this way, the user can manage the system and their personal data more effectively, as they can make better decisions than the system itself based on their understanding of the provided information. The system we have designed and implemented under this design guideline is presented and discussed.",
        "reference": "A Personal Digital Assistant (PDA) used as a private machine, is fundamentally distinct from traditional machines because it is a personalized device. It offers an unprecedented level of personal and private experience, and its operation is designed to handle network and power failures, which are expected to occur frequently. In designing distributed systems, PDAs are given high importance and treated as \"first class citizens\". This means that important assets like electronic money and authentication keys will be stored in PDAs. The user should retain ownership and control of these assets and the media that stores and communicates them. This principle should be incorporated into the design of private systems. We introduced the concept of the \"open-ended argument\" to describe our design strategy, which focuses on creating systems that are intended to provide information to the user rather than hiding it. We argue and demonstrate that when systems are designed in this manner, the user (a human) gains improved control over both the system and their personal data. By enabling users to make decisions based on qualitative assessment of the information provided, they are better equipped to make informed choices compared to relying solely on the system's automated decisions. We have created a system based on the design guidelines and we will explain it.",
        "metrics": {
          "SARI": 44.79514921227004,
          "BLEU": 0.2464379122375108,
          "FKGL": 13.6
        }
      },
      {
        "source": "Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the development and initial evaluation of a personal digital assistant (PDA)-based decision support system (DSS) for pediatric depression screening in ages 8 to 18 years of age by pediatric advanced practice nurse (APN) students. Three aspects are described: selection of depression screening instrument; integration of the instrument into the PDA; and quantitative (usage) and qualitative (focus group) evaluation. Only one third of eligible patients were screened. Twenty percent of those screened were identified as at risk for mood disorder. The barriers to screening identified through focus groups included a lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, as well as perceived or real cultural barriers. Suggestions for educational, research, and interventions to integrate clinical based PDA-based screening are discussed.",
        "prediction": "Depression is often not recognized in many medical settings for children and adolescents. This paper aims to explain the creation and initial assessment of a personal digital assistant (PDA)-based decision support system (DSS) designed for screening depression in children ages 8 to 18, specifically by pediatric advanced practice nurse (APN) students. The paper discusses three main areas: choosing the right tool to screen for depression, incorporating that tool into the PDA, and evaluating the system both quantitatively (by examining usage data) and qualitatively (through focus group discussions). Only one-third of eligible patients were screened for depression. Of those screened, 20% were found to be at risk for mood disorders. The focus groups revealed several barriers to screening, including lack of time, insufficient knowledge, unclear intervention guidelines, lack of referral resources, issues with the PDA's usability, varying levels of motivation and comfort among teachers, and perceived or actual cultural barriers. The paper also suggests ways to improve education, research, and interventions for using PDA-based screening in clinical settings.",
        "reference": "Depression often goes unnoticed in many pediatric settings. This paper aims to explain the creation and early assessment of a decision support system (DSS) that uses personal digital assistants (PDAs) for screening pediatric depression among children aged 8 to 18 years, carried out by pediatric advanced practice nurse (APN) students. Three aspects are discussed: choosing a depression screening tool, incorporating the tool into the PDA, and evaluating it through quantitative (usage) and qualitative (focus group) assessments. Only one third of eligible patients were screened. 20% of the screened patients were found to be at risk for a mood disorder. Barriers to screening were identified through focus groups, including lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, and cultural barriers. The paper discusses suggestions for integrating PDA-based screening into clinical practice through education, research, and interventions.",
        "metrics": {
          "SARI": 51.34824584297885,
          "BLEU": 0.32567155043434315,
          "FKGL": 15
        }
      },
      {
        "source": "This study explored the potential of the application of wireless and mobile computing technologies to be used in improving the efficiency of patient care and education and future developments in information and communication technologies to support healthcare professionals and medical students in healthcare research, medical education and training. The design used for this study was a systematic review of published materials obtained from EMBASE and MEDLINE online databases, and the Cochrane Library database, including personal observations. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistant with expected growth of more than 75% by year-end 2007. In addition, wireless and mobile computing technologies allows Personal Digital Assistant to connect directly to networks or the Internet. Studies relating to processes of patient care and should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies is only beginning to take its first step in improving patient care and education. They have shown a positive impact on patient safety, health care efficiency, and ultimately patient satisfaction.",
        "prediction": "This study explored how wireless and mobile computing technologies can improve patient care and education, as well as future developments in information and communication technologies to assist healthcare professionals and medical students in healthcare research, medical education, and training. The study design was a systematic review, which is a method that collects and evaluates published materials from online databases like EMBASE and MEDLINE, as well as the Cochrane Library database and personal observations. Today, over 50% of healthcare professionals and medical students use Personal Digital Assistants, which are handheld devices that help manage information, and this number is expected to grow by more than 75% by the end of 2007. Additionally, wireless and mobile computing technologies allow Personal Digital Assistants to connect directly to networks or the Internet. Studies focused on patient care processes should assess mobile computing technologies as potential tools that can save time. Wireless and mobile computing technologies are just starting to enhance patient care and education. They have demonstrated a positive impact on patient safety, healthcare efficiency, and ultimately patient satisfaction.",
        "reference": "This study examined how wireless and mobile computing technologies can enhance patient care and education. It also explored how advancements in information and communication technologies can support healthcare professionals and medical students in research, education, and training. We conducted a systematic review for this study, analyzing published materials from databases such as EMBASE, MEDLINE, and the Cochrane Library, as well as personal observations. Over 50% of healthcare professionals and medical students currently use Personal Digital Assistants, and this number is expected to grow by more than 75% by the end of 2007. Wireless and mobile computing technologies enable Personal Digital Assistants to connect directly to networks or the Internet. Studies should examine whether mobile computing technologies can save time in patient care processes. Wireless and mobile computing technologies are still in the early stages of improving patient care and education. These technologies have positively impacted patient safety, healthcare efficiency, and patient satisfaction.",
        "metrics": {
          "SARI": 65.14878958539056,
          "BLEU": 0.4699329981705619,
          "FKGL": 16.6
        }
      },
      {
        "source": "Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore practical training is an effective learning opportunity for developing the practical nursing care ability of nursing students. Moreover, at hospitals, which are important training facilities, with regard to medical safety, the use of learning tools that produce electrical waves is not possible. So, we created a learning support environment that facilitates the imagination of nursing techniques, and enables effective preparation, review, and learning at anytime and anywhere using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named \"digital nursing dictionary\" that we developed and the evaluation of the practices using it.",
        "prediction": "Nursing students can learn many important skills through practical training by experiencing real medical practice and interacting with patients. Practical training is a valuable opportunity for developing nursing care skills. Hospitals are key places for this training, but due to medical safety concerns, using learning tools that create electrical waves is not allowed. Therefore, we established a supportive learning environment that encourages creativity in nursing techniques and allows for preparation, review, and learning anytime and anywhere using a portable digital assistant (PDA) device for practical training. In this paper, we outline the educational materials we developed called the \"digital nursing dictionary\" and evaluate the use of these materials.",
        "reference": "Nursing students can learn through practical training and by interacting with patients in real medical settings. Practical training is effective for developing the practical nursing care ability of nursing students. Moreover, hospitals, which are important training facilities, do not allow the use of learning tools that produce electrical waves due to medical safety concerns. We developed a learning support environment that helps nursing students visualize and practice techniques. It allows for convenient preparation, review, and learning using a portable digital assistant (PDA) device, enabling anytime and anywhere practical training. We developed an educational tool called the 'digital nursing dictionary' and evaluated its effectiveness, as outlined in this paper.",
        "metrics": {
          "SARI": 46.49219581352077,
          "BLEU": 0.27053763298538697,
          "FKGL": 15.3
        }
      },
      {
        "source": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. Manual reconstruction of web pages for these devices would ease the problem; however, the large variation of display capabilities will greatly increase the burden of web page designers since they have to customize a web page for each possible display device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "prediction": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a Personal Digital Assistant (PDA) screen to present. The same problem exists for devices with low resolution such as WebTV. Manually changing web pages for these devices would help ease the problem; however, the large variation of display capabilities will greatly increase the burden of web page designers since they have to customize a web page for each possible display device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "reference": "Small screens and low resolutions on mobile devices are a major obstacle for the widespread adoption of web applications in a wireless environment. PDA screens are often too small to display entire web pages. Low-resolution devices like WebTV face the same problem. Manually reconstructing web pages for these devices would help, but the wide range of display capabilities would make it challenging for web page designers as they would have to customize each web page for every possible display device, increasing their workload. This paper suggests a system for segmenting and presenting documents. The system divides a web document into logical segments automatically, using information about the screen size, document structure, and content. The system also extracts summaries and overviews to make navigation easier. The system optimizes the use of the screen for information finding by showing the segments and structure of a web document.",
        "metrics": {
          "SARI": 44.90862433215144,
          "BLEU": 0.2392994922913673,
          "FKGL": 12.5
        }
      },
      {
        "source": "Take 8-bit microcontroller (C8051F005) as the nucleus and study a kind of voltage harmonic monitor device that satisfies the long-range monitor. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). This device has two work modes: ”native” and ”remote”. It also can communicate with monitor center through telephone line, serial port, IC card, PDA (Personal Digital Assistant), etc.",
        "prediction": "Take an 8-bit microcontroller called C8051F005 as the main component and study a type of device that monitors voltage harmonics for long-distance tracking. The method used for harmonic analysis is called Fast Fourier Transform (FFT), which is a mathematical algorithm to analyze different frequencies in a signal. This device has two modes of operation: \"native,\" which likely means it functions independently, and \"remote,\" which suggests it can operate from a distance. It can communicate with a monitoring center using various methods like telephone lines, serial ports, Integrated Circuit (IC) cards, and Personal Digital Assistants (PDAs), which are handheld devices that help manage personal information.",
        "reference": "Study a voltage harmonic monitor device for long-range monitoring using the 8-bit microcontroller C8051F005 as the core. The harmonic analysis uses FFT. The device has two modes: \"native\" and \"remote\". It can communicate with a monitoring center via different methods such as telephone line, serial port, IC card, and PDA (Personal Digital Assistant).",
        "metrics": {
          "SARI": 42.417615044687196,
          "BLEU": 0.14398607988251347,
          "FKGL": 15.8
        }
      },
      {
        "source": "With network and small screen device improvements, such as wireless abilities, increased memory and CPU speeds, users are no longer limited by location when accessing on-line information. We are interested in studying the effect of users switching from a large screen device, such as a desktop or laptop to use the same web page on a small device, in this case a PDA (Personal Digital Assistant). We discuss three common transformation approaches for display of web pages on the small screen: Direct Migration, Linear and Overview. We introduce a new Overview method, called the Gateway, for use on the small screen that exploits a user’s familiarity of a web page. The users in an initial study prefer using the Gateway and Direct Migration approach for web pages previously used on the large screen, despite the common Linear approach used by many web sites.",
        "prediction": "With improvements in network and small screen devices like wireless capabilities, more memory, and faster CPU (Central Processing Unit) speeds, users can access online information without being restricted by their location. We are studying how users change from a large screen device, such as a desktop computer or laptop, to a small device, specifically a PDA (Personal Digital Assistant), while using the same web page. We discuss three common ways to transform the display of web pages for small screens: Direct Migration, Linear, and Overview. We introduce a new Overview method called the Gateway for small screens, which takes advantage of a user’s familiarity with a web page. In an initial study, users preferred the Gateway and Direct Migration approaches for web pages they had previously used on large screens, even though many websites commonly use the Linear approach.",
        "reference": "Improvements in network and small-screen devices, such as wireless capabilities, faster CPU speeds, and increased memory, have eliminated location restrictions for accessing online information. We want to study the impact of users switching from a desktop or laptop to a PDA (Personal Digital Assistant) when accessing the same web page. We are looking at three common ways to display web pages on small screens: Direct Migration, Linear, and Overview. Gateway is our new Overview method for small screens that use the user's familiarity with a web page. Many websites use the Linear method, but an initial study showed that users prefer using the Gateway and the Direct Migration method for web pages.",
        "metrics": {
          "SARI": 52.5010460105925,
          "BLEU": 0.27793746345163006,
          "FKGL": 14.1
        }
      },
      {
        "source": "The limitations and constraints of mobile systems need to be adequately addressed in software development. We have been developing a taxonomy of risks based on SEIu0027s risk questionnaire and applied it during the development of a negotiation support system for a Personal Digital Assistant (PDA). In our planned research, we will explore how we can better integrate existing risk management strategies and Agile Methods (AM).",
        "prediction": "The limitations and constraints of mobile systems need to be adequately addressed in software development. We have been developing a classification system of risks based on the Software Engineering Institute's risk questionnaire and applied it during the development of a negotiation support system for a Personal Digital Assistant, which is a handheld device that combines computing, telephone, and internet capabilities. In our planned research, we will explore how we can better integrate existing risk management strategies, which are methods to identify and minimize risks, and Agile Methods, which are approaches to software development that prioritize flexibility and customer satisfaction.",
        "reference": "During software development, restrictions due to mobile systems need to be addressed. Based on SEI's risk questionnaire, we developed a classification of risks and applied it while developing a negotiation support system for a PDA. Risk management strategies and Agile Methods and how we can integrate them better will be studied in our future research.",
        "metrics": {
          "SARI": 45.7540168226017,
          "BLEU": 0.14027738511433413,
          "FKGL": 19.7
        }
      },
      {
        "source": "Expansions of mobile services and private data have required increased level of protection. Speaker recognition, one of the biometric technologies, arises lots of research interests for its simple, cheap and convenient characteristics. In this paper, a robust speaker recognition system which facilitates reliable authentication with multi-channel voices is presented. A large multi-channel corpus, including mobile phone, personal digital assistant (PDA), telephone and microphone, is collected to evaluate the system performance.",
        "prediction": "Expansions of mobile services and private data have required an increased level of protection. Speaker recognition, which is one of the biometric technologies, has generated a lot of research interest because it is simple, cost-effective, and convenient. In this paper, a strong speaker recognition system that enables reliable authentication using voices from multiple sources is presented. A large collection of audio recordings from various sources, including mobile phones, personal digital assistants (PDAs), telephones, and microphones, is gathered to assess the performance of the system.",
        "reference": "The growth of mobile services and private data necessitates higher levels of protection. Speaker recognition, a technology using the voice of users to identify them, leads to a lot of research interest because of its simple, cheap, and convenient characteristics. This work presents a powerful speaker recognition system that makes the authentification with multi-channel voices easier. Data are collected on mobile phones, PDAs, telephones, and microphones to evaluate the system's performance.",
        "metrics": {
          "SARI": 50.12105092947492,
          "BLEU": 0.24132786431856576,
          "FKGL": 15
        }
      },
      {
        "source": "This paper presents a method to generate unique and nevertheless highly random pseudonyms in a distributed environment. More precisely, each user can now generate his pseudonym locally in his personal security environment, e.g. in his smart card or his personal digital assistant. There is no need for any information interchange between issuing parties or global data (especially keys), except unique identifiers for each user and each device of the system. Additionally the holder can prove, that he generated a specific pseudonym without revealing his identity and he can reveal his identity by disclosing the pseudonym. Whereas the verifier of a disclosed pseudonym can be sure, that the presenter of the pseudonym is the holder of the pseudonym (i.e. the person which originally generated it). The identifier of the user and the identifier of the useru0027s device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form.",
        "prediction": "This paper presents a method to create unique and highly random pseudonyms in a distributed environment. Specifically, each user can generate their pseudonym locally in their personal security environment, such as on their smart card or personal digital assistant. There is no need for information exchange between issuing parties or global data, particularly keys, except for unique identifiers for each user and each device in the system. Additionally, the holder can prove that they generated a specific pseudonym without revealing their identity, and they can reveal their identity by disclosing the pseudonym. The verifier of a disclosed pseudonym can be sure that the person presenting the pseudonym is the holder, meaning the person who originally generated it. The identifier of the user and the identifier of the user's device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in an encrypted form.",
        "reference": "This work presents a method to generate unique and random pseudonyms in a distributed system. Every user can now create his pseudonym locally in their own personal security environment, e.g. in his chip card or PDA. No information exchange is required between issuing parties or global data, except for unique identifiers for each user and device in the system. The holder can also prove that he generated a distinct pseudonym without revealing his identity; by divulging it, he can reveal his identity. In that case, the verifier of a divulgated pseudonym can be sure that the one who presents the pseudonym is holding it. (i.e. the person that originally created it) The user identifier and device's identifier will be used to create unique pseudonyms but to ensure anonymity with pseudonyms, both identifiers will be stored within the pseudonym in encrypted form.",
        "metrics": {
          "SARI": 45.43773027634412,
          "BLEU": 0.3076750108040455,
          "FKGL": 15.5
        }
      },
      {
        "source": "In todayu0027s mobile information society, location-based services play an increasingly important role. These services are to be accessed by users with a mobile end device for the use of city maps, route planning, navigation, or traffic information. Mobile end devices, however, do not have computing power or storage capacity comparable to that of a personal computer or laptop. These deficits can be bypassed by employing special methods in development of the respective applications. This paper outlines the experience gained in the development of a prototype for route computation in public transport networks to be used on a personal digital assistant. Subdivision of the route computation process into various steps for minimizing the computation expenditure on the mobile end device plays a significant role.",
        "prediction": "In today's mobile information society, location-based services are becoming very important. These services can be accessed by users with a mobile device for things like city maps, route planning, navigation, or traffic information. Mobile devices, however, do not have the same computing power or storage capacity as a personal computer or laptop. These limitations can be overcome by using special methods in the development of the applications. This paper discusses the experience gained in creating a prototype for calculating routes in public transport networks to be used on a personal digital assistant, which is a type of handheld device. Breaking down the route calculation process into different steps to reduce the computing load on the mobile device is important.",
        "reference": "Location-based service's role in today's mobile information society is increasing . Users can access city maps, route planning, navigation, or traffic information with their mobile devices. Compared to laptops or personal computers, mobile devices do not have the same computing power or storage capacity. This shortage can be overcome by using specific methods during the development of the respective application. This paper shows the gained experience from developing a prototype designed for a PDA that calculates routes in public transport networks. Dividing the route computation process into multiple steps to reduce the computational load on the mobile device is crucial.",
        "metrics": {
          "SARI": 56.727789327416154,
          "BLEU": 0.3212598038102539,
          "FKGL": 13.4
        }
      },
      {
        "source": "DIANE is a digital assistant system that aims to fasten the doctor access to various informations at the hospital such as health care facility, medical records, and also human resource data. The fasten access could be achieved by implementing face recognition and live streaming as part of the digital assistant system.",
        "prediction": "DIANE is a digital assistant system designed to speed up doctors' access to various information at the hospital. This includes data about health care facilities, medical records, and human resource information. The faster access can be achieved by using face recognition technology and live streaming as part of the digital assistant system.",
        "reference": "DIANE is a digital assistant system that allows the doctor a faster access patient and hospital information. This could be done with face recognition and live streaming.",
        "metrics": {
          "SARI": 44.90047089215563,
          "BLEU": 0.13826777579228938,
          "FKGL": 12.4
        }
      },
      {
        "source": "Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time.",
        "prediction": "Digital assistants are becoming more common in our everyday lives. When using these assistants, users can perform several tasks in a short amount of time.",
        "reference": "Digital assistants are becoming popular in our daily lives. It will allow users to do multiple tasks in a faster way.",
        "metrics": {
          "SARI": 43.5014179726162,
          "BLEU": 0.1535259783865636,
          "FKGL": 8.2
        }
      },
      {
        "source": "As extensive experimental research has shown individuals suffer from diverse biases in decision-making. In our paper we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, managerial decisions which are based on different levels of organizational complexity and different incentive systems suffer from biases known from descriptive decision theory. The results illustrate how biases in combination with each other and in different organizational contexts affect organizational performance. We find that, contrary to intuition, some combinations of biases significantly improve organizational performance while these biases negatively affect organizational performance when they occur separately. This might evoke considerations whether decision-making should be as rational as possible.",
        "prediction": "As extensive experimental research has shown individuals experience various biases in decision-making. In our paper we analyze how decision-making biases of managers in group decision processes impact organizational performance. In the simulations managerial decisions that are based on different levels of organizational complexity and different incentive systems exhibit biases recognized from descriptive decision theory. The results demonstrate how biases interacting with each other and in various organizational contexts influence organizational performance. We find that, contrary to what one might expect, some combinations of biases significantly enhance organizational performance while these biases harm organizational performance when they occur alone. This raises questions about whether decision-making should be as rational as possible.",
        "reference": "Research showed that people suffer from diverse biases (disproportionate weight in favor of or against an idea) in decision-making. We analyzed the effects of decision-making biases of managers on organizational performance. In the simulations, managers decisions which are based on organizational skills and rules, suffer from several known biases. The results illustrate how combined biases in different contexts affect organizational performance. Contrary to expectations, some combinations of biases improve organizational performance. However, when they occur separately, they decrease performance. This raises questions about the importance of rationality in decision-making.",
        "metrics": {
          "SARI": 35.02721983042509,
          "BLEU": 0.18691623123656312,
          "FKGL": 18.7
        }
      },
      {
        "source": "Big data, the enhanced ability to collect, store and analyze previously unimaginable quantities of data in tremendous speed and with negligible costs, delivers immense benefits in marketing efficiency, healthcare, environmental protection, national security and more. The central tenets of the current privacy framework, the principles of data minimization and purpose limitation, are severely strained by the big data technological and business reality. As we increasingly interact with these artificial agents in unsupervised settings, with no human mediators, their seeming autonomy and increasingly sophisticated functionality and behavior, raises legal and philosophical questions. The focus on the machine is a distraction from the debate surrounding data driven ethical dilemmas, such as privacy, fairness and discrimination. The machine may exacerbate, enable, or simply draw attention to the ethical challenges, but it is humans who must be held accountable. Policymakers should seek to devise agreed-upon guidelines for ethical data analysis and profiling. Such guidelines would address the use of legal and technical mechanisms to obfuscate data; criteria for calling out unethical, if not illegal, behavior; categories of privacy and non-privacy harms; and strategies for empowering individuals through access to data in intelligible form.",
        "prediction": "Big data is the advanced ability to gather, store, and analyze large amounts of information at high speeds and low costs. This technology provides significant benefits in areas like marketing efficiency, healthcare, environmental protection, and national security. However, the main concepts of the existing privacy laws, which are data minimization (the principle of collecting only the data that is necessary) and purpose limitation (the principle of using data only for its intended purpose), are being challenged by the realities of big data technology and business practices. As we interact more with artificial agents (systems that can operate independently without human intervention) in situations where there is no human oversight, their apparent independence and increasingly complex behaviors raise legal and philosophical issues. Focusing on the technology can distract us from important discussions about the ethical issues that arise from data usage, including privacy (the right to keep personal information secret), fairness (equal treatment without discrimination), and discrimination (unfair treatment based on certain characteristics). While technology can highlight or worsen these ethical problems, it is ultimately humans who should be held responsible. Policymakers are encouraged to create agreed-upon guidelines for ethical data analysis and profiling. These guidelines should include methods for hiding data, identifying unethical or illegal actions, types of privacy and non-privacy harms, and ways to empower individuals by providing them with their data in a clear and understandable format.",
        "reference": "Big data (the ability to collect, store and analyze huge quantities of data in a really fast and cheap way) offers immense benefits in marketing, healthcare, environment, national security and more. The principles of privacy, limitation of data collection and use are stretched by the business and technological reality of big data. Our increasing interactions with these technologies without human supervision, and their growing autonomy, functionality and behavior raises legal and philosophical questions. The focus on the machine is a distraction from the debate on ethical dilemmas about data like privacy, fairness and discrimination. The machine may influence the ethical challenges, but the humans must remain responsible. Responsible people should find guidelines for ethical data analysis and collection. These guidelines would deal with mechanisms to protect data, criteria about unethical or illegal behavior, privacy harms, and strategies to give people access to data.",
        "metrics": {
          "SARI": 37.69711412529306,
          "BLEU": 0.07149141061486189,
          "FKGL": 19.2
        }
      },
      {
        "source": "Amazon's Echo, and Apple's Siri have drawn attention from different user groups; however, these existing commercial VUIs support limited language options for users including native English speakers and non-native English speakers. we conducted a usability study of the Google Home Smart Speaker with 20 participants including native English and non-native English speakers to understand their differences in using the Google Home Smart Speaker. The findings show that compared with their counterparts, the native English speakers had better and more positive user experiences in interacting with the device. It also shows that users' English language proficiency plays an important role in interacting with VUIs. The findings from this study can create insights for VUI designers and developers for implementing multiple language options and better voice recognition algorithms in VUIs for different user groups across the world.",
        "prediction": "Amazon's Echo and Apple's Siri have caught the attention of different user groups but these commercial Voice User Interfaces or VUIs have limited language options for both native English speakers and non-native English speakers. We conducted a usability study of the Google Home Smart Speaker with 20 participants including native and non-native English speakers to understand their differences in using the Smart Speaker. The findings show that native English speakers had better and more positive experiences when interacting with the device compared to non-native speakers. It also indicates that users' proficiency in the English language is important in interacting with VUIs. The insights from this study can help VUI designers and developers create multiple language options and improve voice recognition algorithms for different user groups worldwide.",
        "reference": "The VUIs (voice user interface) like Amazon’s Echo or Apple’s Siri are popular nowadays. However, they have limited language options for users. We studied the usability of the Google Home Smart Speaker with native and non-native English speakers to understand their differences in using this device. The results show that the native English speakers had a better experience with the device than the non-native. It also shows that language skills play an important role. The results of this study can help VUI developers for improving language options and voice recognition in the devices.",
        "metrics": {
          "SARI": 46.28002729845812,
          "BLEU": 0.25434569395522494,
          "FKGL": 15.5
        }
      },
      {
        "source": "the smart speakers cannot distinguish human voice from machine voice. a method to identify which of human or machine is sending voice commands to a smart speaker is desired. to prevent such machine-voice based attacks to a smart speaker in absence of residents, we propose a system consisting of a speaker and a microphone array to detect the existence of a human nearby, supposing it can be incorporated in a smart speaker in the future.",
        "prediction": "The smart speakers cannot tell the difference between a human voice and a machine voice. A method is needed to identify whether a human or a machine is giving voice commands to a smart speaker. To prevent attacks that use machine voices when there are no residents at home, we propose a system made up of a speaker and a microphone array. This system would be able to detect the presence of a human nearby and could potentially be built into a smart speaker in the future.",
        "reference": "The smart speakers cannot differentiate between human voice and machine voice. A method to identify if the voice command is from a human or a machine is required. We propose a system composed of a speaker and microphones to detect the presence of humans. It could help to prevent attacks on a smart speaker with a machine voice in absence of residents",
        "metrics": {
          "SARI": 52.867177876431356,
          "BLEU": 0.2399395541601825,
          "FKGL": 9.4
        }
      },
      {
        "source": "As smart speakers with voice interaction capability permeate continuously in the world, more and more people will gradually get used to the new interaction medium–voice. Although speech recognition, natural language processing (NLP) have been greatly improved over the past few years, users still may encounter errors from time to time like “cannot understand”, “no requested audio resource (such as music)”, which can frustrate users. Therefore, when an error message is reported, it is vital that the smart speaker gives an effective and proper response. However, currently the response strategies adopted by leading smart speaker brands in China differed mainly on two dimensions: “apology or not” and “humor or neutral”. We explored user’s preference of response strategies under two error scenarios——“cannot understand” and “no requested audio resource”. Two dependent variables (satisfaction and perceived sincerity of response) were measured. The results showed that participants were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no significant impact on the perception of satisfaction and sincerity. But in the “cannot understand” scenario, humorous expression decreased perceived sincerity.",
        "prediction": "As smart speakers with voice interaction capability become more common, more people are getting used to using their voices to interact with devices. Although technology for understanding speech and processing natural language (NLP) has improved significantly in recent years, users may still face errors like not being understood or not finding a requested audio resource, such as music. These errors can be frustrating for users. Therefore, it is important for smart speakers to respond effectively when an error occurs. Currently, the response strategies used by major smart speaker brands in China differ mainly in two ways: whether to apologize or not and whether to use humor or be neutral. We investigated user preferences for response strategies in two error situations—when the device cannot understand something and when there is no requested audio resource. We measured two dependent variables, which are satisfaction and perceived sincerity of the response. The results showed that users were more satisfied and perceived the response as more sincere when the smart speaker apologized in both error situations. In the situation of no requested audio resource, humor did not significantly affect satisfaction and sincerity. However, in the situation of not being understood, using humor reduced the perceived sincerity.",
        "reference": "Smart speakers are becoming popular around the world and people will get used to this new way of interacting with the voice. Even with the progress of speech recognition and natural language processing (interactions between computers and human language), users may still get errors like “cannot understand” or “no requested audio resource” which can frustrate users. So, it is important that the smart speaker gives an effective and proper response during an error message. The responses of the most popular brand of smart speakers are based on 2 elements, apology and humor. We studied user’s preference in two error scenarios : “cannot understand” and “no requested resource” Satisfaction of the user and perceived sincerity of the response were measured. The results showed that users were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no impact on the satisfaction or sincerity. But in the “cannot understand” scenario, humor decreased perceived sincerity.",
        "metrics": {
          "SARI": 39.30754718948947,
          "BLEU": 0.16871850716163028,
          "FKGL": 13.5
        }
      },
      {
        "source": "A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process (specifically, drug molecules, compounds in high throughput screening libraries, combinatorial chemistry building blocks and nondrug molecules). Significant differences were observed between the pharmacophore profiles obtained for the drug molecules and those obtained for the high-throughput screening compounds, which appear to be closely related to the nondrug pharmacophore distribution. It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the property-based optimization of compound selection and library design processes, thus improving the odds of success in lead discovery projects.",
        "prediction": "A pharmacophore analysis approach was used to study and compare various types of compounds important for the drug discovery process. This includes drug molecules, compounds used in high-throughput screening libraries, building blocks from combinatorial chemistry, and nondrug molecules. Significant differences were noted between the pharmacophore profiles of drug molecules and those from high-throughput screening compounds, which seem to be closely related to the distribution of nondrug pharmacophores. It is suggested that examining pharmacophore profiles could serve as an extra tool for optimizing the selection of compounds and designing libraries, thereby increasing the chances of success in lead discovery projects.",
        "reference": "A pharmacophore analysis investigated and compared different compounds of the drug discovery process. Significant differences were observed between the pharmacophore profiles of the drug molecules and the high-throughput screening compounds, which appear to be related to the nondrug pharmacophore distribution. It is suggested that the analysis could be used as an additional tool for the optimization of compound selection and library design processes..",
        "metrics": {
          "SARI": 36.3459200516808,
          "BLEU": 0.20678338968804316,
          "FKGL": 15.3
        }
      },
      {
        "source": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been previously designed for this express purpose, and provides a layered hierarchy of meaningful terms which can be linked to.",
        "prediction": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions readable by machines. Protocols are communicated using concise scientific English, which limits the ability of software algorithms to analyze them. With a detailed ontology, which is a formal representation of knowledge as a set of concepts within a domain and the relationships between those concepts, important information can be captured by stating a series of facts, organized as semantic web triples, which consist of a subject, a predicate, and an object. With proper annotation, assays can be searched, grouped, tagged, and evaluated in various ways, similar to other areas of drug discovery informatics. The BioAssay Ontology, known as BAO, was specifically designed for this purpose and offers a structured hierarchy of meaningful terms that can be referenced.",
        "reference": "Annotation of bioassay protocols with web vocabulary can make experiment descriptions machine-readable. Protocols use concise scientific English which prevents most of analysis by software algorithms. There is a sufficient ontology, so the pertinent information can be written using semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated. The BioAssay Ontology (BAO) has been designed for this purpose, and provides many meaningful terms.",
        "metrics": {
          "SARI": 36.17589494602127,
          "BLEU": 0.15134895353504818,
          "FKGL": 15.9
        }
      },
      {
        "source": "Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Subsequent bioinformatic analysis is typically done with the help of open source tools, where each application performs a single step towards the final result. This situation leaves the bioinformaticians with the tasks to combine the tools, manage the data files and meta-information, document the analysis, and ensure reproducibility.",
        "prediction": "Next generation sequencing (NGS) is a method that produces large amounts of data made up of billions of individual sequences, called reads, and can analyze up to thousands of samples at once. After sequencing, the analysis of this data is usually performed with open source tools, which are software programs that anyone can use and modify. Each of these tools carries out a specific task in the overall analysis process. This means that bioinformaticians, who are scientists that use computer technology to analyze biological data, must combine the different tools they used, manage the data and related information about it, keep a record of their analysis process, and make sure their results can be replicated by others.",
        "reference": "Next generation sequencing (NGS) produces datasets of billions of reads and thousands of samples. The analysis is typically done with open source tools, performing a single step towards the final result. The bioinformaticians have to combine the tools, manage the files and information, document the analysis, and ensure reproducibility.",
        "metrics": {
          "SARI": 35.8188023593836,
          "BLEU": 0.1068509474931253,
          "FKGL": 14.7
        }
      },
      {
        "source": "SPSS Clementinel2.0 statistical software was used to mine the association rules between Etiology and traditional Chinese medicine (TCM), Syndromes and TCM, Symptoms and TCM. The classic Apriori algorithm is useful to mine cases of influenza treated by contemporary famous old Chinese medicine.",
        "prediction": "SPSS Clementine 2.0 is a statistical software used to find patterns in data between Etiology and traditional Chinese medicine (TCM), Syndromes and TCM, Symptoms and TCM. The classic Apriori algorithm is helpful for finding cases of influenza that are treated by well-known traditional Chinese medicine practitioners.",
        "reference": "SPSS Clementinel2.0 statistical software was used to extract data and find relations between traditional Chinese medicine and other medicine. The classic Apriori algorithm is useful to find cases of influenza treated by old Chinese medicine.",
        "metrics": {
          "SARI": 35.00145258261843,
          "BLEU": 0.1798896493363303,
          "FKGL": 11.6
        }
      },
      {
        "source": "Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype and phenotype in humans and other species. a Bayesian framework for genotype estimation for mixtures of multiple bacteria, named as Genetic Polymorphisms Assignments (GPA) has reduced the false discovery rate (FDR) and mean absolute error (MAE) in single nucleotide variant (SNV) identification.",
        "prediction": "Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool for identifying genetic variation and understanding the connections between genetic makeup (genotype) and physical traits (phenotype) in humans and other species. A Bayesian framework for estimating genotypes in mixtures of multiple bacteria, called Genetic Polymorphisms Assignments (GPA), has reduced the false discovery rate (FDR) and the mean absolute error (MAE) in identifying single nucleotide variants (SNVs).",
        "reference": "Identifying bacterias that are resistant to antimicrobial in samples of genetic material collected from the environment is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype (genetic code of an individual) and phenotype (physical characteristics of an indvidual) in humans and other species. The Genetic Polymorphism Assignements (GPA) is a framework which determines the genetic constitution of multiple bacteria while reducing the number of false positive results and improving the accuracy of calculations when identifying the variation of a single nucleotide.",
        "metrics": {
          "SARI": 44.44312785613615,
          "BLEU": 0.3025530462372524,
          "FKGL": 18.5
        }
      },
      {
        "source": "The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (altering the genetic sequence) and gene regulation (without altering the genetic sequence). These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "prediction": "The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering. Genome engineering includes genome editing, which means altering the genetic sequence, and gene regulation, which is changing how genes are expressed without altering the genetic sequence. These applications require the design of single guide RNAs, or sgRNAs, that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "reference": "The CRISPR/Cas9 system is a powerful technology for gene editing and gene regulation. These applications require the design of single guide RNAs (which are enzymes that can delete, insert or alter DNA) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "metrics": {
          "SARI": 57.970968984560024,
          "BLEU": 0.4306892955718009,
          "FKGL": 13.7
        }
      },
      {
        "source": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals; it might improve the health of individuals and communities, and, if successful, might be seen as a public good. However, enhancing future generations will require In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. Remarkably, the necessary involvement of women in an enhancing scenario has not been discussed by its proponents. The present discourse on moral obligations of future generations, although not referring to women, seems to imply that women might be required, morally, if not legally, to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. Despite the lack of a unified feminist conclusion in the discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.",
        "prediction": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Gene editing is a technology that allows scientists to change the DNA sequence of an organism, which could improve health for future generations. This improvement might align with public health goals, benefiting individuals and communities and possibly being seen as beneficial for society as a whole. However, enhancing the health of future generations will require In Vitro Fertilisation, which is a medical procedure that involves fertilizing an egg outside the body, and Pre-implantation Genetic Diagnosis, which is a technique used to identify genetic defects in embryos before implantation. Interestingly, the necessary involvement of women in this process has not been addressed by those supporting it. The current discussion about the moral responsibilities towards future generations, even though it does not specifically mention women, suggests that women may be expected, both morally and potentially legally, to undergo IVF to contribute to these enhancements. The process of improving future generations will be influenced by gender unless an artificial womb, which is a device that can support the development of embryos outside of a woman's body, is developed. These are complex issues that need to consider the perspectives of both women and men. Although there is no single feminist agreement on the benefits and dangers of altering human genes, it is urgent to clarify the role of women in this context.",
        "reference": "Gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals. However, enhancing future generations will require In Vitro Fertilisation (IVF) and Pre-implantation Genetic Diagnosis. The necessary involvement of women in an enhancing scenario has not been discussed. The discourse on moral obligations of future generations seems to imply that women might be required to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. There are no unified feminist conclusion about the merits and risks of human genome modification. There is also an urgent need to clarify the role of women in this scenario.",
        "metrics": {
          "SARI": 36.06241349744799,
          "BLEU": 0.21574060866722145,
          "FKGL": 16
        }
      },
      {
        "source": "CRISPR-Cas is a tool that is widely used for gene editing. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins, which are powerful molecules that inhibit the CRISPR-Cas system, may have the potential to promote better utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. Additionally, more in-depth research on these proteins would help researchers to better understand the co-evolution of bacteria and phages. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "prediction": "CRISPR-Cas is a tool that is widely used for editing genes. However, unexpected side effects may happen because of prolonged activity from a type of enzyme called a nuclease. Anti-CRISPR proteins are strong molecules that stop the CRISPR-Cas system from working, and they might help improve the use of the CRISPR-Cas system in gene editing, especially for treatments that involve changing genes. Furthermore, conducting more detailed research on these proteins would aid scientists in understanding how bacteria and viruses that infect them have evolved together. Therefore, it is important to gather and combine information on different kinds of anti-CRISPRs.",
        "reference": "CRISPR-Cas is a tool used for gene editing. However, unintended genetic modifications may occur. Anti-CRISPR proteins (molecules that inhibit the CRISPR-Cas system) may improve utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. More research on these proteins would help to understand the co-evolution of bacteria and bacteriophages (virus that infects and replicates within bacteria). It is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "metrics": {
          "SARI": 38.11733892938554,
          "BLEU": 0.20462596537263483,
          "FKGL": 11
        }
      },
      {
        "source": "CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its function of gene perturbation, a variety of gene editing techniques have been developed to achieve different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen.",
        "prediction": "CRISPR has become an important area of research since it was first introduced because it is effective and accurate in editing almost any part of DNA sequences. This process is based on changing genes, leading to the development of various gene editing techniques for different purposes. The specific locations within a DNA strand can be carefully cut and repaired, allowing genes to be removed, fixed, turned off, or turned on. The high effectiveness of CRISPR/Cas9 tools and the ease of conducting experiments make it a powerful method for extensive screening.",
        "reference": "CRISPR is a popular research area known for its efficiency and specificity in editing DNA sequences. A variety of gene editing techniques have been developed for different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it a powerful tool.",
        "metrics": {
          "SARI": 25.451151740220567,
          "BLEU": 0.12115656959118966,
          "FKGL": 12.1
        }
      },
      {
        "source": "The CRISPR-Cpf1 system has been successfully applied in genome editing. However, target efficiency of the CRISPR-Cpf1 system varies among different gRNA sequences. Using machine learning technology, a SVM model was created to predict target efficiency for any given gRNAs. the first web service application, CRISPR-DT (CRISPR DNA Targeting), to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and specificity is available.",
        "prediction": "The CRISPR-Cpf1 system is a tool used for editing genes, which are the instructions that tell living organisms how to grow and function. However, the effectiveness of this system can differ based on different guide RNA sequences, which are short strands of RNA that help direct the system to the correct part of the DNA. To improve this, a machine learning method was used to create a Support Vector Machine (SVM) model. This model can predict how effective different guide RNAs will be. There is now a web service called CRISPR-DT, which stands for CRISPR DNA Targeting. This service helps users create the best guide RNAs for the CRISPR-Cpf1 system by looking at both how effective they are and how specific they are to their targets.",
        "reference": "The CRISPR-Cpf1 system (a DNA-editing technology) has been successfully applied in gene editing. However target efficiency of the CRISPR-Cpf1 system varies among different gRNA (piece of RNA that serves as a guide to DNA-targeting enzymes) sequences. Using machine learning technology, a support-vector machine model (algorithm used for data classification) was created to predict the target efficiency for any given gRNAs. CRISPR-DT (CRISPR DNA Targeting) is available. It is the first web service application to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and target specificity (recognition mecanism).",
        "metrics": {
          "SARI": 22.85044615956659,
          "BLEU": 0.07372718274445492,
          "FKGL": 9.1
        }
      },
      {
        "source": "Emerging self-driving vehicles are vulnerable to different attacks due to the principle and the type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via vehicular ad hoc networks (VANETs). VANETs add new threats to self-driving vehicles that contribute to substantial challenges in autonomous systems. These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for self-driving and semi self-driving cars. The hybrid detection system relies on the Back Propagation neural networks (BP), to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.",
        "prediction": "Emerging self-driving vehicles are at risk of various attacks because of the way they communicate. These vehicles often use vehicular ad hoc networks, or VANETs, which allow vehicles to communicate with each other and with infrastructure like traffic lights. While this system is beneficial, it introduces new threats that can challenge how autonomous systems operate. The communication methods in self-driving vehicles make them susceptible to several types of harmful attacks. Some of these attacks include a Sybil attack, where one entity pretends to be multiple entities; Denial of Service, or DoS, which disrupts service; black hole attacks, which misroute data; grey hole attacks, which drop some data but not all; and wormhole attacks, which create a shortcut in the network that can be exploited. In this paper, we suggest an advanced security system aimed at protecting the communication channels used by self-driving and semi self-driving cars. Our hybrid detection system uses Back Propagation neural networks, or BP, to identify DoS attacks within VANETs. The results from our experiments indicate that the BP intrusion detection system, or BP-IDS, successfully detects malicious vehicles in both self-driving and semi self-driving cars.",
        "reference": "Self-driving vehicles are vulnerable to different attacks because of their communication systems. These vehicles use external communication via a network. This network add new threats to self-driving vehicles that contribute to the challenges in autonomous systems. With these communication systems, self-driving vehicles are vulnerable to many types of malicious attacks on the network. In this paper, we propose an intelligent security system to secure external communications for self-driving cars. The system will detect a common type of attack in the network: Denial-of-Service (attack trying to make network resource unavailable). The results show that the intrusion detection system is capable of identifying malicious vehicles in self-driving vehicles.",
        "metrics": {
          "SARI": 32.40191506922571,
          "BLEU": 0.07989288963174564,
          "FKGL": 14.8
        }
      },
      {
        "source": "Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications. When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, might become deadlocked, or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols, while ensuring safety.",
        "prediction": "Self-driving vehicle technologies are advancing quickly and will be important for future transportation. One major challenge for self-driving vehicles on public roads is ensuring safe interaction among multiple vehicles that use sensors to perceive their surroundings and communicate with each other. When self-driving vehicles try to be in the same space at the same time, they might crash into each other, get stuck in a standstill, or suddenly stop, which can be uncomfortable or unsafe for passengers. This paper examines how a self-driving vehicle can safely navigate merge points, where two lanes with different levels of priority meet. We introduce a safety procedure for these merge points called the Autonomous Vehicle Protocol for Merge Points. In this protocol, self-driving vehicles communicate with one another and use their own sensors to work together with other self-driving cars and human-driven vehicles. Our simulation results indicate that our traffic protocol allows for greater traffic flow than simpler traffic protocols, while still ensuring safety.",
        "reference": "Self-driving vehicle technologies are progressing and are expected to play a significant role in the future of transportation. A main challenge for self-driving vehicles on public roads is the safe cooperation, collaboration and communication with other vehicles. When self-driving vehicles are in the same area, they might collide, become blocked, or suddenly brake and make uncomfortable or unsafe the passengers. We study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other vehicles (self-driving or human-driven). Our simulation results show that our traffic protocol leads to a better traffic flow, while ensuring safety.",
        "metrics": {
          "SARI": 39.720449857886514,
          "BLEU": 0.2512118415163781,
          "FKGL": 14.6
        }
      },
      {
        "source": "This paper provides a holistic study of how stock prices vary in their response to financial disclosures across different topics. Thereby, we specifically shed light into the extensive amount of filings for which no a priori categorization of their content exists. For this purpose, we utilize an approach from data mining - namely, latent Dirichlet allocation - as a means of topic modeling. This technique facilitates our task of automatically categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings from U.S. companies. We then evaluate the subsequent stock market reaction. Our empirical evidence suggests a considerable discrepancy among various types of news stories in terms of their relevance and impact on financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit rating, but also for disclosures regarding business strategy, the health sector, as well as mergers and acquisitions. Our results yield findings that benefit managers, investors and policy-makers by indicating how regulatory filings should be structured and the topics most likely to precede changes in stock valuations.",
        "prediction": "This paper provides a comprehensive study of how stock prices change in response to financial disclosures on various topics. We specifically focus on the large number of filings that do not have a clear category for their content. To achieve this, we use a method from data mining called latent Dirichlet allocation, which helps us identify the topics within the data. This technique allows us to automatically categorize, in advance, the content of over 70,000 regulatory 8-K filings from companies in the United States. We then assess the following reactions in the stock market. Our findings indicate a significant difference among various types of news stories regarding their importance and effect on financial markets. For example, we observe a noticeable abnormal return in response to earnings results and credit ratings, as well as for disclosures about business strategies, the health sector, and mergers and acquisitions. Our results offer insights that assist managers, investors, and policymakers by showing how regulatory filings should be organized and which topics are likely to lead to changes in stock values.",
        "reference": "This paper studies how stock prices vary with financial disclosures. We particularly study the many uncategorized filings. We use latent Dirichlet allocation for topic modeling. This helps automatically predict the categories based on the content of 70,000 regulatory 8-K filings from U.S. companies, which announce significant events relevant to shareholders. We then evaluate the stock market reaction. Evidence suggests a large difference among types of news stories in relevance and impact on financial markets. For example, we find unusual returns in response to disclosures about earnings results, credit rating, business strategy, the health sector, mergers and acquisitions. Our results are useful for managers, investors and policy-makers. They show how to structure regulatory filings and which topics influence stock valuations.",
        "metrics": {
          "SARI": 48.21546633674536,
          "BLEU": 0.19820357806892677,
          "FKGL": 13
        }
      },
      {
        "source": "A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.",
        "prediction": "A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network, or CNN, to identify the robot body in the image and estimate three-dimensional positions of the joints by using just a two-dimensional image, but it was limited to a range of robots produced by Universal Robots, or UR. In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.",
        "reference": "Limited data available for training affects the use of deep learning techniques. Datasets are available for problems such as item recognition and classification for self-driving cars, but very limited for industrial robotics. A multi-objective Convolutional Neural Network (CNN) was first trained to identify the robot in the image and find 3D positions of the joints using just a 2D image. But this only used robots made by Universal Robots (UR). Here we work with a new robot arm - Kuka LBR iiwa which looks very different and has an additional joint. We collect a number of smaller datasets and use transfer learning techniques on the CNN trained on UR robots to adapt it to a robot that looks different. Transfer learning requires smaller training datasets, learns faster and reaches similar or even better accuracy.",
        "metrics": {
          "SARI": 28.53733788687559,
          "BLEU": 0.2871921741540661,
          "FKGL": 17.7
        }
      },
      {
        "source": "We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a ResNet-18 variant, which is fed in images from the front of a simulated F1 car, and outputs optimal labels for steering, throttle, braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.",
        "prediction": "We introduce Ignition, which is a complete system using a type of artificial intelligence called a neural network to train self-driving cars in virtual environments. This model is a version of ResNet-18, which is a specific architecture designed to process images. It takes images from the front of a simulated Formula 1 car and gives the best instructions for steering, accelerating, and braking. Importantly, we do not specifically teach the model to identify road features such as the edges of a racetrack or the distance to other cars. Instead, we show that the model can automatically learn these important details on its own.",
        "reference": "Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments. This ResNet-18 variant model receives images from the front of a simulated car and uses them to steer, throttle, or brake appropriately. The model is not explicitly trained to detect road features such as track outline or distance to other cars, as these features can automatically be calculated by the network.",
        "metrics": {
          "SARI": 43.2501250980829,
          "BLEU": 0.13877637440873672,
          "FKGL": 12.5
        }
      },
      {
        "source": "As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) system. When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. Then, the second DNN for primary acoustic modeling employs both the bottleneck features tossed from the first DNN and the acoustics features. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "prediction": "Humanoid robots often generate internal noise from their motors, fans, and mechanical parts when they move or shake. This noise can negatively affect how accurately the robots recognize speech. This paper introduces a new speech recognition system designed to be resilient to this internal noise, referred to as ego-noise. In our approach, we use the on/off state of the motor as additional information to help identify the key input features needed for speech recognition. \n\nTo achieve this, we focus on bottleneck features, which are specific types of data that have been successfully used in deep neural networks (DNNs) for automatic speech recognition (ASR). When learning these bottleneck features, we initially use the motor's on/off state data along with acoustic features, which are the sound signals related to speech, as input for the first deep neural network aimed at initial acoustic modeling. The second deep neural network further uses both the bottleneck features derived from the first network and the acoustic features for primary acoustic modeling. \n\nWhen we tested our method using the phoneme error rate (PER) metric on the TIMIT database, our experimental results demonstrated a significant improvement of 11% relative to traditional speech recognition systems.",
        "reference": "Internal noise generated by motors, fans and mechanical components severely decreases speech recognition accuracy, when the humanoid robot is moving or shaking its body. This paper presents a novel speech recognition system robust to ego-noise for humanoid robots. On/off state of the motor is used to find relevant input features. Bottleneck features have been successfully applied to automatic speech recognition (ASR) systems based on deep neural networks (DNN). The motor on/off state data and acoustic features are used as the input of the first deep neural network (DNN) for preliminary acoustic modeling. The second DNN for primary acoustic modeling uses both bottleneck features from the first DNN and acoustic ones. The proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, experimental results show that improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "metrics": {
          "SARI": 37.52591096157995,
          "BLEU": 0.19400930423847418,
          "FKGL": 14
        }
      },
      {
        "source": "Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model is made of residual convolutional blocks with hierarchical dilated skip connections joined in steps. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.",
        "prediction": "Recognizing Traffic Signs using intelligent systems can greatly decrease the number of accidents occurring worldwide. With the introduction of Self-driving cars, it has become a significant challenge to automatically recognize Traffic and Hand-held signs on major streets. Several machine learning techniques, such as Random Forest (a type of algorithm that uses decision trees to make predictions), Support Vector Machine (SVM, which is a supervised learning model that analyzes data for classification and regression tasks), as well as deep learning models (a set of algorithms based on neural networks that aim to learn from data in a hierarchical manner), have been proposed for classifying traffic signs. Although these methods achieve top performance on specific datasets, they struggle to meet the requirements of multiple Traffic Sign Recognition benchmarks. In this paper, we introduce a new all-in-one architecture that outperforms existing state-of-the-art architectures across multiple benchmarks. Our model consists of residual convolutional blocks (layers that help the model learn identity functions and improve the flow of gradients) with hierarchical dilated skip connections (a method that allows other paths to skip layers, helping the model handle different resolutions of inputs) arranged in steps. With this approach, we reached 99.33% accuracy in the German sign recognition benchmark and 99.17% accuracy in the Belgian traffic sign classification benchmark. Furthermore, we present a newly developed dilated residual learning representation technique that is very efficient in terms of memory usage and computational complexity.",
        "reference": "If intelligent systems recognize traffic signs, it can reduce car accidents. For self-driving cars, we need automatic recognition of traffic and hand signs. Machine learning techniques  could be used to classify traffic signs. They perform well on a particular dataset, but fail on multiple traffic sign recognition test datasets. We proposed a new and better method. This model is based on enhanced convolutional neural networks (a deep-learning approach often used for image recognition). This model was more than 99% correct on German and Belgian sign recognition datasets. This new model is very low in both memory and computational complexity.",
        "metrics": {
          "SARI": 39.69171188398986,
          "BLEU": 0.029013356587376176,
          "FKGL": 16
        }
      },
      {
        "source": "In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",
        "prediction": "In this paper, we present a method called transfer learning for controlling self-driving cars. This method allows a convolutional neural network (CNN), which is a type of artificial intelligence model, trained on one area (source domain) to be used for the same task in a different area (target domain). A traditional CNN for controlling cars is designed to take an image from a single front-facing camera and produce a steering command, which tells the car how to steer. To enable transfer learning, we make the CNN not only produce a steering command but also calculate a lane departure level (LDL), which indicates how far the car is from its lane, by adding a new part to the system called a task module. This task module uses the output from the last convolutional layer of the CNN as input. The CNN trained on the source domain is referred to as the source network. This source network is then used to train another task module called the target network, which also takes the output from the last convolutional layer of the source network and learns to produce a steering command for the target domain. The steering commands from both the source and target networks are finally combined based on the lane departure level, and this combined command is used to control the car in the target domain. To show how effective this method is, we used two simulators, TORCS and GTAV, as the source and target domains, respectively. The experimental results indicate that our proposed method performs better than other standard methods in achieving stable and safe control of cars.",
        "reference": "A transfer learning method to control self-driving cars is proposed, in which a convolutional neural network (CNN) is trained on one domain then used for the same task in a different domain. A conventional CNN is designed to map a single front-facing camera image to control a car. CNN produces a steering command and a lane departure level (LDL, mechanism designed to warn the driver when the vehicle begins to move out of its lane). To enable the transfer learning, a new task module takes the output of the last convolutional layer as input. The CNN trained on the source domain is then used to train the target network. It takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are merged and used for car control in the target domain. Two simulators were used to evaluate the proposed method. The proposed method is more stable and safer for car control.",
        "metrics": {
          "SARI": 47.59956951277161,
          "BLEU": 0.27310560184558136,
          "FKGL": 12.6
        }
      },
      {
        "source": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market, based on a Limit Order Book (LOB) such as is common in major stock exchanges. Construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Research aimed at understanding the dynamics of this new style of financial market is hampered by the fact that no operational real-world exchange is ever likely to allow experimental probing of that market while it is open and running live, forcing researchers to work primarily from time-series of past trading data. Similarly, university-level education of the engineers who can create next-generation automated trading systems requires that they have hands-on learning experience in a sufficiently realistic teaching environment. BSE as described here addresses both those needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHuB.",
        "prediction": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), which is a new and simple simulation of a centralized financial market. It is based on a Limit Order Book (LOB), which is a system commonly used in major stock exchanges to track buy and sell orders. The creation of BSE was inspired by the fact that most of the world’s major financial markets have become automated. Previously, human traders handled trading activities, but now high-speed autonomous automated trading systems perform these tasks. Research into understanding how this new type of financial market works faces challenges because no real-world exchange will allow experiments while it is open for trading. This means researchers mainly rely on historical data of past trading. Additionally, universities need to train engineers to build new automated trading systems through hands-on experience in a realistic teaching environment. BSE meets both of these needs: it has been used successfully for teaching and research at a leading UK university since 2012, and the program code for BSE is freely available as open-source on GitHub.",
        "reference": "This paper describes the Bristol Stock Exchange (BSE), a minimal simulation of a centralised financial market based on a Limit Order Book (LOB), a type of order to buy or sell a security at a specific price or better. BSE was built because most of the financial markets became automated. Research tries to understand the dynamics of this automated financial markets but Trainee engineers also need a realistic learning experience to build automated trading systems. BSE has been successfully used for both teaching and research since 2012. BSE code is freely available on GitHuB.",
        "metrics": {
          "SARI": 48.41839375228707,
          "BLEU": 0.1565507950095836,
          "FKGL": 13.2
        }
      },
      {
        "source": "Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict the probabilities of contents to be requested in specific areas. To reduce the car-DC delay, MLP outputs are logged into MEC servers attached to roadside units. Second, in order to cache entertainment contents stylized for car passengers’ features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.",
        "prediction": "Once self-driving cars become a reality and passengers no longer worry about them, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents that need to be stored in the areas surrounding the self-driving car is achieved at 98.04% and our approach can reduce delays. However, retrieving entertainment content from the Data Center (DC) can slow down content delivery services due to high delays in communication between the car and the DC. To address these challenges, we propose a deep learning based caching system for self-driving cars, using Deep Learning methods deployed on the Multi-access Edge Computing (MEC) structure. First, at the DC, a Multi-Layer Perceptron (MLP) is used to predict the likelihood of content being requested in specific areas. To reduce the delay between the car and the DC, MLP outputs are stored in MEC servers connected to roadside units. Second, to cache entertainment contents tailored for car passengers’ characteristics such as age and gender, a Convolutional Neural Network (CNN) is used to predict the age and gender of passengers. This allows the self-driving car to identify which content needs to be downloaded from the MEC server and stored. Finally, we formulate a deep learning based caching system in the self-driving car that enhances entertainment services as an optimization problem aimed at minimizing content downloading delays. To solve this problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.",
        "reference": "Once self-driving cars become reality and passengers no longer worry about it, they will need to find other forms of entertainment. Prediction accuracy was 98.04% for contents to be cached for self-driving cars. Such an approach can minimize delay. However, high communication delay can slow retrieval of entertainment content from Data Centers (DC). Self-driving cars can use deep learning caching to address these challenges. It is deployed on the Multi-access Edge Computing (MEC) structure that moves services from a centralized cloud to the network edges and closer to the customer. Multi-Layer Perceptron (MLP), a type of artificial neural network, is used to predict the content requested in specific areas. To reduce delay, MLP outputs are sent to MEC servers for roadside units. A Convolutional Neural Network (CNN) is used to predict passengers’ age and gender to cache adapted entertainment content. The self-driving car can therefore identify the contents to be downloaded and cached. Deep learning caching is formulated for the self-driving car to enhance entertainment services and minimize content downloading delay. A Block Successive Majorization-Minimization (BS-MM) technique solves the problem.",
        "metrics": {
          "SARI": 47.85603105345628,
          "BLEU": 0.27162141597193207,
          "FKGL": 11.8
        }
      },
      {
        "source": "Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges.",
        "prediction": "Moral responsibility is an important issue in automated decision-making. This decision-making process involves systems that can make choices on their own without human intervention. Examples of its applications include self-driving cars, which are vehicles that use technology to drive themselves without human input, and kidney exchanges, which are systems designed to match donors and recipients for kidney transplants in a fair and efficient way.",
        "reference": "Moral responsibility is a major concern in automated decision-making, from self-driving cars to kidney transplants.",
        "metrics": {
          "SARI": 28.726395218674007,
          "BLEU": 0,
          "FKGL": 15.1
        }
      },
      {
        "source": "A key problem for autonomous car navigation is the understanding, at an object level, of the current driving situation. Addressing this issue requires the extraction of meaningful information from on-board stereo imagery by classifying the fundamental elements of urban scenes into semantic categories that can more easily be interpreted and be reflected upon (streets, buildings, pedestrians, vehicles, signs, etc.). A probabilistic method is proposed to fuse a coarse prior 3D map data with stereo imagery classification. A novel fusion architecture based on the Stixel framework is presented for combining semantic pixel-wise segmentation from a convolutional neural network (CNN) with depth information obtained from stereo imagery while integrating coarse prior depth and label information. The proposed approach was tested on a manually labeled data set in urban environments. The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to what is obtained from the semantic pixel-wise segmentation of a CNN alone.",
        "prediction": "A key problem for self-driving cars is understanding the current driving situation by recognizing objects. To solve this problem, it is necessary to extract useful information from on-board stereo images by classifying important elements of city scenes into categories that are easier to interpret (such as streets, buildings, pedestrians, vehicles, signs, etc.). A statistical method is proposed to combine rough three-dimensional (3D) map data with classified stereo images. A new way to merge these two types of data is introduced using the Stixel framework, which integrates detailed pixel-wise classification from a convolutional neural network (CNN) with depth information gathered from stereo images while also using rough prior depth and label data. This method was tested on a dataset manually labeled in urban areas. The results show that the accuracy of classifying the basic elements of the urban scene was significantly improved by this method compared to using only the pixel-wise classification from a CNN.",
        "reference": "A key problem for autonomous car navigation is the understanding of the environment while driving. It needs to extract information from the sensors and cameras and classify the elements into categories so it can be easily analysed (streets, buildings, pedestrians, vehicles, signs, etc.). A method is to fuse 3D map data with classification of the sensors and cameras. This system combine image segmentation (simplify the representation of an image to facilitate its analysis) and information obtained from sensors and cameras while integrating prior information. This approach was tested on manually entered data in urban environments. The results show that the classification of elements is more accurate with this method than the image segmentation alone.",
        "metrics": {
          "SARI": 45.99746486006781,
          "BLEU": 0.1798025414910452,
          "FKGL": 15.7
        }
      },
      {
        "source": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to the development of self-driving systems is the utilization of ROS which is an open-source middleware framework used in the development of robot applications. On the other hand, the popular approach in the automotive industry is the utilization of MATLAB/Simulink which is software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables to create functionalities of ROS-based robots in MATLAB/Simulink. However, it is not been fully utilized in the development of self-driving systems yet because there are not enough samples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.",
        "prediction": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to the development of self-driving systems is the utilization of ROS, which is an open-source middleware framework used in the development of robot applications. On the other hand, a popular approach in the automotive industry is the utilization of MATLAB/Simulink, which is software for modeling, simulating, and analyzing engineering systems. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables the creation of functionalities for ROS-based robots in MATLAB/Simulink. However, it has not been fully utilized in the development of self-driving systems yet because there are not enough examples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.",
        "reference": "This paper proposes MATLAB/Simulink tests for an open-source self-driving system based on Robot Operating System. One approach to the development of self-driving systems is the utilization of Robot Operating System (open-source software framework used in the development of robot applications). But the popular approach in the automotive industry is the utilization of MATLAB/Simulink (software for modeling, simulating, and analyzing). MATLAB/Simulink provides an interface that enables to create functionalities of robots (based on robot operating system). However, it is not fully utilized in the development of self-driving systems yet because there are not enough samples, and it is difficult for developers to adopt co-development. We provide MATLAB/Simulink tests for a self-driving system (based on robot operating system) called Autoware. Autoware is a popular open-source software that provides a complete set of self-driving modules. The provided tests contain MATLAB/Simulink samples available in Autoware. They help to design self-driving systems (based on robot operating system) using MATLAB/Simulink.",
        "metrics": {
          "SARI": 40.938649906902455,
          "BLEU": 0.5722019224943374,
          "FKGL": 14
        }
      },
      {
        "source": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. One approach to the development of self-driving systems is the use of ROS; however, the system used in the automotive industry is typically designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with ROS-based systems. To allow the two to be used in tandem, it is necessary to rewrite the C++ code and incorporate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, thereby improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "prediction": "The management of self-driving systems is becoming more complex as the technology progresses. One approach to developing self-driving systems is using ROS, which stands for Robot Operating System. However, the automotive industry typically uses a system designed with MATLAB/Simulink. MATLAB/Simulink is a software suite used for modeling, simulating, and analyzing dynamic systems. The models created with MATLAB/Simulink are not compatible with ROS-based systems. To enable using both together, the C++ code must be rewritten and integrated into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created with MATLAB/Simulink to work in a ROS-based self-driving system, improving development efficiency. Furthermore, our evaluations of the proposed framework showed that it has practical potential.",
        "reference": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. Self-driving systems use Robot Operating System (open-source software framework used in the development of robot applications). However, in the automotive industry, the system is designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with systems based on Robot Operating System. To use both, it is necessary to incorporate new code into the system based on Robot Operating System, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a self-driving system, which improve development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "metrics": {
          "SARI": 39.062553977138464,
          "BLEU": 0.3560018020585526,
          "FKGL": 13.7
        }
      },
      {
        "source": "Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common, but costly and risky method, which has already caused fatalities. As a safer alternative, virtual tests, in which self-driving car software is tested in computer simulations, have been proposed. One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for systematically testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car software, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "prediction": "Ensuring the safety of self-driving cars is important, but neither the industry nor authorities have agreed on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common method that is costly and risky, leading to fatalities. As a safer alternative, virtual tests have been proposed. In virtual tests, the software of self-driving cars is examined through computer simulations. It is not feasible to manually create enough tests to cover the vast number of possible driving situations that self-driving cars must be tested for. Therefore, we developed AsFault, a tool for automatically generating virtual tests to systematically test self-driving car software. We demonstrate AsFault by testing the lane keeping feature of self-driving car software based on artificial intelligence. AsFault generates scenarios that cause the car to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "reference": "Ensuring the safety of self-driving cars is important, but there is no standard way to test them. Testing self-driving cars in regular traffic is costly and risky, and has already caused fatalities. As a safer alternative, virtual tests (self-driving car software tested in computer simulations) have been proposed. To cover the huge number of possible driving situations, self-driving cars must be tested in manually created tests. We developed AsFault, a tool for automatically generating virtual tests for testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of a self-driving car software with scenarios going off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "metrics": {
          "SARI": 53.39864503727839,
          "BLEU": 0.495605141722013,
          "FKGL": 12
        }
      },
      {
        "source": "With the developing of the technology on self-driving, more and more L3 driverless vehicles are launched in market, people get opportunities to experience the self-driving cars in their daily life. As a result, there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can not only improve the driving experience, but also accelerate the process of self-driving commercialization. This paper discusses eye-movement interaction, voice interaction and gesture interaction in self-driving car, and analyzes the technology, advantages and disadvantages of the existing interaction modes, and prospect the future development trend of self-driving human-computer interaction.",
        "prediction": "With the development of technology in self-driving cars, more L3 (Level 3) driverless vehicles are being introduced to the market, allowing people to experience self-driving cars in their daily lives. As a result, there is a growing demand for the autopilot experience. Natural and efficient interaction between humans and computers can improve the driving experience and speed up the commercialization of self-driving technology. This paper discusses three types of interactions in self-driving cars: eye-movement interaction, voice interaction, and gesture interaction. It analyzes the technology, advantages, and disadvantages of these existing interaction methods and forecasts future trends in human-computer interaction for self-driving cars.",
        "reference": "Self-driving technology is developing, more and more driverless vehicles are launched in market and accessible for people in their daily life. So there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can improve the driving experience and accelerate the process of self-driving commercialization. This paper discusses interactions with the eye, voice and gesture in self-driving car, analyzes the technology, the interaction modes, and considers the future of self-driving human-computer interaction.",
        "metrics": {
          "SARI": 42.61320628199638,
          "BLEU": 0.2538351227570323,
          "FKGL": 16
        }
      },
      {
        "source": "In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on highfidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The state of the art in driving scenario generation, as adopted by some of the front-runners of the self-driving car industry, still relies on human input [1]. In this paper we propose to automate the process using Bayesian optimization to generate adversarial self-driving scenarios that expose poorly-engineered or poorly-trained self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy using vision-based imitation learning we obtain safer self-driving behavior.",
        "prediction": "In recent years, self-driving vehicles have become more common on public roads. These vehicles promise to improve safety and efficiency in modern transportation systems. To increase the reliability of these vehicles, a comprehensive set of software tests is necessary. Ideally, these tests are performed on high-fidelity simulators. High-fidelity simulators are advanced computer programs that replicate real-world environments, allowing multiple vehicles and pedestrians to interact with self-driving vehicles. It is very important to ensure that self-driving software is tested in a variety of challenging simulated driving situations. Currently, the best methods for creating driving scenarios still require human input. In this paper, we propose to automate this process using Bayesian optimization. Bayesian optimization is a statistical technique used to find the best solution by exploring different options efficiently. We aim to create challenging self-driving scenarios that reveal weaknesses in poorly designed or poorly trained self-driving systems and increase the chances of collisions with simulated pedestrians and vehicles. We demonstrate that by adding these generated scenarios to the training data of the self-driving system and refining the system using vision-based imitation learning, we can achieve safer self-driving behavior.",
        "reference": "Self-driving vehicles have become common on public roads, with the promise of a safe and efficient mode of transport. To be reliable, these vehicles need many software tests on simulators with interactions of multiple vehicles and pedestrians. It is essential that self-driving software is evaluated with many different challenging driving scenarios. The driving scenario generation still needs human contribution. We propose to automate the process and generate conflicting self-driving that expose weaknesses of self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. By incorporating the generated scenarios into the parameters of the self-driving policy, and by adjusting the policy with the imitation learning, we obtain safer self-driving behavior.",
        "metrics": {
          "SARI": 42.732537853050246,
          "BLEU": 0.12985170348501815,
          "FKGL": 14.6
        }
      },
      {
        "source": "At the brink of the introduction of self-driving vehicles, only little is known about how potential users perceive them. This is especially true for self-driving vehicles deployed in public transport services. In this study, the relative preferences for a trip with a self-driving bus is assessed compared to a trip with a regular bus, based on a stated preference experiment. Based on the responses of 282 respondents from the Netherlands and Germany, a discrete choice model is estimated as a Mixed Logit model including attitudes towards trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. This is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. Findings from this study further suggest that the popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences towards automated public transport services are expected to evolve along with the transition from demonstration pilots to their deployment in regular operations.",
        "prediction": "At the start of using self-driving vehicles, not much is known about how potential users view them. This is particularly true for self-driving vehicles used in public transport. This study looks at the preferences for a trip on a self-driving bus compared to a regular bus through a stated preference experiment. The study includes the answers of 282 people from the Netherlands and Germany and estimates a discrete choice model, which is a type of statistical model used to predict choices by respondents. It uses a Mixed Logit model, which allows for the analysis of varying preferences across individuals. It takes into account attitudes towards trust in self-driving vehicles and interest in technology. The results indicate that public transport users prefer self-driving buses over regular buses only for short trips. This is because the value of travel time is about twice as high for self-driving buses compared to regular buses during short commuting trips. The study also finds that the popularity of self-driving buses declines when a human steward is present on board or when they are operated as a demand-responsive service with fixed routes. People who have a strong interest in technology or trust in automated vehicle technology have a better perception of self-driving buses than others. Preferences for automated public transport services are expected to change as these services move from pilot demonstrations to regular operations.",
        "reference": "Little is known about how potential users perceive self-driving vehicles. This is especially true for self-driving vehicles in public transport services. In this study, the preferences between a trip in a self-driving bus and a regular bus were compared. The results of 282 participants were studied, including trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. The preference is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. The popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences for automated public transport services are expected to evolve with their deployment in regular operations.",
        "metrics": {
          "SARI": 37.914663326742165,
          "BLEU": 0.3075699184912015,
          "FKGL": 12.5
        }
      },
      {
        "source": "In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. The fundaments of human dignity, autonomous decision-making, and democracies are shaking, but I believe that they need to be vigorously defended, as they are not only core principles of livable societies, but also the basis of greater efficiency and success.",
        "prediction": "In the wake of the ongoing digital revolution, we will see a dramatic transformation of our economy and most societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society where actions are controlled from the top down and then carried out through force or manipulative technologies, like personalized advertisements and nudging, or a society in which decisions are made freely and collectively. Modern information and communication systems enable both options, but the latter has economic and strategic advantages. The foundations of human dignity, self-determination, and democracies are at risk, but I believe they must be strongly defended, as they are not only essential for livable societies but also the basis for greater efficiency and success.",
        "reference": "The on-going digital revolution will dramatically transform our economy and societal institutions. But this also presents risks for our society. Automation of society is coming. We must choose either between a society in which the actions are determined top-down by coercion or manipulative technologies (such as personalized ads) or in which decisions are taken freely and cooperatively. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. Foundations of human dignity, autonomous decision-making, and democracies are weakening. These core principles of society and the basis of greater efficiency and success need to be defended.",
        "metrics": {
          "SARI": 33.221779180449914,
          "BLEU": 0.1860513344847328,
          "FKGL": 16.1
        }
      },
      {
        "source": "This paper addresses the financial markets problem space with particular emphasis on trading systems seen in banking today. With the advent of modern computing, cross-regional trading transactions can now be executed within milliseconds, a reality that is impossible without the advancements of software systems technology. We then move on and discuss the anatomy of a trading system and how it fits in with the banks ecosystem of vital inter-working components.",
        "prediction": "This paper focuses on the financial markets, especially the trading systems used in banking today. Thanks to modern computers, cross-regional trading transactions can now happen in milliseconds. This speed is only possible due to advancements in software systems technology. We will then discuss the structure of a trading system and how it integrates with the important interconnected components of the bank's ecosystem.",
        "reference": "This paper addresses problems of trading systems in banking. Modern computing allows cross-regional trading transactions within milliseconds. We then discuss how a trading system fits in with the bank ecosystem.",
        "metrics": {
          "SARI": 41.40018054853367,
          "BLEU": 0,
          "FKGL": 11.7
        }
      },
      {
        "source": "In this paper we present queueing-theoretical methods for the modeling, analysis, and control of autonomous mobility-on-demand MOD systems wherein robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first cast an autonomous MOD system within a closed Jackson network model with passenger loss. The theoretical insights are used to design a robust, real-time rebalancing algorithm, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study of New York shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles roughly 70% of the size of the current taxi fleet operating in Manhattan. Finally, we extend our queueing-theoretical setup to include congestion effects, and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to the problem of system-wide coordination of autonomously driving vehicles, and provides one of the first characterizations of the sustainability benefits of robotic transportation networks.",
        "prediction": "In this paper we present methods based on queueing theory for modeling, analyzing, and controlling autonomous mobility-on-demand (MOD) systems. In these systems, robotic self-driving vehicles transport customers in city areas and adjust their positions to maintain good service across the network. We first describe an autonomous MOD system using a closed Jackson network model, which is a mathematical representation that includes the possibility of losing passengers. The insights gained from this theory are used to create a strong real-time rebalancing algorithm. This algorithm is tested in a case study of New York City with an eight-vehicle mobile robot setup. The New York case study shows that the current demand for taxis in Manhattan can be satisfied with about 8,000 robotic vehicles. This is roughly 70% of the size of the existing taxi fleet operating in Manhattan. Lastly, we expand our queueing theory setup to consider congestion effects and analyze how the rebalancing of autonomous vehicles influences overall traffic congestion. Altogether, this paper offers a detailed approach to the coordination of self-driving vehicles on a large scale and highlights the sustainability benefits of robotic transportation networks.",
        "reference": "Queueing models are used for autonomous mobility-on-demand MOD systems. A queueing model is constructed so that queue lengths and waiting time can be predicted. In MOD systems,  robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure quality of service. We first cast an autonomous MOD system within a closed Jackson network model,a class of queueing network, with passenger loss. A real-time rebalancing algorithm is applied to a case study of New York City. It is implemented on an eight-vehicle mobile robot testbed. The current taxi demand in Manhattan can be met with about 8,000 robotic vehicles. It represents roughly 70% of the size of the current taxi fleet in Manhattan. Congestion effects are also included to study the impact of autonomously rebalancing vehicles on overall traffic jams. This paper describes the sustainability benefits of robotic transportation networks.",
        "metrics": {
          "SARI": 41.50718212212672,
          "BLEU": 0.24630421317197965,
          "FKGL": 13.7
        }
      },
      {
        "source": "Many model based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. However sensor based robot control overcomes many of the difficulties of uncertain models and unknown environments which limit the domain of application of the previous methods. Furthermore, for service and manipulation tasks, it is more suitable to study the interaction between the robot and its environment at the contact point using the sensor based control, rather than specifying the joint positions and velocities required to achieve them.",
        "prediction": "Many methods that use models have been suggested for getting humanoid robots to perform domestic tasks, such as remote control, learning from watching others, and mimicking actions. However, controlling robots with sensors overcomes many challenges posed by uncertain models and unknown environments that limit the usefulness of these earlier methods. Additionally, for tasks involving service and manipulation, it makes more sense to focus on the robot's interaction with its environment at the point of contact using sensor-based control, instead of just defining the joint positions and speeds needed to achieve those tasks.",
        "reference": "Model based techniques exist to apply domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. Sensor based robot control overcomes difficulties of uncertain models and unknown environments. Interaction between the robot and its environment using the sensor based control is important.",
        "metrics": {
          "SARI": 32.49618191212515,
          "BLEU": 0.0748425040544128,
          "FKGL": 16.4
        }
      },
      {
        "source": "Inaccurate information, in the field of library and information science, is often regarded as a problem that needs to be corrected or simply understood as either misinformation or disinformation without further consideration. Misinformation and disinformation, however, may cause significant problems for users in online environments, where they are constantly exposed to an abundance of inaccurate and/or misleading information. This paper aims to establish conceptual groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of cues to deception, as means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related yet distinct sub-categories of information. Misinformation is a multifaceted concept, more complex than simply being inaccurate or incomplete, and disinformation does not always entail misinformation.",
        "prediction": "Inaccurate information in the field of library and information science is often seen as a problem that needs to be corrected or understood as either misinformation or disinformation without much thought. Misinformation and disinformation can create serious issues for users in online environments where they are constantly exposed to a lot of inaccurate or misleading information. This paper seeks to lay the foundation for future research by looking at the connections between information, misinformation, and disinformation. Our analysis will also include a discussion on how to detect deception as a way to find misinformation and disinformation. We argue that misinformation and disinformation are related but different types of information. Misinformation is a complex concept that goes beyond just being inaccurate or incomplete, and disinformation doesn't always involve misinformation.",
        "reference": "Inaccurate information is often regarded as a problem that needs to be corrected or simply understood as misinformation or disinformation without further consideration. Misinformation and disinformation may cause problems online because users are constantly exposed to inaccurate and/or false information. This paper aims to establish preliminary work for future research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related but distinct sub-categories of information. Misinformation is more complex than simply being inaccurate or incomplete, and disinformation does not always involve misinformation.",
        "metrics": {
          "SARI": 40.71564063290575,
          "BLEU": 0.39283069801543763,
          "FKGL": 16.3
        }
      },
      {
        "source": "The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.",
        "prediction": "The growth of social media provides an easy way for people to communicate, but it also leads to the spread of false information. This widespread false information is harmful to the public. We create a system that combines collective intelligence (the shared knowledge or opinions of a group) and machine intelligence (the use of computer systems to simulate human intelligence) to help identify false information. The main idea is: (1) automatically categorize the knowledge of users based on what they post on microblogs (short, frequent posts on platforms like Twitter); and (2) connect those knowledgeable users with questionable false information. By sending the questionable information to the right experts, we can gather their evaluations to determine if the information is trustworthy and help correct false information. In this paper, we concentrate on finding experts to help identify false information. We suggest a method that uses tags (keywords or labels) to categorize the knowledge of microblog users using social tags. Tests on real-world data show that our method is effective for finding experts regarding false information identification in microblogs.",
        "reference": "The growth of social media leads to better communication between people, but also propagation of misinformation. The wide spread of misinformation over social media have bad consequences for public interest. We design a framework to help identify misinformation. The idea is to index the expertise of users and to match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, they can judge the credibility of information, and help refute misinformation. In this paper, we focus on finding appropriate experts for misinformation identification. We propose a method to index the expertise of users with tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding for misinformation identification.",
        "metrics": {
          "SARI": 28.048846760922235,
          "BLEU": 0.09538748554883572,
          "FKGL": 13.2
        }
      },
      {
        "source": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation in support of such research. This paper aims to address the aforementioned challenges by conceptualizing misinformation and enabling the interoperability of misinformation. In particular, a representation and a model of misinformation are proposed through surveying, synthesizing, and explicating existing work in the field. The ontology-supported misinformation model can not only guide future misinformation research but also lay the foundation for building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a formal methodology for managing misinformation in a digital library, and suggest future research directions related to the misinformation model.",
        "prediction": "Research on misinformation is increasingly recognized as important. Two main challenges for this research community are the lack of theoretical models, which are frameworks that help understand and explain complex concepts, and the limited amount of misinformation available for study. This paper aims to address these challenges by defining misinformation and promoting the ability to work with different types of misinformation together. Specifically, it introduces a way to represent and model misinformation by reviewing and combining existing studies in the area. The model supported by an organized framework, known as ontology, can not only guide future research on misinformation but also help create a digital library of misinformation. This will enhance our understanding of misinformation and improve how it is shared, managed, and reused. Additionally, we provide a clear method for managing misinformation within a digital library and suggest future research paths related to the misinformation model.",
        "reference": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the rare misinformation data. This paper aims to address these challenges by conceptualizing misinformation and being able to completely understand and work on misinformation. A representation and a model of misinformation are proposed through existing work in the field. The model can guide future misinformation research and help building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a methodology for managing misinformation in a digital library, and suggest future research directions.",
        "metrics": {
          "SARI": 30.336876818367426,
          "BLEU": 0.19109280038527712,
          "FKGL": 15
        }
      },
      {
        "source": "Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to the event. We define as misinformation any false or inaccurate information that is spread either intentionally or unintentionally. In this paper we study the problem of misinformation identification in OSNs, and we focus in particular on the Twitter social network. Based on user and tweets characteristics, we build a misinformation detection model that identifies suspicious behavioral patterns and exploits supervised learning techniques to detect misinformation. Our extensive experimental results on 80294 unique tweets and 59660 users illustrate that our approach effectively identifies misinformation during emergencies. Furthermore, our model manages to timely identify misinformation, a feature that can be used to limit the spread of the misinformation.",
        "prediction": "Online Social Networks (OSNs) have become popular platforms for sharing information among users. News about emergency events often spreads through OSNs, along with misinformation related to these events. Misinformation is defined as any false or inaccurate information that is shared, whether intentionally or unintentionally. In this paper, we examine how to identify misinformation in OSNs, focusing specifically on the Twitter social network. We develop a misinformation detection model based on the characteristics of users and tweets, which identifies suspicious behavior patterns and uses supervised learning techniques to detect misinformation. Our extensive experiments involved 80,294 unique tweets and 59,660 users, showing that our method effectively identifies misinformation during emergencies. Additionally, our model is able to identify misinformation in a timely manner, which can help reduce its spread.",
        "reference": "Social media have become popular means of information sharing. The spread of news regarding emergency events is common in social media but so is the spread of misinformation. Misinformation is any false or inaccurate information that is spread intentionally or not. In this paper we study the problem of misinformation identification in social media, and we focus in particular on Twitter. We build a misinformation detection model that identifies suspicious behavioral patterns and detect misinformation. We found 80294 unique tweets and 59660 users. It illustrates that our approach identifies misinformation during emergencies. Our model manages to timely identify misinformation, it can be used to limit the spread of the misinformation.",
        "metrics": {
          "SARI": 29.221711656714856,
          "BLEU": 0.1671075318373256,
          "FKGL": 15
        }
      },
      {
        "source": "The advent of WWW changed the way we can produce and access information. Recent studies showed that users tend to select information that is consistent with their system of beliefs, forming polarized groups of like-minded people around shared narratives where dissenting information is ignored. In this environment, users cooperate to frame and reinforce their shared narrative making any attempt at debunking inefficient. Such a configuration occurs even in the consumption of news online, and considering that 63% of users access news directly form social media, one hypothesis is that more polarization allows for further spreading of misinformation. Along this path, we focus on the polarization of users around news outlets on Facebook in different European countries (Italy, France, Spain and Germany). First, we compare the page posting behavior and the users interacting patterns across countries and observe different posting, liking and commenting rates. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities generated around specific pages.",
        "prediction": "The arrival of the World Wide Web (WWW) changed how we produce and access information. Recent studies showed that users tend to choose information that matches their beliefs, creating groups of similar-minded people around shared stories while ignoring opposing information. In this setting, users work together to support and strengthen their common story, making efforts to disprove it ineffective. This situation happens even in how people consume news online, and since 63% of users get news directly from social media, one theory suggests that increased polarization leads to more spreading of false information. In this context, we focus on the polarization of users around news pages on Facebook in various European countries, including Italy, France, Spain, and Germany. First, we compare the behavior of page postings and the patterns of user interactions across these countries and notice differences in posting, liking, and commenting rates. Second, we investigate how users tend to engage with different pages, known as selective exposure, and the formation of polarized communities around specific pages.",
        "reference": "The WWW changed the ways we can produce and access information. Users tend to select information that agrees with their beliefs. They form polarized groups of like-minded people sharing narratives that ignore contradicting information. Here, users reinforce their shared narratives and can resist debunking This is the case with online news. Because 63% of users access news from social media, polarization could increase misinfomation spread. We focus on polarisation of Facebook news consumers in different European countries. Posting, liking and commenting rates differ across countries. We look at how users interact with different pages and polarized communities form around specific pages.",
        "metrics": {
          "SARI": 42.15101051100946,
          "BLEU": 0.09479597452826705,
          "FKGL": 13.8
        }
      },
      {
        "source": "We find that Italy is the most polarized country, followed by France, Germany and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the usersu0027 engagement and trust on the news. Our findings suggest that trust in information broadcaster plays a pivotal role against polarization of users online.",
        "prediction": "We find that Italy is the most polarized country, followed by France, Germany, and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the users' engagement and trust in the news. Our findings suggest that trust in information broadcasters plays a pivotal role against the polarization of users online.",
        "reference": "Italy is the most polarized country, then France, Germany and Spain. A variant of the Bounded Confidence Model simulates the emergence of these communities by considering user engagement and trust in the news. Trust in an information source plays a major role against polarization of users online.",
        "metrics": {
          "SARI": 50.98875642616737,
          "BLEU": 0.39362525167716106,
          "FKGL": 11.2
        }
      },
      {
        "source": "Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative strategies to contrast misinformation. Simultaneously, they allow the spread of misinformation by empowering individuals to self-select the narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. A precise theoretical understanding of such trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs unbiasedly based on the arrival of new information, while a fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. We show that this simple confirmation bias mechanism can generate permanent opinion polarisation. Furthermore, the model results in states where unbiased agents behave \"as if\" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. We derive analytic results for the distribution of individual agents’ beliefs, explicitly demonstrating the aforementioned trade-off between confirmation bias and social connectivity, which we further validate against US county-level data on the impact of Internet access on the formation of beliefs about global warming. Our findings indicate that confirmation bias in small doses may actually result in improved accuracy across individuals by preserving information diversity in a social network. However, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups.",
        "prediction": "Online social media provide users with unique opportunities to engage with different opinions. We discuss the effects of our model on policies, pointing out the negatives of debunking strategies and suggesting alternative ways to counter misinformation. At the same time, social media enable the spread of false information by allowing people to choose the stories they wish to see, using both active methods (confirmation bias) and passive methods (personalized news algorithms) that reinforce their existing beliefs. A clear theoretical understanding of these trade-offs is still mostly lacking. We introduce a simplified social learning model where most people in a network update their beliefs fairly based on new information, whereas a smaller group shows confirmation bias, which allows them to dismiss news that does not align with their existing views. We demonstrate that this straightforward confirmation bias mechanism can lead to lasting division of opinions. Additionally, the model shows that unbiased individuals can behave as if they were biased, due to their biased neighbors acting as gatekeepers, limiting their access to varied and free information. We obtain analytical results for the beliefs of individual agents, clearly showing the trade-off between confirmation bias and social connections, which we further test against data from U.S. counties on how Internet access impacts beliefs about global warming. Our findings suggest that a small amount of confirmation bias may actually improve accuracy among individuals by maintaining information diversity in a social network. However, our results also show that when confirmation bias exceeds a certain level, accuracy decreases as biased individuals limit information flow to certain groups.",
        "reference": "Social media give users opportunities to interact with different opinions. We discuss the meaning of our model for policy and the downsides of myth-busting We suggest alternatives to fight misinformation. But misinformation is spread via social media because individuals can search for information that confirms their beliefs and personalized news algorithms may supply it. Theoretical understanding of the balance between opportunities and misinformation is incomplete. We introduce a stylized social learning model. In this model, most participants in a network update their beliefs based on new information, while others show confirmation bias and can reject news that differs from their beliefs. This simple confirmation bias mechanism can create lasting opinion polarisation. Besides, in the model, unbiased people behave “as if” biased because biased neighbours limit their access to diverse information. With our model, we show the trade-off between confirmation bias and social connectivity. We validated the model using the data on how internet access influences beliefs on global warming. Small confirmation bias may even improve accuracy across individuals by keeping information diversity in a social network. But large confirmation bias lowers accuracy as biased individuals limit information flow to others.",
        "metrics": {
          "SARI": 51.43082364187629,
          "BLEU": 0.18484422850797852,
          "FKGL": 14.9
        }
      },
      {
        "source": "Superintelligence is a potential type of future artificial intelligence (AI) that is significantly more intelligent than humans in all major respects. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. Meanwhile, the prospect of superintelligence is the subject of major ongoing debate, which includes a significant amount of misinformation. Superintelligence misinformation is potentially dangerous, ultimately leading bad decisions by the would-be developers of superintelligence and those who influence them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: strategies to prevent the spread of superintelligence misinformation and strategies to correct it after it has spread. In general, misinformation can be difficult to correct, suggesting a high value of strategies to prevent it. The strategies proposed can be applied to lay public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "prediction": "Superintelligence is a potential type of future artificial intelligence (AI) that is much smarter than humans in all important ways. If created, superintelligence could change everything, with possible outcomes that could be very helpful or extremely harmful. The idea of superintelligence is currently a topic of significant debate, which includes a lot of incorrect information. This incorrect information about superintelligence can be dangerous, leading to poor decisions by those who try to develop superintelligence and those who have influence over them. This paper looks at strategies to counter misinformation about superintelligence. Two types of strategies are discussed: strategies to stop the spread of misinformation about superintelligence and strategies to fix it after it has spread. Generally, misinformation can be hard to correct, which highlights the importance of strategies to prevent it from spreading. The proposed strategies can be used to raise public awareness about superintelligence, in education programs about AI, and in efforts to create a consensus among experts.",
        "reference": "Superintelligence is a potential future artificial intelligence (AI) significantly more intelligent than humans. Superintelligence could be a major event, with potential consequences that are beneficial or catastrophic. The prospect of superintelligence is the subject of major debate, which includes a lot of misinformation. Superintelligence misinformation could be dangerous, and lead to bad decisions by the people that could work on the project. This paper searches for strategies to counter superintelligence misinformation. Two strategies are examined: to prevent the spread of misinformation and to correct it after it has spread. Misinformation can be difficult to correct, so preventing it seems to be the best strategy. The strategies proposed can be used to bring public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "metrics": {
          "SARI": 35.8432483368789,
          "BLEU": 0.2002373796180662,
          "FKGL": 13.4
        }
      },
      {
        "source": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\\\\Omega(2^{\\\\log^{1-\\\\epsilon}n^4})$ in polynomial time unless $NP \\\\subseteq DTIME(n^{\\\\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "prediction": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims to limit the spread of false information in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem when an arbitrary number of information spread events, called cascades, are allowed. First, we provide a formal model for multi-cascade diffusion, which is how information spreads through multiple interconnected events, and introduce an important concept called cascade priority, which refers to the importance or urgency of certain information sources over others. Second, we show that the misinformation containment problem cannot be approximated within a factor of Omega(2^(log^(1-epsilon)n^4)) in polynomial time unless NP, a class of decision problems for which a solution can be verified quickly, is a subset of DTIME(n^(polylog(n))), which refers to a class of problems that can be solved by a deterministic Turing machine within a time that grows polynomially with the logarithm of n. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "reference": "The online misinformation could cause public panic and serious economic damages. The goal of misinformation containment is to limit the spread of misinformation in online social networks by launching campaigns to counter misinformation. We present the first analysis of the misinformation containment problem with a random number of cascades (people observe the actions of others and then make the same choice) allowed. First, we provide a diffusion model and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Ω(2log1−ϵn4) in polynomial time unless $NP \\subseteq DTIME(n^{\\polylog{n}})$ Third, we introduce several types of cascade priority that are frequently seen in social networks. Finally, we design algorithms for solving the misinformation containment problem. The proposed algorithm is effective and shows encouraging results.",
        "metrics": {
          "SARI": 46.851076332395266,
          "BLEU": 0.3628236276113196,
          "FKGL": 16.7
        }
      },
      {
        "source": "Yet contemporary work in Philosophy argues provisional belief in conspiracy theories is—at the very—least understandable (because conspiracies occur) and if we take an evidential approach—judging individual conspiracy theories on their particular merits—belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine the kinds of evidence typically associated with conspiracy theories, showing that the evidential problems typically associated with conspiracy theories are not unique to such theories. As such, if there is a problem with the conspiracy theorist’s use of evidence, it is one of principle: is the principle which guides their use of evidence somehow in error? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for a scepticism of conspiracy theories based purely on their use of evidence.",
        "prediction": "Contemporary work in Philosophy suggests that it is at least understandable to have a temporary belief in conspiracy theories, because conspiracies do happen. If we evaluate individual conspiracy theories based on the evidence for each one, we can find that belief in them is justified in several situations. In this context, I explore the types of evidence that are usually connected to conspiracy theories, demonstrating that the issues with evidence related to these theories are not specific to them alone. Therefore, if there is a problem with how conspiracy theorists use evidence, it may stem from a fundamental principle: is the principle guiding their evidence use flawed in some way? I argue that regardless of our opinions on conspiracy theories as a whole, there is no obvious reason to be skeptical about them solely based on how they use evidence.",
        "reference": "To believe in conspiracy theories is understandable as conspiracies occur and if we take an evidential approach, belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine evidence associated with conspiracy theories, showing that the evidential problems are not unique to conspiracy theories. The problem with the use of evidence of conspiracy theorist could be the principle which guides their use of it. I argue that there is no ground to be suspicious of conspiracy theories as a whole, because those are based on evidence.",
        "metrics": {
          "SARI": 33.45178149138979,
          "BLEU": 0.09756874075753753,
          "FKGL": 15.4
        }
      },
      {
        "source": "Conspiracy theories have gained much academic and media attention recently, due to their large impact on public events. However, little is known about how conspiracy theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis--the Zika virus outbreak. Using a mixed-methods approach including content analysis and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the Zika crisis, the conditions under which Zika conspiracy theories emerge, and the particular discursive strategies through which Zika conspiracy theories developed in online forums. Our analysis shows that conspiracy talk emerged as people attempted to make sense of a public health crisis, reflecting their emergent information needs and their pervasive distrust in formal sources of Zika information. Practical implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "prediction": "Conspiracy theories have received a lot of attention from academics and the media lately because they significantly influence public events. However, there is limited knowledge about how these theories are created and spread on social media. We present a detailed study of conspiracy thinking on Reddit during a public health emergency, specifically the Zika virus outbreak. Using a combination of different methods, including content analysis and discourse analysis, we identified various types of conspiracy theories that were shared on Reddit in response to the Zika crisis, the situations that led to the emergence of these theories, and the specific ways in which they developed in online discussions. Our analysis indicates that discussions about conspiracy theories arose as individuals sought to understand a public health issue, highlighting their growing need for information and their widespread distrust of official sources of information about Zika. We discuss practical implications for researchers in social computing, healthcare professionals, and policymakers.",
        "reference": "Conspiracy theories have gained much attention recently, due to their large impact on public events. Little is known about how conspiracy theories are produced and developed on social media. We present a study of conspiracy theory creation on Reddit during a public health crisis. Using content and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the crisis, the conditions of their creation, and the strategies of their development in online forums. Our analysis shows that conspiracy talk come from people trying to make sense of a public health crisis, which reflect their information needs and their lack of confidence in formal sources of information. Implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "metrics": {
          "SARI": 34.056240900599704,
          "BLEU": 0.224417352194856,
          "FKGL": 16.9
        }
      },
      {
        "source": "Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories evolve, multiply, and interconnect, further complicating efforts to understand them and to limit their propagation. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif such as \\\"governmental agency-controls-communications\\\" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.",
        "prediction": "Conspiracy theories are everywhere in online discussions. They can be used to explain breaking news that hasn’t been officially reported or to express disagreement with political issues. Conspiracy theories change, grow, and connect with each other, making it harder to understand them and to stop their spread. It is important to create methods that can be easily used to look at the nature of discussions about conspiracy theories in online communities. What topics do users discuss when they talk about conspiracy theories online? What are the common themes in their discussions? What do these themes reveal about how users think? This work answers these questions by analyzing more than ten years of discussions in r/conspiracy, which is a group on Reddit focused on conspiracy theories. We examine the main elements of a conspiracy theory: the people or groups believed to be involved, the actions they supposedly take, and the targets of these actions. For instance, a common theme like \"government agency-controls-communications\" shows the different ways various conspiracy statements suggest that government agencies control information. Therefore, these common themes reveal similarities between different conspiracy theories, even if they relate to various events or situations. This analysis helps us understand how users discuss conspiracy theories and gives us a way to interpret those discussions. Our method allows for a broad study of conspiracy theories in alternative news and social media, which is important for understanding how they are accepted and for finding ways to fight their spread.",
        "reference": "Conspiracy theories are omnipresent in online discussions. Conspiracy theories evolve, multiply, and interconnect, complicating how to understand them and to limit their propagation. It is crucial to develop methods to examine the nature of these conspiratorial discussions. What do users talk about? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in an online community on the social media Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif (an idea) such as \"governmental agency-controls-communications\" represents the various ways in which multiple conspiratorial statements suggest how governmental agencies control information. Narrative-motifs expose similarities between multiple conspiracy theories even when they refer to different events or circumstances. These representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media and ways to understand them and limit their propagation.",
        "metrics": {
          "SARI": 32.09217137096654,
          "BLEU": 0.24908602032889335,
          "FKGL": 11.8
        }
      },
      {
        "source": "Blockchain technology is the underlying enabling technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force. Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make that transaction simpler and easier between entities. We investigate factors that influence an individualu0027s intention to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption grounded in the theory of planned behavior (TPB) to: identify the determinants for the acceptance of cryptocurrency and explore the relative importance of each construct. We offer empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context.",
        "prediction": "Blockchain technology is the foundational technology created for Bitcoin which is the most widely used type of digital currency known as cryptocurrency. Blockchain technologies have gained popularity due to their potential to disrupt traditional systems. Individuals and organizations can benefit from blockchain as it allows for more secure data exchange and makes transactions simpler and easier between parties. We examine the factors that influence a person's intention to use a blockchain-based cryptocurrency. We create a model for cryptocurrency adoption based on the theory of planned behavior (TPB). This model aims to identify the key factors that affect the acceptance of cryptocurrency and to examine the importance of each factor. We provide evidence that contributes to a better understanding of cryptocurrency adoption and its practical applications in the context of e-government.",
        "reference": "Blockchain technology is a technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become popular with the potential to become a powerful disruptive force. People and organizations may use it to increase secure data exchange and make transactions simpler and easier. We investigate what influence people to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption to identify how cryptocurrency is accepted. Our evidence allow a better understanding of cryptocurrency adoption.",
        "metrics": {
          "SARI": 37.43566562102416,
          "BLEU": 0.14779355321172916,
          "FKGL": 14.1
        }
      },
      {
        "source": "With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. We find that cryptojacking is common, with currently 1 out of 500 sites hosting a mining script. Moreover, we perform several secondary analyses to gain insight into the cryptojacking landscape, including a measurement of code characteristics, an estimate of expected mining revenue, and an evaluation of current blacklist-based countermeasures.",
        "prediction": "With the introduction of memory-bound cryptocurrencies such as Monero the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs Memory-bound cryptocurrencies are digital currencies that require a significant amount of memory to mine effectively Based on this technology a new form of parasitic computing known as cryptojacking or drive-by mining has become more common on the web Cryptojacking is when a website secretly uses the computer resources of its visitors to mine for cryptocurrencies In this paper we systematically explore this phenomenon For this we propose a 3-phase analysis approach which helps us identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites Alexa is a web traffic analysis company that ranks websites We find that cryptojacking is common with currently 1 out of 500 sites hosting a mining script Moreover we perform several secondary analyses to gain insight into the cryptojacking landscape including a measurement of code characteristics an estimate of expected mining revenue and an evaluation of current blacklist-based countermeasures Blacklist-based countermeasures are tools that block websites known to engage in cryptojacking",
        "reference": "Memory-bound cryptocurrencies like Monero can be mined  from a browser with JavaScript, better than with a dedicated rig. However, this technology enables cryptojacking, whereby a site covertly mines for cryptocurrencies on its visitors’ computers. We propose a new approach to identify mining scripts. The approach was evaluated on Alexa 1 websites. Cryptojacking is common, as 1 in 500 sites host a mining script. We also measured code characteristics, estimated mining revenue, and evaluated blacklist-based countermeasures.",
        "metrics": {
          "SARI": 34.101333164097625,
          "BLEU": 0,
          "FKGL": 79.8
        }
      },
      {
        "source": "The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.  contracts :[39],\"are commonly written in a high-level language such as Ethereumu0027s Solidity, and translated to compact low-level bytecode for deployment on the blockchain. Once deployed, the bytecode is autonomously executed, usually by a %Turing-complete virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts :[39],\"are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal consists of an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations. Users of the framework can express security analyses in a declarative fashion: a security analysis is expressed in a logic specification written in the \\souffle language.",
        "prediction": "The rise of modern blockchains has facilitated the emergence of smart contracts, which are self-operating programs that exist and execute on the blockchain. Smart contracts have quickly gained popularity and are expected to be used in areas like law, business, commerce, and governance. Smart contracts are usually written in a high-level programming language such as Ethereum's Solidity and are converted into compact low-level bytecode for deployment on the blockchain. Once deployed, this bytecode is executed automatically, typically by a Turing-complete virtual machine. Like all software, smart contracts can be very vulnerable to cyber attacks due to poor programming methods, languages, and development tools, which may include faulty compilers. Furthermore, smart contracts are attractive targets for attacks, as they often hold significant amounts of cryptocurrency. Therefore, developers and security auditors require frameworks that can analyze low-level bytecode to identify potential security weaknesses. In this paper, we introduce Vandal, a security analysis framework for Ethereum smart contracts. Vandal includes an analysis process that transforms low-level Ethereum Virtual Machine (EVM) bytecode into semantic logic relations. Users of this framework can perform security analyses in a clear manner, by expressing the analysis in a logic specification written in the Souffle language.",
        "reference": "Smart contracts have appeared thanks to blockchain technology. These autonomous programs are used in law, business, commerce, and governance. Smart contracts are written in a high-level language such as Ethereum’s Solidity and translated to bytecode. that live and run on the blockchain are predicted in Once deployed on the blockchain, the bytecode is autonomous. Smart contracts are vulnerable to malicious attacks due to poor programming, languages and toolchains Smart contracts can command large amounts of cryptocurrency. Developers and auditors need frameworks to detect security vulnerabilities. Vandal is a security analysis framework for Ethereum smart contracts. Vandal converts Ethereum Virtual Machine bytecode to semantic logic relations. Users can express security analyses in the Soufflé language.",
        "metrics": {
          "SARI": 45.53989394095388,
          "BLEU": 0.2056873515504841,
          "FKGL": 14.5
        }
      },
      {
        "source": "Vandal is both fast and robust, successfully analysing over 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.",
        "prediction": "Vandal is both fast and strong, successfully analyzing over 95 percent of all 141,000 unique contracts with an average runtime of 4.15 seconds. It performs better than the best current tools, which include Oyente, EthIR, Mythril, and Rattle, under the same conditions.",
        "reference": "Vandal is both efficient (95% accuracy), fast (141k in 4.15 s) and robust. It outperforms the state of the art tools like Oyente, EthIR, Mythril, and Rattle",
        "metrics": {
          "SARI": 47.498212017664734,
          "BLEU": 0.19319522417917573,
          "FKGL": 10.3
        }
      },
      {
        "source": "As Bitcoinu0027s popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds.",
        "prediction": "As Bitcoin's popularity has grown over the decade since its creation, it has become a more appealing target for various adversaries.",
        "reference": "One of the most powerful potential adversaries is the country of China, which has expressed adversarial positions regarding the cryptocurrency and demonstrated powerful capabilities to influence it.",
        "metrics": {
          "SARI": 36.05744789018811,
          "BLEU": 0,
          "FKGL": 12.7
        }
      },
      {
        "source": "For decentralised P2P networks, it is very important to have a mechanism in place that allows the nodes to control resource usage and prevent flooding and denial-of-service attacks with spam. In this paper, we discuss and compare the different approaches to fully decentralised resource control that are used by projects in the cryptocurrency space. The introduced methods are then applied to design a decentralised exchange for Namecoin names (or more generally, crypto assets) as an example.",
        "prediction": "For decentralized peer-to-peer (P2P) networks, it is very important to have a system that allows the nodes, which are individual computers or servers in the network, to manage resource usage and prevent unwanted interference like flooding and denial-of-service attacks caused by spam, which is irrelevant or junk data. In this paper, we discuss and compare the various methods for completely decentralized resource control that are used by different projects in the cryptocurrency space, which is the sector of finance that uses digital currencies secured by cryptography. The introduced methods are then applied to design a decentralized exchange for Namecoin names, which are digital assets representing identities or services, or more generally, for crypto assets as an example.",
        "reference": "In decentralised P2P networks nodes need a way  to control resource usage and prevent flooding and denial-of-service attacks. We compared different approaches to decentralised resource control in cryptocurrency projects. We designed a decentralised exchange for crypto assets, for  example Namecoin names, with these methods.",
        "metrics": {
          "SARI": 32.60017295090349,
          "BLEU": 0.06685027417755393,
          "FKGL": 20.9
        }
      },
      {
        "source": "Cryptocurrency is a rapid developing financial technology innovation which has attracted a large number of people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have triggered panic and chain reactions towards the application and adoption of cryptocurrency and have caused public security related events. In this paper, we analyze the dynamics and systemic risk of the cryptocurrency market based on the public available price history. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.",
        "prediction": "Cryptocurrency is a rapidly developing financial technology that has gained a lot of attention worldwide. The fast changes, large price swings, and differing views of financial authorities in various countries have caused confusion and reactions regarding the use and acceptance of cryptocurrency, leading to security issues. In this paper, we examine the behaviors and risks of the cryptocurrency market using publicly available price data. Additionally, in line with public opinion, our analysis shows that the cryptocurrency market is generally fragile and unstable.",
        "reference": "Cryptocurrency is a financial technology innovation which has attracted many people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have caused panic. In this paper, we analyze the dynamics and risks of the cryptocurrency market. Consistent with public perception, our analysis reveals that the cryptocurrency market is fragile and unstable.",
        "metrics": {
          "SARI": 37.46359250280795,
          "BLEU": 0.240650493962848,
          "FKGL": 14.8
        }
      },
      {
        "source": "Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by treating it as a financial asset. We position cryptocurrency by comparing its dynamic characteristics with two traditional and massively adopted financial assets: foreign exchange and stock. Our investigation suggests that the dynamics of cryptocurrency are more similar to stock. As to the robustness and clustering structure, our analysis shows cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "prediction": "Cryptocurrency was originally created as a type of digital currency. Over time, various forms of cryptocurrency have emerged in different areas, including financial transactions, managing capital, and even in ways that do not involve money. This paper seeks to provide insights to help people understand cryptocurrency by looking at it as a financial asset. We compare cryptocurrency to two well-known and widely used financial assets: foreign exchange and stocks. Our investigation indicates that cryptocurrency behaves more like stocks. In terms of stability and market structure, our analysis reveals that the cryptocurrency market is more vulnerable than the stock market, making it a high-risk financial market right now.",
        "reference": "Cryptocurrency was initially an implementation of digital currency, then derivatives were created in various fields. This paper aims to help understand cryptocurrency. We compared cryptocurrency with foreign exchange and stock. Our investigation suggests that cryptocurrency is similar to stock. Our analysis shows that cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "metrics": {
          "SARI": 35.62086060484983,
          "BLEU": 0.09637781126668263,
          "FKGL": 12.6
        }
      },
      {
        "source": "The smart device owning rate such as smart phone and smart watch is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time, blockchain-based cryptocurrency is becoming a nonnegligible type of currency and the total value of all types of cryptocurrency has reached USD 200 billion. Therefore, it is a natural demand to support cryptocurrency payment on mobile devices. Considering the poor infrastructure and low penetration of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are the two main obstacles for mobile payment using cryptocurrency. We propose two different schemes for cryptocurrency mobile payment, one involves a centralized bank and the other one does not require any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.",
        "prediction": "The ownership of smart devices like smartphones and smartwatches is at an all-time high, and mobile payment has become a major method of payment in many areas. Meanwhile, cryptocurrency based on blockchain technology is becoming an important type of currency, with the total value of all cryptocurrencies reaching USD 200 billion. This creates a natural need to support cryptocurrency payments on mobile devices. This combination is particularly appealing in developing countries, where financial services may not be widely available. However, two main challenges exist for mobile payments using cryptocurrency: the high cost of storing data and delays in processing payments. We suggest two different approaches for mobile payment using cryptocurrency: one that involves a centralized bank and another that does not rely on any central authority. Additionally, we offer a solution for the bank to comply with KYC (Know Your Customer) and AML (Anti-Money Laundering) regulations when processing cryptocurrency mobile payments.",
        "reference": "The owning rate of smart devices is higher than ever before and mobile payment has become a major payment method. Cryptocurrency is becoming an important type of currency and the total value of cryptocurrencies has reached USD 200 billion. Therefore, support of cryptocurrency payment on mobile devices is a natural demand. Considering the poor infrastructure and the low usage of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are two obstacles for cryptocurrency mobile payment. We propose two solutions, one with a centralized bank and the other one without any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) expectations about cryptocurrency mobile payment.",
        "metrics": {
          "SARI": 36.344665610083766,
          "BLEU": 0.1996860746928231,
          "FKGL": 14.1
        }
      },
      {
        "source": "Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.",
        "prediction": "Motivated by recent financial crises, significant research efforts have focused on contagion effects and herding behavior in financial markets. Contagion effects refer to the spread of crises or financial instability from one market or institution to others. Herding behavior is when individuals in a market act collectively rather than independently, often leading to market trends. Much less has been studied about the influence of financial news on these markets. We propose a new measure of collective behavior in financial news on the Internet called the News Cohesiveness Index (NCI) and demonstrate that it can serve as an indicator of systemic risk. Systemic risk is the potential for a system-wide financial crisis. We evaluate the NCI using financial documents from large online news sources daily from October 2011 to July 2013, analyzing the relationship between financial markets and news that is financially related. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesion refers to the degree to which news items are related to one another in sentiment or content. We believe cohesiveness is a more comprehensive and reliable measure of systemic risk in news compared to measures that only count occurrences of specific terms. Our results suggest that the cohesion in financial news is strongly correlated with and influenced by volatility in the financial markets. Volatility refers to the degree of variation of trading prices over time, often indicating uncertainty or instability.",
        "reference": "Motivated by recent financial crises, research efforts have been put into studying contagion effects (spread of market disturbances) and herding behaviour (individuals acting collectively without centralized direction) in financial markets. Much less has been said about influence of financial news on financial markets. We propose a measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator, which signals probability of financial system crises. We evaluate the NCI on financial documents from Web news sources and analyse the relation between financial markets and financial news. We hypothesized that strong cohesion in financial news reflects variations in the financial markets. Cohesiveness is a better measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results show that cohesiveness in the financial news is highly correlated with variations in the financial markets.",
        "metrics": {
          "SARI": 35.835189351339054,
          "BLEU": 0.2613543268356049,
          "FKGL": 13.4
        }
      },
      {
        "source": "In this paper, I propose a methodology to study the comovement between the entropy of different financial markets. The entropy is derived using singular value decomposition of the components of stock market indices in financial markets from selected developed economies, i.e., France, Germany, the United Kingdom, and the United States. I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using a dynamic factor model and derive a common factor behind the entropy movements in these four markets.",
        "prediction": "In this paper, I propose a way to study the relationship between the randomness or uncertainty of different financial markets. The randomness is calculated using a method called singular value decomposition, which is used to analyze the components or building blocks of stock market indices in financial markets from selected developed economies, specifically France, Germany, the United Kingdom, and the United States. I examine how a sudden change in the randomness in the United States affects the randomness in the other financial markets. Additionally, I use a model called a dynamic factor model to analyze the randomness and identify a common underlying factor that drives the changes in randomness in these four markets.",
        "reference": "In this paper, I propose a methodology to study the comovement (correlation on profits on investment) between the entropy (randomness and uncertainty) of different financial markets. The entropy is based on the components measuring the stock market in financial markets from selected developed economies (France, Germany, the United Kingdom, and the United States). I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using different time variables and obtain a common factor behind the entropy movements in these four markets.",
        "metrics": {
          "SARI": 37.70114981425306,
          "BLEU": 0.3647131039849722,
          "FKGL": 16.7
        }
      },
      {
        "source": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties - none of which passes through company-owned systems, and is to all intents and purposes 'off the grid'. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics. Detecting 'tech-savvy' corporate fraudsters is a constant game of catch-up. It's not only about playing catch-up with the intellect, motives and awareness of the e-fraudster, but also the technologies that can be misused. What's more, the methods through which the technology can be manipulated to secrete, disguise and protect fraudulent activities - all while staying away from corporate networks where they can readily be monitored and detected - are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.",
        "prediction": "The use of mobile phone forensics to investigate fraudulent activity is not new. However, mobile phones have now become smartphones, and fraudsters have adapted to this change. Smartphones have many features that can be exploited for stealing data or making inappropriate contact with others. This activity often does not go through company-owned systems, making it difficult to track and monitor. Employers should be aware of these risks when providing devices and should have procedures in place for when suspicions arise, according to Philip Ridley of CCL-Forensics. Identifying technologically skilled corporate fraudsters is an ongoing challenge. It requires keeping up not only with the knowledge and intentions of the online fraudster, but also with the technologies that can be misused. Additionally, the methods that fraudsters use to hide, conceal, and protect their activities are continuously changing. This puts a company's intellectual property, which includes creative works and inventions, and sensitive data at risk of damage or theft.",
        "reference": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones have functionalities that can be used for data theft or inappropriate contact with other parties. They are out of the systems of the company and are not monitored or controlled. Employers need to be aware of these risks when devices are delivered and have processes when suspicions are raised. Detecting fraudsters with computer skills is a constant game of catch-up. It's not only about playing catch-up with the cyberfraudster, but also the technologies that can be misused. Moreover, the methods assisting fraudulent activities (still out of reach of company systems) are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or theft.",
        "metrics": {
          "SARI": 39.06219412472786,
          "BLEU": 0.23659461934558906,
          "FKGL": 11.2
        }
      },
      {
        "source": "Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes to phishing campaigns, a cadre of file matching algorithms is implemented to detect phishing websites based on their content, employing a custom data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using a variety of different content-based approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "prediction": "Phishers continue to change the underlying code of the web pages they use in their attacks. They do this to make their sites look like real websites of fake organizations and to avoid being caught by tools that prevent phishing. These changes can be very subtle, like small code adjustments, or more obvious, like adding or taking away important information. To effectively deal with these changes in phishing attacks, a set of file matching algorithms is used to find phishing websites based on their content. This involves a custom data set made up of 17,992 phishing attacks that target 159 different brands. The results from testing various content-based methods show that some can detect over 90% of phishing attempts while keeping the number of false alarms low.",
        "reference": "Phishers, people who attempt to trick people over the internet to steal their money, continue to modify the web pages used in their attacks to imitate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as changing the source code, which is the programming instructions, or as apparent as adding or removing significant content. To respond to these changes to phishing campaigns, algorithms are used to detect phishing websites based on their content, employing a data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using different approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "metrics": {
          "SARI": 24.723233367755057,
          "BLEU": 0.1723107108268787,
          "FKGL": 10.4
        }
      },
      {
        "source": "Mobile devices are very common in everyone’s day-to- day life. Nowadays such devices come with many features of desktop or laptop. Hence people can use these devices for diverse applications. As the acceptability and usability of such devices are very high, there are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is in hike. So it becomes necessary to digitally analyze such devices requiring cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and an agent based approach for logically acquiring such devices. Also it describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.",
        "prediction": "Mobile devices are very common in everyone's daily life. Nowadays, these devices have many features similar to desktop or laptop computers. As a result, people can use mobile devices for a variety of applications. Because of their high acceptance and usability, there is a risk that these devices can be used for illegal activities. The percentage of mobile phones or smartphones involved in cyber crimes is increasing. Therefore, it is necessary to analyze these devices digitally using cyber forensics tools. This paper discusses different types of digital evidence found in Microsoft's Windows Mobile smartphones and an agent-based approach for logically acquiring such devices. It also describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and Windows CE Personal Digital Assistants (PDAs).",
        "reference": "Mobile devices are very common in everyone’s day-to-day life. Nowadays such devices come with many features of computers. People can use these devices for diverse applications. There are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is increasing. So it becomes necessary to digitally analyze such devices with cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and how to acquire such devices. Also it describes a forensic tool for acquiring and analyzing Windows Mobile devices and personal digital assistants.",
        "metrics": {
          "SARI": 40.692552840284215,
          "BLEU": 0.36462121176003337,
          "FKGL": 11.7
        }
      },
      {
        "source": "Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It gives the information about the processing history of an image, and also can expose forgeries present in an image. There have been a few methods proposed to detect different types of image editing operations in a single framework. However, all the operations have to be known a priori in the training phase. But, in real-forensics scenarios it may not be possible to know about the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.",
        "prediction": "Detection of different types of image editing operations on an image is an important problem in image forensics. Image forensics is the field that focuses on identifying the authenticity of an image and determining whether it has been altered. Understanding the processing history of an image can help reveal any forgeries or manipulations. Several methods have been suggested to detect various image editing operations within a single framework. However, these methods require knowing all possible operations in advance during the training phase. In real forensic situations, it may not be feasible to anticipate the editing operations applied to an image. To address this issue, we propose a new method based on deep learning that can distinguish between different types of image editing operations. Our method uses a deep siamese neural network, which is a type of artificial intelligence model designed to compare two inputs. It classifies parts of images in pairs as either similarly or differently processed. Once the network learns to identify features that differentiate editing operations, it can recognize and distinguish operations that were not included during training. Experimental results demonstrate the effectiveness of our method in detecting and differentiating various image editing operations.",
        "reference": "Detection of image editing is an important task in image forensics. The edit history of an image can expose forgeries in the image. Various methods have been proposed to detect image editing operations. However all of the possible types of image manipulation must be known in advance. But, in the real world, manipulations on images are sometimes unknowable. A novel deep learning-based method can distinguish between different types of image editing operations. This method uses a deep siamese neural network to compare images two by two. Using deep learning, the computer can learn to see features of edited images as compared to unedited and learn to differentiate between different image editing operations. This method works to find out what editing an image has undergone.",
        "metrics": {
          "SARI": 46.04209254877375,
          "BLEU": 0.1670194690841406,
          "FKGL": 13.8
        }
      },
      {
        "source": "The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices – potentially progressing a wide variety of currently hindered digital investigations.",
        "prediction": "The increasing use of Internet of Things (IoT) devices means they will be more important in digital forensic investigations in the future. IoT devices, made by different companies, usually have limited ways to communicate, like USB ports or WiFi/Bluetooth wireless connections. At the same time, there is a growing focus on the security and privacy of user data, making built-in encryption common in consumer devices, including IoT devices. This situation creates a big challenge for digital forensic investigations that need to analyze data from IoT devices. This work looks into electromagnetic (EM) side-channel analysis to help with digital forensic investigations on IoT devices. EM side-channel analysis is a method that uses unintentional electromagnetic signals to secretly listen to the operations and data management of computing devices. The non-intrusive nature of EM side-channel methods makes them a suitable choice for digital forensic investigations since these techniques do not change the target device. The literature on different EM side-channel analysis attack methods is discussed, chosen based on how applicable they are in IoT device investigations. The knowledge gained from this background study is used to find promising future uses of the technique for digital forensic analysis on IoT devices, potentially improving many digital investigations that are currently facing obstacles.",
        "reference": "Internet of Things (IoT) devices (devices connected to the internet, collecting and sharing data) will be used in digital forensic investigations in the future. These devices have limited interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing focus on the security and privacy of user data, built-in encryption (which encode information) is becoming commonplace in devices. This presents a significant challenge to digital forensic investigations, where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where electromagnetic emissions are used for eavesdropping on the operations and data handling of devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected according to their applicability in IoT device investigations. The data obtained from our survey are used to identify promising future applications of the technique for digital forensic analysis on IoT devices.",
        "metrics": {
          "SARI": 40.02534690517845,
          "BLEU": 0.3769450346127235,
          "FKGL": 14.6
        }
      },
      {
        "source": "The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is of paramount importance. However, this can be difficult since the attacker has many opportunities and means to hide such changes. In particular, cloud computing, host and guest machine time can be manipulated in various ways by an attacker. Guest virtual machines are especially vulnerable to attacks coming from their (more privileged) host. As such, it is important to guarantee the timeline integrity of both hosts and guests in a cloud, or at least to ensure that the alteration of such timeline does not go undetected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. Further, we describe a novel architecture for host and guest time alteration detection and correction/resilience with respect to compromised hosts and guests. The proposed framework has been implemented on an especially built simulator.",
        "prediction": "The harmful change of machine time is a major problem in computer forensics. Performance data show that our proposal is feasible. Identifying these changes and reconstructing the real order of events is very important. However, this can be challenging because attackers have many ways to hide their changes. Specifically, in cloud computing, both the main computer (host) and the virtual computer being run (guest) can be manipulated in several ways by an attacker. Guest virtual machines are particularly at risk from attacks that come from their more powerful hosts. Therefore, it is crucial to ensure the accuracy of the timeline for both hosts and guests in a cloud environment, or at least to make sure that any changes to this timeline are not unnoticed. In this paper, we discuss the problems related to the accuracy of host and guest machine time in the cloud. Additionally, we present a new structure for detecting and correcting changes to time for both hosts and guests, particularly in situations where hosts and guests are compromised. The proposed system has been implemented on a specially designed simulator.",
        "reference": "Machine timestamp falsification can be used in cyber attacks. The algorithm detects manipulations effectively. That is why it is important to detect such changes and to reconstruct the actual timeline of events. However, the hacker can try to hide their tracks. In cloud computing, hackers can manipulate host and guest machine time. Guest virtual machines use computing resources provided by a physical machine called a host. Guest virtual machines are especially vulnerable to attacks coming from their host. That is why the timeline integrity of both hosts and guests in a cloud is important. Any manipulation of such a timeline should be detected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. A new algorithm is proposed to detect host and guest time manipulations and to correct or reject them. A simulator has been built for this purpose.",
        "metrics": {
          "SARI": 41.685598571511825,
          "BLEU": 0.13927752818068884,
          "FKGL": 10.4
        }
      },
      {
        "source": "Image tampering, being readily facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. As the most popular multimedia data, JPEG images can be easily tampered without leaving any clues; therefore, JPEG-based forensics , including the detection of double compression, interpolation, rotation, etc., has become an active research topic in multimedia forensics. Nevertheless, the interesting issue of detecting image tampering and its related operations by using the same quantization matrix has not been fully investigated. Aiming to detect such forgery manipulations under the same quantization matrix, we propose a detection method by using shift-recompression -based reshuffle characteristic features. The learning classifiers are applied for classification. Our experimental results indicate that the approach is indeed highly effective in detecting image tampering and relevant manipulations with the same quantization matrix.",
        "prediction": "Image tampering is becoming a major issue due to today's digital techniques, which make it easy to alter images without leaving any evidence. JPEG images are the most common type of multimedia data and can be easily changed. This has led to the study of JPEG-based forensics, which focuses on detecting signs of alterations, like double compression (compressing an already compressed image), interpolation (filling in missing pixels), and rotation (turning the image). However, the challenge of identifying image tampering that uses the same quantization matrix (a method for reducing file size that affects image quality) has not been thoroughly explored. To address this, we suggest a detection method based on shift-recompression, which involves rearranging features of the image. We use learning classifiers, which are algorithms that identify patterns in data, for classification. Our experimental results show that this method is very effective in detecting image tampering and related manipulations that share the same quantization matrix.",
        "reference": "Image tampering has become a serious problem with the advancement of digital techniques. JPEG images can be easily manipulated without leaving any clues. This is why researchers are trying to develop methods for JPEG image manipulation detection. However, methods based on image compression techniques, like quantization, are rarely applied. To detect such manipulations, we propose a detection method to identify inconsistencies in the quantization matrix caused by the manipulation. Then machine learning algorithms are applied to detect image manipulation. Experimental results show that the approach is highly effective in detecting image tampering.",
        "metrics": {
          "SARI": 46.146979781534455,
          "BLEU": 0.07409700521757948,
          "FKGL": 14.3
        }
      },
      {
        "source": "In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations of its monocular camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets of the whole body including the legs, as well as the camera extrinsic and intrinsic parameters. Furthermore, we developed an approach to automatically select a subset of configurations for the calibration process that yields a good trade-off between the number of observations and accuracy. Further, our approach to configuration selection yields substantially better optimization results compared to randomly chosen viable configurations. Hence, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "prediction": "In this paper, we present a new way to accurately adjust the movement model of a humanoid robot using data from its single camera. Our method estimates the settings for the complete model, which includes the joint angle adjustments for the entire body, such as the legs, along with the external and internal settings of the camera. Additionally, we created a method to automatically choose a specific group of positions for the calibration process that offers a good balance between the number of data points and the precision. Our method for selecting these positions results in significantly better optimization performance compared to randomly chosen suitable positions. Therefore, our system needs fewer positions to achieve precise results. Our optimization method is versatile, and the available online implementation can be easily used for different humanoid robots.",
        "reference": "In this paper, we present a novel approach to accurately calibrate a humanoid and its motions (robot with human form or characteristics) based on observations of its camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets and the camera parameters. We developed an approach to automatically select configurations for the calibration process that produces a good compromise between the number of observations and accuracy. Our approach to configuration selection generates better optimization results compared to randomly chosen viable configurations. Thus, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "metrics": {
          "SARI": 29.637010015852134,
          "BLEU": 0.19488203516561187,
          "FKGL": 13.2
        }
      },
      {
        "source": "In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. From the data of an onboard consumer-grade depth camera, our system estimates the robot's pose to compensate for drift of odometry and maintains a heightmap representation of the environment. Based on this model, our system iteratively computes sequences of safe actions including footsteps and whole-body motions, leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system leads to robust navigation in cluttered environments and the robot is able to traverse highly challenging passages.",
        "prediction": "In this paper, we present a system that helps humanoid robots move around on their own in unfamiliar and messy places. Using data from a basic depth camera installed on the robot, our system figures out the robot's position to correct any errors in movement tracking and keeps a height map of the surroundings. From this model, our system continuously calculates safe actions the robot can take, such as where to place its feet and how to move its body, to reach specific locations. To effectively check for potential collisions while planning these movements, we created a new method that considers the robot's shape and the obstacles it may encounter. As shown in tests with a Nao humanoid robot, our system allows for reliable navigation in complicated environments and enables the robot to successfully navigate through difficult paths.",
        "reference": "In this paper, we present a navigation system that allows humanoid robots (robots with human form or characteristics) to autonomously navigate in unknown, cluttered environments. From the data of a camera which can judge depth and distance, our system estimates the robot’s position and maintains a height representation of the environment. Our system determines safe actions (including footsteps and body motions) leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments, our system allows the robot to navigate in clustered environments and traverse challenging passages.",
        "metrics": {
          "SARI": 31.452110625842177,
          "BLEU": 0.12713921421554125,
          "FKGL": 14.1
        }
      },
      {
        "source": "In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict traversal costs corresponding to potential obstacles in regions not yet observable by the robot's sensors. By taking into account the predicted costs for path planning, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.",
        "prediction": "In this paper, we introduce a method for helping robots move efficiently in messy indoor spaces. We suggest estimating how many obstacles are nearby based on already identified objects, and we use this information to predict how difficult it will be to move through areas not yet seen by the robot's sensors. By considering these predicted difficulties when planning its path, the robot can navigate more wisely and is less likely to get stuck in crowded areas. Our experimental results show that this method allows the robot to move through cluttered environments more effectively and completes tasks in much shorter times compared to a standard method that does not use any predictions.",
        "reference": "In this paper, we introduce an approach for robot to navigate through cluttered indoor environments. We propose to estimate obstacles based on already detected objects and use them to predict potential obstacles in regions not yet observable by the robot. The robot is then able to navigate in a more anticipated manner and reduces the risk of getting stuck. Our method enables the robot to efficiently navigate through cluttered environments and faster than a standard approach not using any prediction.",
        "metrics": {
          "SARI": 28.6972079983641,
          "BLEU": 0.10173096988339922,
          "FKGL": 14.2
        }
      },
      {
        "source": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, step-by-step following of the human might not be the optimal solution since better paths for the robot exist. We propose to perform a prediction about the human's future movements and use this information in a reinforcement learning framework to generate foresighted navigation actions for the robot. Since frequent occlusions of the human will occur due to obstacles and the robot's constrained field of view, the estimate about the humans's position and the prediction of the next destination are affected by uncertainty. Our approach deals with such situations by explicitly considering occlusions in the reward function such that the robot automatically considers to execute actions to get the human in its field of view. We show in simulated and real-world experiments that our technique leads to significantly shorter paths compared to an approach in which the robot always tries to closely follow the user and, additionally, can handle occlusions.",
        "prediction": "In many situations, users follow common paths to reach certain destinations where a mobile robot's help is needed. However, depending on the environment and paths, simply following the user may not be the best option since there could be better routes for the robot. We suggest predicting the user's future movements and using this information in a reinforcement learning framework, which is a method where agents learn to make decisions by receiving rewards for good actions. This can help the robot plan better navigation steps. Since the robot often loses sight of the user due to obstacles and its limited viewing area, estimating the user's location and predicting their next stop can be uncertain. Our approach addresses these challenges by specifically accounting for these lost views in the reward system, ensuring the robot takes actions to keep the user in sight. We demonstrate in tests, both in simulations and real situations, that our method results in much shorter paths compared to when the robot tries to always closely follow the user and also manages to deal with situations where the user is not visible.",
        "reference": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, following the human might not be the optimal solution because better paths for the robot exist. We propose to predict the human’s future movements and use this information to generate navigation actions for the robot. The estimation of the human’s position and the prediction of the next destination are uncertain because of the occlusions due to obstacles and the robot’s limited field of view. Our approach deals with considering occlusions so the robot automatically considers to execute actions to get the human in its field of view. We show that our technique leads to significantly shorter paths compared to an approach in which the robot follows the user and, additionally, can handle occlusions.",
        "metrics": {
          "SARI": 27.565315628415615,
          "BLEU": 0.12120323000863623,
          "FKGL": 13.5
        }
      },
      {
        "source": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem, especially when this person moves around since the robot’s field of view is constrained and the environment structure typically poses further visibility constraints that influence the perception of the user. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. As the robot needs time to reach the possible search locations, we take this time as well as the visibility constraints into account when computing effective search locations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.",
        "prediction": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem, especially when the person moves around since the robot’s field of view, which is the area it can see at one time, is limited and the layout of the environment usually makes it harder to see the user. In this paper, we propose a new method that calculates how likely it is that the user can be seen at each possible location in the environment using Monte Carlo simulations, which are mathematical techniques that use random sampling to obtain numerical results. As the robot needs time to reach the potential search locations, we consider this time along with the visibility constraints, which are the limitations on what can be seen, when figuring out the best search locations. This way, the robot can select the next search spot that offers the highest chance of seeing the user. Our experiments in different simulated environments show that our method results in a much shorter search time compared to a greedy approach, which prioritizes immediate gains, that uses background information.",
        "reference": "An autonomous service robot often first has to search for a user to execute a task. This is a challenging problem, especially when this person moves around because the robot’s field of view is limited. The environment also poses visibility limitations that influence the perception of the user. In this paper, we propose a method that estimate the user’s observability in the environment. When estimating effective search locations, we take into account the time for the robot to reach the search location and the visibility limitations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments demonstrate that our approach leads to a significantly shorter search time compared to other approach.",
        "metrics": {
          "SARI": 40.43145986459327,
          "BLEU": 0.2732457868495931,
          "FKGL": 15.6
        }
      },
      {
        "source": "Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Considering dynamic obstacles introduces another level of complexity, since they can lead to necessary replanning or collisions at later stages. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.",
        "prediction": "Perception of the local environment is essential for mobile robots to navigate safely in changing surroundings. Most robots, such as humanoids and smaller wheeled robots, rely on flat areas for movement. For humanoid robots, a basic two-dimensional occupancy map, which shows where obstacles are and helps plan a path, is not enough because they can step over and onto objects, so they need information about height. Adding dynamic obstacles, which are objects that can move, makes things more complicated as they may require adjusting the path or could cause collisions later on. In this paper, we introduce a system that first identifies flat areas in height maps and finds dynamic obstacles. Our system then uses this information to create prediction maps, allowing paths to be planned quickly and efficiently in real time with low central processing unit cost. We demonstrate in simulations and real-world tests that our system keeps processing times below 10 milliseconds for each computation cycle and enables advanced real-time three-dimensional footstep planning.",
        "reference": "Perception of the environment is necessary for mobile robots to navigate safely in dynamic environments. Most robots (humanoids and wheeled robots) rely on planar regions. For humanoids (robots with human form or characteristics), a 2D map is not sufficient because they can step over and onto objects, so they need height information. Dynamic obstacles bring another level of complexity, because it can lead to necessary replanning or collisions. In this paper, we present a framework that extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be planned in real time at low processor cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms and allows for real-time 3D footstep planning.",
        "metrics": {
          "SARI": 36.70083470343579,
          "BLEU": 0.2656255047894971,
          "FKGL": 13.7
        }
      },
      {
        "source": "In this paper, a lab automation drone notional concept is introduced. Here, a robotic limb is attached to a robotic rotorcraft. The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high throughput systems (HTS). The resulting drone could augment existing HTS operations. The 6 degree-of-freedom (DOF) arm and gripper design are presented. Test-and-evaluation approach and results are also given.",
        "prediction": "In this paper, a concept for a lab automation drone is introduced. This drone includes a robotic arm attached to a flying robot known as a rotorcraft. The arm has a gripper that allows the drone to skillfully handle small items like micro-arrays and test tubes, which are commonly used in high throughput systems. This new drone could enhance current operations in these systems. The design of the arm and gripper, which can move in 6 different directions, is presented. The paper also includes the methods used for testing and the evaluation results.",
        "reference": "In this paper, a concept of lab automation drone is introduced. Here, a robotic arm is attached to a drone. The arm’s gripper allows the drone to manipulate objects such as chips and test tubes often used in scientific experimentations like high throughput systems. The drone could augment existing high throughput systems operations. The 6 degree-of-freedom arm and gripper design are presented. Results of evaluations and tests are also given.",
        "metrics": {
          "SARI": 42.8359901521511,
          "BLEU": 0.21731131839135406,
          "FKGL": 8.2
        }
      },
      {
        "source": "Robot companionship has become more popular in past years. However, humanoid gait might be somewhat unstable for these applications. Even with miniature humanoids, falls occur frequently. Thus, wheel attachments have been added onto a miniature humanoid, so it can move faster and with more stability than walking. In addition, with such attachments a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as an experimentation and performance evaluation platform. This paper discusses preliminary work regarding robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "prediction": "Robot companionship has become more popular in recent years. However, humanoid gait, which refers to how robots move in a way similar to humans, might be somewhat unstable for these applications. Even with small humanoid robots, falls happen often. Therefore, wheel attachments have been added to a small humanoid robot so it can move faster and more steadily than by walking. Additionally, with these attachments, a robot can switch from walking to rolling when needed. DARwIn-OP is a humanoid robot that has been used for experimentation and performance evaluation. This paper discusses initial work on robot companionship applications using a small humanoid robot that can fetch different toys based on voice commands.",
        "reference": "Robot companionship has become more popular in past years. However, humanoid (robots with human form or characteristics) walking might be unstable. Even with miniature humanoids, falls occur frequently. Wheels have been added onto a miniature humanoid, so it can move faster and with more stability than walking. With wheels, a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used for experimentation and evaluation. This paper discusses preliminary work for robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "metrics": {
          "SARI": 36.48527079783035,
          "BLEU": 0.38565181167051543,
          "FKGL": 9.5
        }
      },
      {
        "source": "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds of up to 6m/sec. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning based navigation. With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest. In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot. Derived from the classic image classification algorithms, our classifier has been constructed in the form of a fast 39-layered Inception model, that evaluates the presence of roads using the tomographic reconstructions of the input frames. Based on the Inception-v3 architecture, our system performs better in terms of processing complexity and accuracy than many existing models for imitation learning. The data used for training the system has been captured from the drone, by flying it in and around urban and semi-urban streets, by experts having at least 6-8 years of flying experience. Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process. With the extensive amount of drone data that we collected, we have been able to navigate successfully through roads without crashing or overshooting, with an accuracy of 98.44%.",
        "prediction": "In the modern era of automation and robotics, autonomous vehicles, which are vehicles that can drive or fly without human intervention, are currently the focus of research in both academic and industrial settings. The computational efficiency of MAVNet, which is a specific type of technology used for controlling drones, enables the drone to fly at high speeds of up to 6 meters per second. We present the same results in this research and compare them with other advanced methods of vision and learning-based navigation. With the ever-increasing number of unmanned aerial vehicles, or drones, being used in civilian and commercial activities, there is a greater need for these systems to operate autonomously. Due to guidelines set by governments regarding how high civil drones can operate, there is growing interest in navigation that focuses on tracking roads. To achieve the aforementioned tasks, we propose a solution based on imitation learning, which means teaching the drone to navigate by mimicking an expert pilot. Our classifier is designed using a fast 39-layered Inception model, which is a type of deep learning model used for image classification, that evaluates the presence of roads based on detailed images taken from various angles. Based on the Inception-v3 architecture, our system performs better in terms of how much processing power it requires and its accuracy compared to many existing models for imitation learning. The data used to train the system was collected by flying the drone in urban and semi-urban areas, captured by experts who have at least 6 to 8 years of flying experience. Permissions were obtained from the relevant authorities to ensure that minimal risk to pedestrians was involved in the data collection process. With the extensive amount of data collected from drones, we have successfully navigated through roads without crashing or overshooting, achieving an accuracy rate of 98.44%.",
        "reference": "Current academic and industrial research is interested in autonomous vehicles. MAVNet computational efficiency enables the drone to fly up to 6m/sec. The solution is compared with other recent methods. Drones are increasingly used in the civilian and commercial domain and need to be autonomous. Governments set guidelines on the operation ceiling of civil drones. So, road-tracking based navigation is attracting interest. Researchers propose data-driven solutions allowing drones to autonomously navigate city streets, learning to fly by imitating an expert pilot. The algorithm, based on the Inception model, detects roads using tomographic reconstructions. The Inception model is an extension of classic image classification algorithms. The Inception-v3 architecture has better accuracy than many existing models of imitation learning. The data used for training the system was captured from the drone over urban streets, navigated by an expert pilot. Data collection requires special permission to ensure pedestrian security. The drone can navigate successfully through roads without crashing or overshooting; its accuracy is 98.44%.",
        "metrics": {
          "SARI": 38.44407576217599,
          "BLEU": 0.09516834465706811,
          "FKGL": 15.2
        }
      },
      {
        "source": "Advances in electronics and sensor technology have widened the scopes of networked drones to include applications as diverse as surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording for their operations. Large drones are used singly in missions while small ones are used in formations or swarms. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "prediction": "Advances in electronics and sensor technology have expanded the uses of networked drones to include a variety of tasks such as surveillance, video recording, operations, entertainment and advertising, signal emission, transportation, and delivery. These tasks require video recording for their operations. Large drones are typically used individually for missions, while small drones are often used together in formations or swarms. Small drones are becoming increasingly useful in civilian applications. The consideration of small drones for purposes such as group flight, entertainment, and signal emission has led to the deployment of networked drones. To create group display applications, a real-time drone formation control for group display has been proposed. Simulations indicate that drone formations can effectively display messages.",
        "reference": "Advances in technology allowed drones to be used for surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording. Large drones are used individually while small ones are used in groups. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "metrics": {
          "SARI": 31.979788881888542,
          "BLEU": 0.4217514815700717,
          "FKGL": 12.2
        }
      },
      {
        "source": "This study focuses on the digital marketing capabilities of tourism SMEs. The study addresses the question of how the use of ICT-based tools benefit the organisational capabilities of a company. By adopting marketing as a set-of-skills approach, the study provides new insights into the existing tourism literature on e-marketing. Initial findings indicate that the digital marketing capabilities of companies are transformed through ICT-based uptake. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that the use of ICT-based tools transforms digital marketing capabilities from a set of abilities that enables tourism SMEs not only to float in a web-marketing stream, but also to lead such a stream.",
        "prediction": "This study focuses on the digital marketing abilities of small and medium-sized enterprises (SMEs) in tourism. It examines how using Information and Communication Technology (ICT) tools can improve a company's organizational skills. By viewing marketing as a collection of skills, the study offers new perspectives on existing research about online marketing in tourism. Initial findings show that companies’ digital marketing abilities change when they use ICT tools. Four main abilities were identified, each growing as a result of using these tools. A key finding is that using ICT tools changes digital marketing abilities from just a set of skills that help tourism SMEs keep up in online marketing to also enabling them to lead in this area.",
        "reference": "This study focuses on the digital marketing capabilities of tourism small and medium-sized enterprises (SMEs). The study questions on how information and communication technology (ICT) tools can help the organisational capabilities of a company. The study provides new insights from tourism studies on digital marketing. Findings show that digital marketing capabilities of companies are transformed by ICT tools. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that ICT tools transform these capabilities in a set of abilities for SMEs in web-marketing.",
        "metrics": {
          "SARI": 47.6352522004179,
          "BLEU": 0.2511125064042657,
          "FKGL": 12.1
        }
      },
      {
        "source": "The prevalence and rapid development of the Internet and mobile technology in recent decades has revamped our living styles and daily habits. To ride on the digital trend, more business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. The rise of Key Opinion Leaders (KOLs), social media platforms, and Omni-channel retailing have attracted countless business entities to consider the adoption of digital marketing tools for promoting and advertising their brands and products. However, with the increasing diversity of the types of digital marketing tools, they must be carefully selected based on a multiple number of criterion. In this paper, a fuzzy-AHP method is proposed and developed for assisting industry practitioners in systematically and effectively evaluate and select proper digital marketing tool(s) for adoption. The developed method not only streamlines the internal business process of digital marketing tool selection, but it also increases the practitioner's effectiveness of achieving the pre-defined strategic marketing objectives.",
        "prediction": "The widespread use and quick advancement of the Internet and mobile technology in recent years have changed how we live and behave daily. To keep up with this digital trend, many businesses have moved their activities online. Marketing and advertising are key areas that have undergone digital transformation. The emergence of Key Opinion Leaders (KOLs), which are influential figures on social media, as well as social media platforms and Omni-channel retailing, which integrates various shopping channels, have encouraged many businesses to use digital marketing tools to promote their brands and products. However, due to the growing variety of digital marketing tools available, businesses must carefully choose which ones to use based on several criteria. In this paper, a fuzzy Analytical Hierarchy Process (fuzzy-AHP) method is proposed and developed to help industry professionals systematically and effectively evaluate and select appropriate digital marketing tool(s) for use. This method not only simplifies the internal processes involved in choosing digital marketing tools but also enhances the effectiveness of professionals in reaching their clearly defined marketing goals.",
        "reference": "The development of the Internet and mobile technology in recent decades has modified our living styles and daily habits. More business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. Many businesses consider the adoption of digital marketing tools for their promotion and advertisement, due to the rise of Key Opinion Leaders (person with expert status and influence), social media platforms, and Omni-channel retailing (multiple channels but a uniform experience for the customer). However, with the increasing diversity of digital marketing tools, they must be carefully selected. In this paper, a method to organize and analyze complex decisions is proposed and developed for assisting businesses to evaluate and select appropriate digital marketing tools. The developed method rationalizes and simplifies the process of digital marketing tool selection. It also increases effectiveness of achieving strategic marketing objectives.",
        "metrics": {
          "SARI": 33.98271703403799,
          "BLEU": 0.20930121342213576,
          "FKGL": 15.2
        }
      },
      {
        "source": "In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be primarily contingent on changes in muscle size. Given this assumption, there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength. However, the conclusion that a change in muscle size affects a change in strength is surprisingly based on little evidence. We suggest that these changes may be completely separate phenomena based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with detraining, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load resistance training, yet divergent results in strength.",
        "prediction": "In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength Muscle hypertrophy refers to the growth and increase of muscle cells To this day long-term adaptations in strength are thought to be primarily contingent on changes in muscle size Given this assumption there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength However the conclusion that a change in muscle size affects a change in strength is surprisingly based on little evidence We suggest that these changes may be completely separate phenomena based on the weak correlation between the change in muscle size and the change in muscle strength after training the loss of muscle mass with detraining yet a maintenance of muscle strength and the similar muscle growth between low-load and high-load resistance training yet divergent results in strength",
        "reference": "In this paper we revisit a topic originally discussed in 1955, namely the lack of evidence that muscle growth plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be related to changes in muscle size. Given this assumption, training programs made for increasing both muscle size and strength were popular. However, the conclusion that a change in muscle size has an influence on strength is surprisingly based on little evidence. We suggest that these changes may be based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with the interruption of training, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load strength training, yet different results in strength.",
        "metrics": {
          "SARI": 27.03037931057246,
          "BLEU": 0.5581714336323561,
          "FKGL": 64.9
        }
      },
      {
        "source": "Low-intensity occlusion (50-100 mm hg) training provides a unique beneficial training mode for promoting muscle hypertrophy. Training at intensities as low as 20% 1 repetition maximum with moderate vascular occlusion results in muscle hypertrophy in as little as 3 weeks. A typical exercise prescription calls for 3 to 5 sets to volitional fatigue with short rest periods. The metabolic buildup causes positive physiologic reactions, specifically a rise in growth hormone that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "prediction": "Low-intensity occlusion training is a type of exercise that involves applying pressure to blood vessels at a level between 50 to 100 millimeters of mercury (mm Hg). This training is beneficial for building muscle size, known as muscle hypertrophy. You can achieve muscle hypertrophy by training at low intensities, around 20% of what a person can lift for one complete repetition, while restricting blood flow. This approach can show results in as little as 3 weeks. A standard workout includes performing 3 to 5 sets of an exercise until the muscles are very tired, with short breaks in between. This method leads to a buildup of metabolic byproducts in the muscles, which triggers healthy responses in the body, notably increasing the level of growth hormone beyond what occurs with heavier weights. Occlusion training is useful for people who cannot lift heavy weights, such as those with joint pain, individuals recovering from surgery, patients in cardiac rehabilitation, athletes who are reducing their training load, and astronauts.",
        "reference": "Low-intensity occlusion training, which is a training method with blood flow restriction, provides a unique beneficial training mode for muscle growth. Training at intensities as low as 20% of maximum strength with moderate occlusion results in muscle growth in 3 weeks. A typical exercise consists of 3 to 5 sets to muscle failure with short rest periods. The metabolic accumulation causes positive physiologic reactions, specifically a rise in growth hormone, which is one of the main hormone responsible of muscle growth, that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "metrics": {
          "SARI": 26.534030534396397,
          "BLEU": 0.12261201811918156,
          "FKGL": 12.5
        }
      },
      {
        "source": "Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. We and others have demonstrated that aerobic exercise acutely and chronically alters protein metabolism and induces skeletal muscle hypertrophy. These findings promote an antithesis to the status quo by providing novel perspective on skeletal muscle mass regulation and insight into exercise-countermeasures for populations prone to muscle loss.",
        "prediction": "Current beliefs suggest that aerobic exercise, which is any physical activity that uses large muscle groups and increases heart rate, has little impact on the size of skeletal muscle, which is the muscle attached to bones that helps with movement. However, we and others have shown that aerobic exercise can change how proteins are made and used in the body both in immediate and long-term ways, leading to an increase in the size of skeletal muscle, known as skeletal muscle hypertrophy. These results challenge the common understanding and offer a new viewpoint on how skeletal muscle mass is controlled, as well as providing ideas for exercise strategies that can help people at risk of losing muscle.",
        "reference": "Current belief suggests cardio exercise training has minimal effect on muscle size. We and others have demonstrated that cardio exercise alters protein metabolism (responsible for protein synthesis and muscle growth) and induces muscle growth. These findings against the current belief provide a novel perspective on muscle mass regulation and insight into exercise to prevent muscle loss.",
        "metrics": {
          "SARI": 30.877637245384946,
          "BLEU": 0,
          "FKGL": 17.2
        }
      },
      {
        "source": "Stretch training is widely used in a variety of fitness-related capacities such as increasing joint range of motion, preventing contractures and alleviating injuries. Moreover, some researches indicate that stretch training may induce muscle hypertrophy; however, studies on the topic have been primarily relegated to animal and in vitro models. The purpose of this brief review was to evaluate whether stretch training is a viable strategy to induce muscle hypertrophy in humans. Of the 10 studies identified, 3 observed some significantly positive effects of stretch training on muscle structure. Intriguingly, in these studies, the stretching was carried out with an apparatus that aided in its performance, or with an external overload. Of the 5 available studies that integrated stretching into a resistance training programme, 2 applied the stretching in the interset rest period and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to confer beneficial changes in muscle size and architecture; alternatively, albeit limited evidence suggests that when stretching is done with a certain degree of tensile strain (particularly when loaded, or added between active muscle contractions) may elicit muscle hypertrophy.",
        "prediction": "Stretch training is commonly used in fitness for various purposes like increasing the range of motion in joints, preventing stiffness in muscles, and helping with injuries. Some research suggests that stretch training might help muscles grow larger, but most studies have focused on animals or cells in a laboratory setting. This brief review aimed to check if stretch training can effectively promote muscle growth in humans. Out of 10 studies found, 3 showed significant positive effects of stretch training on muscle structure. Interestingly, in these studies, the stretching was done with a device that helped or with added weight. Of the 5 studies that included stretching in a resistance training program, 2 used stretching during the breaks between sets and showed more muscle growth. In summary, gentle, low-intensity stretching does not seem to result in changes to muscle size or structure. However, some limited evidence suggests that stretching performed with some tension, especially when combined with weight or done between muscle contractions, may lead to muscle growth.",
        "reference": "Stretch training is used in a variety of fitness capacities such as increasing flexibility, preventing contractures (shortening of muscle) and reduce injuries. Moreover, some researches indicate that stretch training may induce muscle growth. The purpose of this review was to evaluate if stretch training is a viable strategy to induce muscle growth in humans. Of the 10 studies identified, 3 observed positive effects of stretch training on muscles. In these studies, the stretching was performed with external help like equipment or load. Of the 5 available studies that integrated stretching into strength training, 2 applied the stretching in the rest period between the sets and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to produce beneficial changes in muscles but evidence suggests that when stretching is done with a certain intensity (particularly when loaded, or added between active muscle contractions), it may induce muscle growth.",
        "metrics": {
          "SARI": 34.328002712396405,
          "BLEU": 0.17493522936921918,
          "FKGL": 11.4
        }
      },
      {
        "source": "Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. Therefore, the purpose of this review is to discuss the effects of cycle training on muscle size and strength of the lower extremity and the possible mechanisms for increasing muscle size with cycle training. It is plausible that cycle training requires a longer period to significantly increase muscle size compared to typical resistance training due to a much slower hypertrophy rate. Cycle training induces muscle hypertrophy similarly between young and older age groups, while strength gain seems to favor older adults, which suggests that the probability for improving in muscle quality appears to be higher in older adults compared to young adults. For young adults, higher-intensity intermittent cycling may be required to achieve strength gains. It also appears that muscle hypertrophy induced by cycle training results from the positive changes in muscle protein net balance.",
        "prediction": "Cycle training is commonly used as an important part of exercise programs aimed at improving aerobic capacity and cardiovascular health. However, its impact on muscle size and strength still needs more investigation, although it is known that professional cyclists have larger muscles than non-cyclists. Thus, this review aims to explore how cycle training affects the muscle size and strength of the lower body and the possible reasons for muscle size increase from cycle training. It is likely that cycle training takes a longer time to noticeably increase muscle size when compared to regular resistance training due to a slower muscle growth rate. Cycle training causes muscle growth in both younger and older individuals, while strength gains appear to benefit older adults more. This indicates that there is a greater chance of improving muscle quality in older adults than in younger ones. For young adults, higher-intensity cycling that involves short bursts may be necessary to see strength improvements. Muscle growth from cycle training seems to come from positive changes in the balance of muscle proteins in the body.",
        "reference": "Cycling training is performed as a major part of any training program which objective is to improve cardiovascular capacities and health. However, the effect of cycling training on muscle size and strength still requires a better comprehension. Therefore, the purpose of this review is to discuss the effects of cycling training on muscle size and strength of the legs and the possible mechanisms for increasing muscle size with cycling training. It is plausible that cycling training requires a longer period to significantly increase muscle size compared to strength training due to a much slower muscle growth rate. Cycling training induces muscle growth similarly between young and older people, while strength gain seems to favor older adults. For young adults, series of high-intensity cycling may be required to achieve strength gains. It also appears that muscle growth induced by cycling training results from the positive changes in protein balance, which is the balance between protein intake which leads to muscle growth and protein loss which leads to muscle loss).",
        "metrics": {
          "SARI": 32.187988494123964,
          "BLEU": 0.18696251813672413,
          "FKGL": 11.9
        }
      },
      {
        "source": "Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship between training dose/volume and physiological response has been hypothesized to exist. However, the majority of available evidence supports a clear dose-response relationship between resistance training volume and physiological responses, such as muscle hypertrophy and health outcomes. Additionally, there is a paucity of data to support the inverted U-shaped response. The overarching principle argued herein is that volume is the most easily modifiable variable that has the most evidenced-based response with important repercussions, be these muscle hypertrophy or health-related outcomes.",
        "prediction": "Resistance training is the most effective method to increase muscle mass. It also offers many health benefits. Although it is considered safe and important for treating and preventing many diseases, research has focused on finding a time-efficient and minimal amount of exercise needed. There is a theory that suggests an inverted U-shaped relationship exists, which means that as the amount of training increases, there might be a peak level after which additional training does not produce more benefits and may even cause harm. However, most evidence supports a clear dose-response relationship, meaning that as the volume of resistance training increases, the physiological responses like muscle growth (hypertrophy) and health outcomes improve. Furthermore, there is a lack of data to support the inverted U-shaped response. The main idea presented here is that training volume is the easiest factor to adjust, and it has the strongest evidence showing its impact on important outcomes, whether related to muscle growth or overall health.",
        "reference": "Strength training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is helpful for treating and preventing diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship (which means that the efficiency increases until a point, and then starts decreasing) between training volume and physiological response has been hypothesized to exist. However, evidence supports a clear correlation between resistance training volume (number of exercises, sets, etc.) and physiological responses, such as muscle growth and health outcomes. Additionally, there is a lack of data to support the inverted U-shaped response. The principle suggested is that volume is the most easily modifiable variable that has the most important response, for muscle growth or health outcomes.",
        "metrics": {
          "SARI": 39.41319869286012,
          "BLEU": 0.30734907054128185,
          "FKGL": 13.3
        }
      },
      {
        "source": "Although the effects of short versus long inter-set rest intervals in resistance training on measures of muscle hypertrophy have been investigated in several studies, the findings are equivocal and the practical implications remain unclear. Current evidence indicates that both short and long inter-set rest intervals may be useful when training for achieving gains in muscle hypertrophy. Novel findings involving trained participants using measures sensitive to detect changes in muscle hypertrophy suggest a possible advantage for the use of long rest intervals to elicit hypertrophic effects. However, due to the paucity of studies with similar designs, further research is needed to provide a clear differentiation between these two approaches.",
        "prediction": "The effects of short and long rest periods between sets in resistance training on muscle growth have been studied, but the results are mixed and not very clear. Current evidence shows that both short and long rest periods can help increase muscle size. New findings involving trained individuals using sensitive measurements indicate that longer rest periods might have an advantage for muscle growth. However, because there are not many studies with similar methods, more research is needed to clearly differentiate between these two approaches.",
        "reference": "The effects of short versus long rest intervals between sets in strength training on muscle growth have been investigated in several studies but the findings and the practical implications are unclear. Current evidence indicates that both short and long rest intervals may be useful for muscle growth. Novel findings suggest for trained people a possible advantage for the use of long rest intervals to induce muscle growth. However, due to the lack of studies with similar methodologies, further research is needed to provide a clear differentiation between these two approaches.",
        "metrics": {
          "SARI": 38.97223037409961,
          "BLEU": 0.2847135904756785,
          "FKGL": 11.5
        }
      },
      {
        "source": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review describes a cellular memory in skeletal muscle in which hypertrophy is 'remembered' such that a fibre that has previously been large, but subsequently lost its mass, can regain mass faster than naive fibres. A new cell biological model based on the literature, with the most reliable methods for identifying myonuclei, can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells before hypertrophic growth. Even if subsequently subjected to grave atrophy, the higher number of myonuclei is retained, and the myonuclei seem to be protected against the elevated apoptotic activity observed in atrophying muscle tissue. Fibres that have acquired a higher number of myonuclei grow faster when subjected to overload exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit in the elderly, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "prediction": "Memory is a process in which information is encoded, stored, and retrieved. In vertebrates, the current understanding is that this process occurs only in the brain. This review discusses a type of cellular memory in skeletal muscle that allows it to \"remember\" hypertrophy, meaning an increase in size. This means that a muscle fiber that was once large but has lost its mass can regain that mass faster than new, untrained fibers. A new cell biology model based on existing research and the best methods for identifying myonuclei explains this phenomenon. Myonuclei are nuclei, which are the control centers within muscle cells, that help manage muscle functions. The model suggests that previously untrained muscle fibers recruit myonuclei from satellite cells, which are specialized cells that help in muscle repair and growth, before going through hypertrophic growth. Even if these fibers undergo significant atrophy, which is a decrease in size, they retain a higher number of myonuclei. These myonuclei seem to be protected from the increased cell death activity associated with shrinking muscle tissue. Fibers that have more myonuclei grow faster when subjected to overload exercise, meaning exercise that places more than normal strain on the muscles. Therefore, these nuclei act as a significant form of \"memory\" for previous strength. This muscle memory may last a very long time in humans, as myonuclei remain stable for at least 15 years and might even be permanent. However, it is more challenging to recruit myonuclei in older individuals, and if this long-lasting muscle memory is also present in humans, early strength training should be considered important public health advice.",
        "reference": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, it has been said that it occurs only in the brain. This review describes a cellular memory in muscles in which muscle growth is 'remembered' such that a fibre that has lost its mass, can regain it faster than untrained fibres. A new model, with the most reliable methods for identifying myonuclei (nuclei in muscle fiber), can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells (quiescent cells acting as a reserve population of cells for regeneration of muscle) before muscle growth. Even after grave muscle loss, the higher number of myonuclei is retained, and the myonuclei seem to be protected against programmed cell death observed during muscle loss. Fibres that have acquired a higher number of myonuclei grow faster during strength exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit for old people, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "metrics": {
          "SARI": 32.11354243876705,
          "BLEU": 0.2617565213034862,
          "FKGL": 11.3
        }
      },
      {
        "source": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several observational studies lend support to this hypothesis, with an inverse relationship noted between the frequency of eating and adiposity. The purpose of this narrative review is to present and discuss a meta-analysis with regression that evaluated experimental research on meal frequency with respect to changes in fat mass and lean mass. Feeding frequency was positively associated with reductions in fat mass and body fat percentage as well as an increase in fat-free mass. However, sensitivity analysis of the data showed that the positive findings were the product of a single study, casting doubt as to whether more frequent meals confer beneficial effects on body composition. In conclusion, although the initial results of this meta-analysis suggest a potential benefit of increased feeding frequencies for enhancing body composition, these findings need to be interpreted with circumspection.",
        "prediction": "It has been suggested that eating small, frequent meals may help with fat loss and better weight maintenance. Several observational studies support this idea, showing a negative relationship between how often people eat and their body fat levels. The aim of this review is to present and discuss a meta-analysis, which is a type of research that combines results from multiple studies, evaluating experimental research on how often people eat in relation to changes in fat mass and lean mass. Meal frequency was positively linked to reduced fat mass and body fat percentage as well as an increase in fat-free mass. However, a closer examination of the data showed that the positive results came from a single study, raising questions about whether more frequent meals truly have beneficial effects on body composition. In conclusion, while the initial results of this meta-analysis suggest there may be benefits to eating more frequently for improving body composition, these findings should be looked at carefully.",
        "reference": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps for better weight maintenance. Several studies support this hypothesis, with a link between the frequency of eating and being overweight. This review aims to present and discuss a research on meal frequency with respect to changes in body mass. Eating frequency was positively associated with reductions in fat mass and increasing in fat-free mass. However, the positive findings were the product of a single study, leaving a doubt about the beneficial effect of frequent meals on body composition. In conclusion, although the initial results suggest a potential benefit of increased eating frequencies for enhancing body composition, these findings need to be carefully interpreted.",
        "metrics": {
          "SARI": 35.798162043995085,
          "BLEU": 0.23254372076804422,
          "FKGL": 13.7
        }
      },
      {
        "source": "Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by resting metabolic rate reduction and fat accumulation. Ten weeks of resistance training may increase lean weight by 1.4 kg, increase resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of resistance training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Resistance training may assist prevention and management of type 2 diabetes by decreasing visceral fat, reducing HbA1c, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Resistance training may enhance cardiovascular health, by reducing resting blood pressure, decreasing low-density lipoprotein cholesterol and triglycerides, and increasing high-density lipoprotein cholesterol. Resistance training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Resistance training may be effective for reducing low back pain and easing discomfort associated with arthritis and fibromyalgia and has been shown to reverse specific aging factors in skeletal muscle.",
        "prediction": "Inactive adults lose 3% to 8% muscle mass every ten years, which also leads to a decrease in resting metabolic rate and an increase in fat accumulation. After ten weeks of resistance training, people may gain 1.4 kilograms of lean weight, increase their resting metabolic rate by 7%, and lose 1.8 kilograms of fat weight. The benefits of resistance training include better physical performance, improved movement control, increased walking speed, greater functional independence, enhanced cognitive abilities, and higher self-esteem. Resistance training can help prevent and manage type 2 diabetes by reducing visceral fat, lowering HbA1c (a common blood test that measures average blood sugar levels), increasing the density of glucose transporter type 4 (a protein involved in transporting glucose into cells), and improving insulin sensitivity (how effectively the body uses insulin). It may also improve heart health by lowering resting blood pressure, reducing low-density lipoprotein cholesterol (often referred to as \"bad\" cholesterol) and triglycerides (a type of fat in the blood), while increasing high-density lipoprotein cholesterol (known as \"good\" cholesterol). Furthermore, resistance training may aid in bone development, with studies indicating a 1% to 3% increase in bone mineral density (the amount of bone mass). Additionally, resistance training can help reduce low back pain and alleviate discomfort related to arthritis and fibromyalgia, and it has been shown to reverse certain aging effects in skeletal muscle.",
        "reference": "Per decade, inactive adults experience a muscle loss of 3% to 8%, a reduction of resting metabolic rate (energy consumed by the body at rest) and fat accumulation. Ten weeks of strength training may increase fat-free mass by 1.4 kg, resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of strength training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Strength training may assist prevention and management of type 2 diabetes by decreasing abdominal fat, reducing HbA1c which is a form of hemoglobin linked to a sugar, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Strength training may enhance cardiovascular health, by reducing resting blood pressure, decreasing 'bad' cholesterol and triglycerides (main constituents of body fat), and increasing 'good' cholesterol. Strength training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Strength training may be effective for reducing low back pain, joint and muscle pain, and reverse specific aging factors in muscles.",
        "metrics": {
          "SARI": 32.520750169220655,
          "BLEU": 0.22320743801930215,
          "FKGL": 14.2
        }
      },
      {
        "source": "We hypothesized that compared to post-exercise protein intake, co-ingestion of alcohol would down-regulate translation initiation signaling and decrease rates of MPS.",
        "prediction": "We proposed that when people consume protein after exercise, drinking alcohol at the same time would reduce the signaling process that starts protein creation in the body and would lower the rate of muscle protein synthesis, which is the process where the body builds new muscle proteins.",
        "reference": "We assumed that consumption of protein and alcohol (compared to protein only) would reduce protein synthesis.",
        "metrics": {
          "SARI": 55.85801453448512,
          "BLEU": 0,
          "FKGL": 20.4
        }
      },
      {
        "source": "Eight healthy physically active male subjects (age 21.4±4.8 yr, body mass (BM) 79.3±11.9 kg, peak oxygen uptake (VO2peak) 48.1±4.8 mL·kg−1·min−1, leg extension one repetition maximum (1RM) 104±20 kg; values are mean ± SD) who had been participating in regular exercise (3 times wk−1 for >6 months) volunteered for this study.",
        "prediction": "Eight healthy and physically active male subjects aged approximately 21 years with a body mass of around 79 kilograms and a peak oxygen uptake of about 48 mL per kilogram per minute participated in this study. Their leg extension one repetition maximum, which is the maximum weight they could lift once, was around 104 kilograms. All subjects had been exercising regularly, three times a week for more than six months.",
        "reference": "Eight healthy physically active men (3 trainings per week for at least 6 months) volunteered for this study.",
        "metrics": {
          "SARI": 37.51805292105783,
          "BLEU": 0,
          "FKGL": 11.2
        }
      },
      {
        "source": "The study employed a randomized counter-balanced, cross-over design in which each subject completed bouts of consecutive resistance, continuous and intermittent high-intensity exercise with either post-exercise ingestion of alcohol-carbohydrate (ALC-CHO), alcohol-protein (ALC-PRO) or protein only (PRO) beverages on three separate occasions.",
        "prediction": "The study used a randomized counter-balanced, cross-over design. This means that each participant went through different types of exercises in a specific order to reduce bias and ensure reliable results. Each subject completed sessions of consecutive resistance exercise, as well as continuous and intermittent high-intensity exercise. After these exercises, subjects consumed one of three types of beverages on three different occasions. The beverages were either alcohol-carbohydrate (ALC-CHO), which contains both alcohol and carbohydrates, alcohol-protein (ALC-PRO), which includes both alcohol and protein, or a protein-only (PRO) beverage that contains only protein.",
        "reference": "Subjects did high intensity exercise and just after consumed an alcohol-carbohydrate, an alcohol-protein or a protein drink in three separate occasions.",
        "metrics": {
          "SARI": 36.15873268446883,
          "BLEU": 0,
          "FKGL": 15
        }
      },
      {
        "source": "Resistance exercise consisted of eight sets of five repetitions at ∼80% of 1RM.",
        "prediction": "Resistance exercise involved doing eight sets of five repetitions each at about 80% of one-repetition maximum. One-repetition maximum (1RM) is the maximum amount of weight that a person can lift for a single repetition of a given exercise.",
        "reference": "Training consisted of eight sets of five repetitions of leg extension at 80% of their maximum strength.",
        "metrics": {
          "SARI": 50.54489616108806,
          "BLEU": 0.1204774450467807,
          "FKGL": 13.1
        }
      },
      {
        "source": "After completion of the final set, subjects rested for 5 min before commencing 30 min of continuous cycling at ∼63% PPO (∼70% VO2peak).",
        "prediction": "After finishing the last exercise group, participants took a 5-minute break before starting 30 minutes of uninterrupted cycling at approximately 63% of their peak power output, which is about 70% of their maximum oxygen uptake.",
        "reference": "After 5 minutes rest, subjects cycled for 30 minutes at medium intensity,",
        "metrics": {
          "SARI": 54.08084428514536,
          "BLEU": 0,
          "FKGL": 18.1
        }
      },
      {
        "source": "Upon completion, subjects rested on the bike for 2 min before undertaking 10×30 s high intensity intervals at ∼110% of PPO, with 30 s active recovery (∼50% PPO) between each work bout.",
        "prediction": "After finishing, the participants rested on the bike for 2 minutes before doing 10 intervals of 30 seconds of high intensity exercise at about 110% of their Peak Power Output (PPO), with 30 seconds of active recovery at around 50% of their PPO between each exercise period.",
        "reference": "then 5 minutes at high intensity.",
        "metrics": {
          "SARI": 37.82967032967033,
          "BLEU": 0,
          "FKGL": 21.6
        }
      },
      {
        "source": "Immediately following exercise and after 4 h recovery, subjects ingested a 500 mL solution of either protein (PRO, 25 g whey protein powder; ISO8, Musashi, Melbourne, VIC Australia) or an energy-match in the form of CHO (25 g maltodextrin, International Health Investments, Helensvale, QLD Australia).",
        "prediction": "Immediately after exercise and after 4 hours of recovery, participants consumed a 500 milliliter solution containing either protein or an energy-equivalent amount of carbohydrates. The protein solution contained 25 grams of whey protein powder from ISO8, a brand by Musashi located in Melbourne, Victoria, Australia. The carbohydrate solution contained 25 grams of maltodextrin, which is produced by International Health Investments based in Helensvale, Queensland, Australia.",
        "reference": "Immediately following exercise and after 4 h recovery, subjects consumed a 500mL drink of protein or carbohydrates.",
        "metrics": {
          "SARI": 42.13409545135697,
          "BLEU": 0.0509127942287191,
          "FKGL": 15.3
        }
      },
      {
        "source": "Furthermore, a CHO-based meal (1.5 g·kg−1 BM) was consumed ∼2 h post-exercise, immediately after the muscle biopsy, according to recommendations for post-exercise glycogen recovery [24].",
        "prediction": "Furthermore, a carbohydrate-based meal (1.5 grams per kilogram of body mass) was consumed approximately 2 hours after exercise, immediately after the muscle sample collection, according to recommendations for recovering glycogen after exercise.",
        "reference": "A carbohydrate meal was consumed 2 h after exercise, according to recommendation for glycogen recovery.",
        "metrics": {
          "SARI": 44.556210726169354,
          "BLEU": 0,
          "FKGL": 16.6
        }
      },
      {
        "source": "The 8 h time frame represents an important phase of post-exercise recovery [1] as well as the period during which blood alcohol concentrations are likely to be elevated by a post-event drinking binge [14].",
        "prediction": "The 8 hour time frame represents an important phase of recovery after exercise as well as the period during which levels of alcohol in the blood are likely to be increased due to drinking heavily after an event",
        "reference": "The 8 h period after exercise is an important phase for recovery but also the period during which blood alcohol level may be the highest after excessive consumption of alcohol.",
        "metrics": {
          "SARI": 45.803171562861486,
          "BLEU": 0.12496315176496607,
          "FKGL": 18.1
        }
      },
      {
        "source": "The alcohol ingestion protocol (1.5 g·kg−1 BM; 12±2 standard drinks) began 1 h post-exercise and was consumed in 6 equal volumes of 1 part vodka (∼60 mL) to four parts orange juice (∼240 mL, 1.8 g CHO·kg−1 BM) during a 3 h period.",
        "prediction": "The alcohol ingestion protocol involved consuming alcohol at a rate of 1.5 grams per kilogram of body mass, which is equivalent to approximately 12 standard drinks. This protocol started one hour after exercise and consisted of drinking six equal servings. Each serving was made with one part vodka, which is roughly 60 milliliters, mixed with four parts orange juice, which is about 240 milliliters. This mixture provided 1.8 grams of carbohydrates per kilogram of body mass over a period of three hours.",
        "reference": "The alcohol consumption started 1 h after exercise with 6 drinks (60mL of vodka and 240mL of orange juice) and during 3 h.",
        "metrics": {
          "SARI": 39.02090084746786,
          "BLEU": 0,
          "FKGL": 7.5
        }
      },
      {
        "source": "The focus of the early post-exercise period (i.e., 1–8 h) is to enhance physiological processes that are critical for reversing the exercise-induced disturbances to homeostasis and physiological function and for promoting adaptations to training [1].",
        "prediction": "The focus of the early post-exercise period, which is the time from 1 to 8 hours after exercising, is to improve bodily processes that are important for correcting the disruptions caused by exercise to the balance of bodily functions and for supporting adaptations to training.",
        "reference": "The period after exercise (1-8h) enhances the body's processes that reverse the effects of the exercise and promotes adaptation to the training.",
        "metrics": {
          "SARI": 41.31926132149148,
          "BLEU": 0,
          "FKGL": 22
        }
      },
      {
        "source": "For the PRO condition, orange juice was consumed with a matched volume of water in place of the alcohol.",
        "prediction": "For the PRO condition, orange juice was consumed together with the same amount of water instead of alcohol.",
        "reference": "For the protein drink, orange juice was consumed with water instead of alcohol.",
        "metrics": {
          "SARI": 62.24857468239821,
          "BLEU": 0.3942058093215873,
          "FKGL": 10.3
        }
      },
      {
        "source": "Subjects ingested the beverages within 5 min every 30 min.",
        "prediction": "Subjects consumed the beverages within 5 minutes every 30 minutes",
        "reference": "Subjects consumed the drinks in 5 minutes every 30 minutes.",
        "metrics": {
          "SARI": 58.63247863247864,
          "BLEU": 0.47538527325677404,
          "FKGL": 9.6
        }
      },
      {
        "source": "Blood, cell signaling and mRNA data were analyzed by two-way ANOVA (two factor: time × treatment) with repeated measures and myofibrillar protein synthesis was analyzed by one-way ANOVA with repeated measures.",
        "prediction": "Blood, cell signaling, and messenger RNA data were analyzed using a two-way Analysis of Variance (ANOVA) method that considers two factors: time and treatment, with repeated measurements taken. Myofibrillar protein synthesis was analyzed using a one-way ANOVA method with repeated measurements.",
        "reference": "Data (blood, protein synthesis, responses, etc.) were analyzed several times for the 3 different consumptions of drinks.",
        "metrics": {
          "SARI": 42.087479270315086,
          "BLEU": 0,
          "FKGL": 14.8
        }
      },
      {
        "source": "The first novel finding of this study was that mTOR signaling and rates of myofibrillar protein synthesis (MPS) following concurrent resistance, continuous and intermittent high-intensity exercise, designed to mimic the metabolic profile of many team sports, were impaired during the early (8 h) recovery phase by the ingestion of large amounts (1.5 g•kg−1 BM) of alcohol.",
        "prediction": "The first important discovery of this study was that a process called mTOR signaling and the rates of myofibrillar protein synthesis, which is the creation of proteins that help muscles contract, were negatively affected during the early recovery phase, specifically 8 hours after exercise. This impairment occurred after consuming large amounts of alcohol, specifically 1.5 grams for each kilogram of body mass, following types of exercise that included concurrent resistance exercises and both continuous and intermittent high-intensity workouts. These exercises were designed to replicate the metabolic profile often seen in many team sports.",
        "reference": "The first finding of this study was that the mTOR enzyme (responsible for muscle growth) and protein synthesis (after exercise) were reduced by an excessive alcohol consumption during the 8 h period of recovery.",
        "metrics": {
          "SARI": 44.903092480414685,
          "BLEU": 0.05793574651661995,
          "FKGL": 14.7
        }
      },
      {
        "source": "These outcomes were most evident (37% reduction in rates of MPS) when alcohol was consumed in the absence of post-exercise protein intake, as is likely to occur when intoxication reduces the athlete's compliance to sound recovery practices.",
        "prediction": "These outcomes were most clear when there was a 37% reduction in the rates of Muscle Protein Synthesis (MPS) when alcohol was consumed without eating protein after exercise. This situation likely happens when being drunk makes it hard for athletes to follow good recovery practices.",
        "reference": "Consequences were most evident when alcohol was consumed without protein (37 % reduction of protein synthesis).",
        "metrics": {
          "SARI": 49.84985337937115,
          "BLEU": 0.11103081472293377,
          "FKGL": 12.1
        }
      },
      {
        "source": "However, a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate MPS [8] during post-exercise recovery, the intake of alcohol reduced MPS by ∼24%, representing only a partial ‘rescue’ of the anabolic response compared with protein alone.",
        "prediction": "However a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate Muscle Protein Synthesis during post-exercise recovery the intake of alcohol reduced Muscle Protein Synthesis by about 24 percent representing only a partial rescue of the anabolic response compared with protein alone",
        "reference": "A second finding was that even with protein consumption, alcohol reduced protein synthesis by around 24%.",
        "metrics": {
          "SARI": 46.586693763335816,
          "BLEU": 0.09212480089422209,
          "FKGL": 25.5
        }
      },
      {
        "source": "The mechanistic target of rapamycin complex 1 (mTORC1) is a central node for integrating nutrient (i.e. amino acid) and exercise/contraction signal transduction [31], [32].",
        "prediction": "The mechanistic target of rapamycin complex 1, also known as mTORC1, is an important part of the process that combines signals from nutrients, specifically amino acids, and signals from exercise and muscle contractions.",
        "reference": "mTOR is important in absorption of nutrients (like amino acids) and in sending signals to the body to produce an effort.",
        "metrics": {
          "SARI": 40.09166181041181,
          "BLEU": 0,
          "FKGL": 17.3
        }
      },
      {
        "source": "In conclusion, the current data provide the novel observation that alcohol impairs the response of MPS in exercise recovery in human skeletal muscle despite optimal nutrient provision.",
        "prediction": "In conclusion, the current data show that alcohol negatively affects the response of skeletal muscle protein synthesis (MPS) during recovery after exercise, even when nutrients are provided in adequate amounts.",
        "reference": "The data from this study show that alcohol reduce the protein synthesis after exercise even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 43.27801963487556,
          "BLEU": 0,
          "FKGL": 18.5
        }
      },
      {
        "source": "The quantity of alcohol consumed in the current study was based on amounts reported during binge drinking by athletes.",
        "prediction": "The amount of alcohol consumed in this study was based on what athletes reported during binge drinking.",
        "reference": "The quantity of alcohol consumed in this study was based on consumption by athletes during binge drinking.",
        "metrics": {
          "SARI": 62.656400053609765,
          "BLEU": 0.6133706010874769,
          "FKGL": 8.7
        }
      },
      {
        "source": "However, published reports suggest intakes of some individuals can be significantly greater [9], [50], which is of concern for many reasons related to health and safety [13].",
        "prediction": "However, published reports suggest some individuals consume significantly more than recommended, which raises concerns for various health and safety reasons.",
        "reference": "However, reports show that some individuals can consume higher quantities, which is a concern for health and safety issues.",
        "metrics": {
          "SARI": 44.943013164436735,
          "BLEU": 0,
          "FKGL": 18.2
        }
      },
      {
        "source": "Recommended nutritional strategies to maximize recovery in skeletal muscle include protein for enhancing rates of protein synthesis and carbohydrate for replenishing glycogen stores [2],[3].",
        "prediction": "Recommended nutritional strategies to improve recovery in skeletal muscle include protein to boost the production of new proteins and carbohydrates to restore glycogen stores",
        "reference": "Nutritional strategies to optimize muscle recovery include proteins which are recommended to allow protein synthesis (production of new proteins which improve, for example, muscle mass) and carbohydrates to recharge glycogen stores (energy storage).",
        "metrics": {
          "SARI": 49.612080936766475,
          "BLEU": 0.09930980659450005,
          "FKGL": 16.2
        }
      },
      {
        "source": "Regrettably, there has been difficulty in finding an educational message with alcohol consumption related to sports performance that has resonance with athletes.",
        "prediction": "There has been difficulty in finding an educational message about how alcohol consumption affects sports performance that resonates with athletes",
        "reference": "Regrettably, it is difficult to find an educational message about alcohol consumption and sport performances that speaks to athletes.",
        "metrics": {
          "SARI": 39.97510890681952,
          "BLEU": 0.14569814203855377,
          "FKGL": 14.6
        }
      },
      {
        "source": "Given the need to promote protein synthesis that underpins adaptation, repair and regeneration of skeletal muscle the results of the current study provide clear evidence of impaired recovery when alcohol is consumed after concurrent (resistance, continuous and intermittent high-intensity) exercise even in the presence of optimal nutritional conditions.",
        "prediction": "The current study shows that drinking alcohol after doing different types of exercise, such as strength training, continuous exercise, and short bursts of intense activity, leads to poor recovery of muscles. This happens even when the right nutrients are available to support the muscles. Protein synthesis, which is the process where cells create proteins important for adapting, repairing, and regenerating skeletal muscle, is negatively affected by alcohol consumption after exercise.",
        "reference": "Considering the importance of protein synthesis (adaptation, repair and regeneration of muscles), the results of this study provide evidence of reduced recovery when alcohol is consumed after exercise, even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 32.86473954339714,
          "BLEU": 0,
          "FKGL": 14.7
        }
      },
      {
        "source": "We propose our data is of paramount interest to athletes and coaches.",
        "prediction": "Our data is very important to athletes and coaches",
        "reference": "We consider our data crucial for athletes and coaches.",
        "metrics": {
          "SARI": 42.23875661375661,
          "BLEU": 0,
          "FKGL": 6.8
        }
      },
      {
        "source": "Our findings provide an evidence-base for a message of moderation in alcohol intake to promote recovery after exercise with the potential to alter current sports culture and athlete practices.",
        "prediction": "Our findings provide support for a message encouraging moderate drinking of alcohol to help recovery after exercise. This could change the current culture of sports and the practices of athletes.",
        "reference": "Our findings provide evidence for a message of moderation in alcohol consumption to promote recovery after exercise, with the potential to change sport cultures and athlete practices.",
        "metrics": {
          "SARI": 32.631991934528166,
          "BLEU": 0,
          "FKGL": 10.3
        }
      },
      {
        "source": "Muscle contraction and the intake of leucine-rich protein sources activate independent but complimentary signaling responses that converge at the mechanistic target of rapamycin (mTOR) to stimulate protein translation enhancing rates of muscle protein synthesis [4]–[6].",
        "prediction": "Muscle contraction and the intake of protein sources that are rich in leucine activate separate but helpful signaling responses that come together at a key protein known as mTOR, which stands for mechanistic target of rapamycin. This process stimulates the creation of proteins, increasing the rates of muscle protein synthesis.",
        "reference": "Muscle contraction and consumption of leucine-rich protein (essential amino acid composing proteins) induce reactions which increase the protein synthesis.",
        "metrics": {
          "SARI": 39.77626307547285,
          "BLEU": 0,
          "FKGL": 13
        }
      },
      {
        "source": "The ingestion of ∼20–25 g of high quality protein soon after exercise [7], repeated every 4 h [8] has been shown to maximise the anabolic response in skeletal muscle.",
        "prediction": "The intake of about 20 to 25 grams of high quality protein soon after exercise, repeated every 4 hours, has been shown to maximize muscle growth in skeletal muscle.",
        "reference": "Consumption of 20-25 g of high quality protein after exercise and repeated every 4 h showed an optimal anabolic response (increasing muscle mass) of the muscles.",
        "metrics": {
          "SARI": 43.27016240968147,
          "BLEU": 0.13072010980679707,
          "FKGL": 13.4
        }
      },
      {
        "source": "The cultural environment surrounding some sports often involves the intake of large amounts of alcohol after training and competition, with athletes in several team sports being particularly at risk of “binge drinking” practices [9]–[11].",
        "prediction": "The cultural environment around some sports often includes consuming large amounts of alcohol after training and competitions. Athletes in certain team sports are especially at risk of engaging in \"binge drinking,\" which means drinking excessive amounts of alcohol in a short period.",
        "reference": "Several studies showed that athletes are more likely to drink alcohol to excess (50-65% drink above dangerous level).",
        "metrics": {
          "SARI": 35.40266106442577,
          "BLEU": 0,
          "FKGL": 12.7
        }
      },
      {
        "source": "The outcomes of binge drinking after exercise are likely to include the direct effect of alcohol on physiological processes as well as the indirect effect on the athlete's recovery due to not eating or resting adequately as a result of intoxication.",
        "prediction": "Binge drinking after exercise can lead to two main effects. First, alcohol has a direct impact on the body's physiological processes, which refers to the functions and activities occurring within the body. Second, alcohol can indirectly affect an athlete's recovery because intoxication may prevent them from eating or resting properly.",
        "reference": "Consequences of an excessive consumption after exercise are direct effect on the body but also indirect effect on the recovery due to an inappropriate eating and rest.",
        "metrics": {
          "SARI": 34.98234992956497,
          "BLEU": 0,
          "FKGL": 13.3
        }
      },
      {
        "source": "Although the concurrent consumption of carbohydrate can partially offset the deleterious effects of alcohol intake on post-exercise glycogen resynthesis [14], the effect of alcohol consumption on muscle protein synthesis is unknown.",
        "prediction": "Although consuming carbohydrates at the same time can help lessen the harmful effects of drinking alcohol on the restoration of glycogen after exercise, the impact of drinking alcohol on the building of muscle protein is not known",
        "reference": "Consumption of carbohydrates can partially counter balance the effects of alcohol on the recharge of glycogen stores but the effect on muscle protein synthesis are unknown.",
        "metrics": {
          "SARI": 38.38232766802289,
          "BLEU": 0,
          "FKGL": 17.7
        }
      },
      {
        "source": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis (MPS) in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.",
        "prediction": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis in humans during recovery from a bout of strenuous exercise that simulates the stresses an athlete may experience in training and performance for various team sports such as football, rugby, and court sports",
        "reference": "The aim of this study was to determine the effect of alcohol on anabolism and protein synthesis in humans during recovery following an effort an athlete could do (in rugby or football for example).",
        "metrics": {
          "SARI": 44.920391943833344,
          "BLEU": 0.23444596522131253,
          "FKGL": 26.7
        }
      },
      {
        "source": "Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the age-associated loss of muscle mass and is one of the main contributors to musculoskeletal impairments in the elderly. Previous research has demonstrated that resistance training can attenuate skeletal muscle function deficits in older adults, however few articles have focused on the effects of resistance training on functional mobility. The purpose of this systematic review was to 1) present the current state of literature regarding the effects of resistance training on functional mobility outcomes for older adults with skeletal muscle function deficits and 2) provide clinicians with practical guidelines that can be used with seniors during resistance training, or to encourage exercise. We set forth evidence that resistance training can attenuate age-related changes in functional mobility, including improvements in gait speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in progressive resistance training activities, and should be admonished to move along a continuum of exercise from immobility, toward the recommended daily amounts of activity.",
        "prediction": "Human aging leads to various changes in skeletal muscle. Sarcopenia is the loss of muscle mass that comes with age and is a major factor contributing to movement issues in older people. Previous studies have shown that resistance training, which involves exercises that improve strength, can reduce problems with muscle function in older adults. However, there are few studies that specifically look at how resistance training affects movement ability. This systematic review aimed to 1 present the current research on how resistance training impacts movement outcomes for older adults with muscle function issues and 2 provide practical guidelines for healthcare professionals to use with older adults during resistance training or to promote exercise. We found evidence that resistance training can help reduce age-related changes in movement ability, which includes improvements in walking speed, balance while standing still and moving, and lowering the risk of falling. Older adults should be encouraged to engage in progressive resistance training exercises and should be guided to gradually increase their activity levels from low movement to the suggested daily amounts of exercise.",
        "reference": "Human aging results in a variety of changes to muscles. Sarcopenia is the age-associated muscle loss and is one of the main contributors to muscle disorders in older adults. Previous research has demonstrated that strength training can attenuate muscle function deficits in older adults, however few articles have focused on the effects of strength training on mobility. The purpose of this review was to present the effects of strength training on mobility for older adults with muscle function deficits and to provide guidelines that can be used with seniors during strength training. We present evidence that strength training can attenuate age-related changes in mobility, including improvements in walking speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in strength training activities, and should avoid immobility.",
        "metrics": {
          "SARI": 31.033755745400825,
          "BLEU": 0.14897198300509779,
          "FKGL": 14.3
        }
      },
      {
        "source": "Maximizing the hypertrophic response to resistance training (RT) is thought to be best achieved by proper manipulation of exercise program variables including exercise selection, exercise order, length of rest intervals, intensity of maximal load, and training volume. An often overlooked variable that also may impact muscle growth is repetition duration. Duration amounts to the sum total of the concentric, eccentric, and isometric components of a repetition, and is predicated on the tempo at which the repetition is performed. We conducted a systematic review and meta-analysis to determine whether alterations in repetition duration can amplify the hypertrophic response to RT. Results indicate that hypertrophic outcomes are similar when training with repetition durations ranging from 0.5 to 8 s. From a practical standpoint it would seem that a fairly wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at volitionally very slow durations (>10s per repetition) is inferior from a hypertrophy standpoint, although a lack of controlled studies on the topic makes it difficult to draw definitive conclusions.",
        "prediction": "Maximizing the hypertrophic response to resistance training, which refers to the increase in muscle size, is thought to be best achieved by properly changing exercise program variables. These variables include exercise selection, exercise order, length of rest intervals, intensity of maximal load (the heaviest weight you can lift), and training volume (the total amount of weight lifted in a session). One often overlooked variable that may also impact muscle growth is repetition duration. Repetition duration is the total time taken to complete one full movement in a strength training exercise and includes three phases: the concentric phase (when the muscle shortens), the eccentric phase (when the muscle lengthens), and isometric phase (when the muscle is under tension without changing length). This duration depends on the tempo or speed at which the exercise is performed. We conducted a systematic review and meta-analysis, which is a study that combines results from multiple studies to draw a conclusion, to determine whether changes in repetition duration can enhance the hypertrophic response to resistance training. Results indicate that the outcomes for muscle growth are similar when training with repetition durations ranging from 0.5 seconds to 8 seconds. From a practical standpoint, it appears that a fairly wide range of repetition durations can be used if the primary goal is to maximize muscle growth. Findings suggest that training at very slow durations, specifically more than 10 seconds per repetition, is not as effective for increasing muscle size, although the lack of controlled studies on this topic makes it hard to reach definite conclusions.",
        "reference": "Maximizing the muscle growth following strength training can be done by manipulating variables like exercise selection, exercise order, rest intervals, intensity of maximal load, and training volume (number of exercises, sets, etc.). A neglected variable that also may impact muscle growth is repetition duration. Total duration of a repetition is the sum of the contraction, the lengthening and isometric (static contraction) and is based on the tempo. We conducted a review and analysis of multiple studies to determine if modification of repetition duration can amplify the muscle growth. Results indicate that muscle growth is similar when training with repetition durations from 0.5 to 8 s. Using a wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at very slow durations (>10s per repetition) is less efficient for muscle growth, although a lack of studies on the topic makes it difficult to draw definitive conclusions.",
        "metrics": {
          "SARI": 44.64415217136642,
          "BLEU": 0.2379247125574883,
          "FKGL": 14.5
        }
      },
      {
        "source": "Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also refers to specific physiological processes or states occurring after exercise that are distinct from the physiology of either the exercising or the resting states. In this context, recovery of the cardiovascular system after exercise occurs across a period of minutes to hours, during which many characteristics of the system, even how it is controlled, change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. Furthermore, some of these changes may provide insight into when the cardiovascular system has recovered from prior training and is physiologically ready for additional training stress. This review focuses on the most consistently observed hemodynamic adjustments and the underlying causes that drive cardiovascular recovery and will highlight how they differ following resistance and aerobic exercise. Primary emphasis will be placed on the hypotensive effect of aerobic and resistance exercise and associated mechanisms that have clinical relevance, but if left unchecked, can progress to symptomatic hypotension and syncope. Finally, we focus on the practical application of this information to strategies to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities of this state. We will explore appropriate field measures, and discuss to what extent these can guide an athlete's training.",
        "prediction": "Recovery from exercise is the time between finishing exercise and returning to a resting or recovered state. It also includes specific bodily processes or conditions that happen after exercise, which are different from the conditions when exercising or resting. During recovery, the cardiovascular system, which includes the heart and blood vessels, undergoes changes over a period of minutes to hours. These changes can affect how the cardiovascular system functions and is controlled. Some changes are important for long-term improvements from exercise training, while others can cause instability in the cardiovascular system during recovery. Additionally, understanding these changes can help determine when the cardiovascular system has fully recovered from previous training and is ready for more exercise. This review examines the most commonly noticed changes in blood flow and the reasons behind them that influence cardiovascular recovery. It will also highlight how these changes vary after resistance training, which involves heavy lifting, and aerobic exercise, which is endurance-based like running. The focus will be on the lowering of blood pressure, known as hypotension, that can occur after both types of exercise and the mechanisms behind it that are relevant in a clinical setting, as unchecked hypotension can result in fainting. Finally, we will look at ways to apply this knowledge to enhance cardiovascular recovery and reduce risks associated with it. We will also discuss suitable methods to measure recovery in the field and how these measures can help guide an athlete's training.",
        "reference": "Recovery from exercise refers to the time period between the end of exercise and the return to a resting or recovered state. It also refers to specific physiological processes occurring after exercise. Recovery of the cardiovascular system occurs across a period of minutes to hours, during which many characteristics of the system change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. These changes may provide insight into when the cardiovascular system has recovered and is physiologically ready for additional training. This review focuses on the most consistently observed blood flow adjustments and the causes that drive cardiovascular recovery and how they differ following strength and cardio exercise. First we focus on the effect leading to low blood pressure of cardio and strength exercise and associated mechanisms, which can progress to symptomatic hypotension (low blood pressure) and fainting. Finally, we focus on the practical application to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities. We will explore measures, and discuss how these can guide an athlete's training.",
        "metrics": {
          "SARI": 33.96149984376753,
          "BLEU": 0.17030291258195712,
          "FKGL": 13
        }
      },
      {
        "source": "Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed \"exercise as medicine\". Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often overlooked as a target for exercise-based therapy. Despite the vast range of severity in kidney disease (e.g., pre-dialysis, dialysis, transplant), exercise has a potential role in all patients suffering from the condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'.",
        "prediction": "Exercise and physical activity are increasingly becoming important tools for treating and preventing several medical conditions such as arthritis and diabetes. This idea is called \"exercise as medicine.\" Exercise can help reduce the risk of heart disease, lower inflammation, prevent muscle wasting (cachexia), and manage high blood pressure (hypertension). It also improves physical ability, strength, and the capacity of the cardiovascular system. Chronic kidney disease, which affects about 10% of the population, is often forgotten as a condition that could benefit from exercise therapy. There is a wide range of severity in kidney disease, including stages like pre-dialysis (before needing dialysis), dialysis (a treatment for kidney failure), and transplant (receiving a new kidney). However, exercise can be beneficial for all patients with this condition. In this review, we summarize the important role exercise may play in managing kidney disease and how this type of \"medicine\" should be best given and recommended.",
        "reference": "Exercise and physical activity are key tools in the treatment and prevention of several medical conditions including joint disorders and diabetes. Exercise can reduce cardiovascular risk, inflammation, illness causing muscle loss, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often neglected as a target for exercise-based therapy. Despite the vast range of severity in kidney disease, exercise has a potential role in all patients suffering from the condition. In this review, we summarise the potential important role of exercise in the management of kidney disease and how this form of 'medicine' should be used.",
        "metrics": {
          "SARI": 33.94259356957654,
          "BLEU": 0.22099395773333505,
          "FKGL": 13
        }
      },
      {
        "source": "Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The pooled standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% CI=-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.",
        "prediction": "Blood flow restriction (BFR) is a clinical method used to help healthy people gain strength. However, it is unclear how BFR affects pain and function in people with knee pain. The objective of this study was to find out if adding BFR to resistance exercises can help relieve pain and improve function in patients with knee pain. The methods used included a systematic review and meta-analysis of randomized clinical trials. Randomized clinical trials that looked at resistance exercises with or without BFR to treat knee pain and function in people over 18 years old with knee pain were included. The combined results showed that resistance exercises with BFR were not more effective than resistance exercises alone for reducing pain and improving knee function in patients with knee pain.",
        "reference": "Exercise with blood flow restriction (BFR) is used to increase strength in healthy individuals. However, its effects on individuals with knee pain are unknown. The objective is to determine the effectiveness of adding BFR to strength exercise for pain relief and improvement of function in patients with knee pain. We conducted a review and analysis of multiple studies. Studies that compared strength exercise with or without BFR to treat knee pain and function. The results showed that resistance exercises with BFR was not more effective than strength exercises for reducing pain and improving knee function in patients with knee pain.",
        "metrics": {
          "SARI": 46.00704436510426,
          "BLEU": 0.34372765348932566,
          "FKGL": 11.6
        }
      },
      {
        "source": "In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports). In this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review recommends safe and effective whole-body aerobic exercise at moderate intensity (i.e., 50-65% of maximum oxygen intake, 30-60 min per session, 3-4 times a week) that primarily focuses on the major muscle groups for the prevention and treatment of hypertension. Resistance exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but resistance exercise is contraindicated in patients with hypertension who have chest symptoms such as chest pain.",
        "prediction": "In Japan, there were an estimated 43 million patients with high blood pressure in 2010. The management of this condition is very important in disease control, and the significance of lifestyle changes for preventing and treating high blood pressure has been recognized in Japan. Specifically, there has been a focus on increasing the levels of daily physical activities and exercise. In this literature review, we looked at suitable exercise recommendations (such as type, how hard to exercise, how long each session should last, and how often to exercise) for preventing and treating high blood pressure based on Japanese and international articles. This review suggests safe and effective whole-body aerobic exercise at a moderate intensity, which means exercising at 50-65% of maximum oxygen intake, for 30-60 minutes per session, 3-4 times a week, with a focus on major muscle groups to help prevent and treat high blood pressure. Strength training should be done at a low intensity without holding one’s breath and should be considered as an additional form of exercise, but strength training is not recommended for patients with high blood pressure who experience chest symptoms such as chest pain.",
        "reference": "In Japan, 43 million patients had high blood pressure in 2010. The management of this condition and the importance of lifestyle changes for the prevention and treatment of high blood pressure has been recognized in Japan. In particular, increasing the levels of activities of daily living and physical exercise. In this review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of high blood pressure. This review recommends full body cardio exercise at moderate intensity (i.e., 50-65% of maximum intensity, 30-60 min per session, 3-4 times a week) that focuses on the major muscle groups. Strength exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but it is contraindicated in patients with high blood pressure who have chest symptoms such as chest pain.",
        "metrics": {
          "SARI": 33.49645048376839,
          "BLEU": 0.30150482642865095,
          "FKGL": 15.7
        }
      },
      {
        "source": "Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common sense advices call for late inception of intense, strength training-related activities, like weight lifting and plyometrics, which are usually postponed at the end of the growth age, even among sport practitioners. However, such advices seem to have a mainly anecdotal nature. Current literature does not seem to have any particular aversion against the practice of strength training by children and adolescents, provided that some safety rules are followed, like medical clearance, proper instruction from a qualified professional and progressive overload. At the same time, several studies provide consistent findings supporting the benefits of repeated, intense physical efforts in young subjects. Improved motor skills and body composition, in terms of increased fat free mass, reduced fat mass and enhanced bone health, have been extensively documented, especially if sport practice began early, when the subjects were pubescent. It can be therefore concluded that strength training is a relatively safe and healthy practice for children and adolescents.",
        "prediction": "Physical activity has been shown to effectively prevent several diseases and improve overall health. Common advice suggests that intense activities that build strength, like weight lifting and plyometrics (which are exercises designed to increase power and speed), should start later in life, usually after growth has finished, even for young athletes. However, this advice often comes from personal experience and not scientific evidence. Current research does not generally oppose strength training for children and teenagers as long as certain safety measures are taken. These include getting medical clearance (approval from a doctor), receiving proper instruction from a qualified professional, and gradually increasing the intensity and amount of exercise (a practice known as progressive overload). Furthermore, several studies consistently show the benefits of repeated, intense physical efforts among young individuals. Benefits include improved motor skills, better body composition (which refers to the proportion of fat and muscle in the body), such as increased muscle mass, reduced fat mass, and enhanced bone health. These benefits are especially notable if young people start participating in sports during puberty. Therefore, it can be concluded that strength training is a relatively safe and beneficial activity for children and teenagers.",
        "reference": "Physical activity has many benefits like preventing diseases and improving general health. In popular opinion, it is recommended to start intense physical activities like weight lifting and plyometrics (exercises like sprint and jumps) after adolescence. But this advice does not seem to be based on facts. We did not find any recommendation against strength training at a young age if it is well supervised with safety rules, medical clearance, proper instruction from a professional and progressive overload (gradual increase of stress on muscle and nervous system). Several studies showed many benefits of repeated, intense physical efforts in young people. Improved motor skills and body composition (muscle mass increased, fat mass decreased, healthier bone) were found in studies, especially if sport practice began during puberty. We can conclude that strength training is a safe and healthy practice for children and adolescents.",
        "metrics": {
          "SARI": 46.94337064604316,
          "BLEU": 0.13161923596688385,
          "FKGL": 14.1
        }
      },
      {
        "source": "recently, there has been a renewed public interest in IFast. Given the importance of nutrition in optimizing athletic performance, there is a concern about the effects of IFast on athletics. Looking at high-intensity, endurance, and resistance exercises, studies have been varied but are uniform in showing that there is no benefit to athletic performance while fasting.",
        "prediction": "Recently, there has been a renewed public interest in IFast. IFast refers to a type of intermittent fasting, which is a dietary pattern that cycles between periods of eating and fasting. Given the importance of nutrition, meaning the food and drink that are necessary for health and growth, in optimizing athletic performance, there is a concern about the effects of IFast on athletes. Looking at high-intensity exercises, which are activities performed at a high level of effort, endurance exercises that require sustained energy over time, and resistance exercises that involve using force against an object to strengthen muscles, studies have shown varied results. However, all studies agree that there is no benefit to athletic performance while fasting.",
        "reference": "Nowadays, people are interested in intermittent fasting (an eating pattern where you cycle between periods of eating and fasting) Nutrition is important for the optimization of sport performance so there is a concern about the effects of intermittent fasting on performance. Studies showed that there are no benefit in high-intensity, endurance or strength training while fasting.",
        "metrics": {
          "SARI": 48.10710583928397,
          "BLEU": 0.12672362869175896,
          "FKGL": 13.6
        }
      },
      {
        "source": "Nearly every physically active person encounters periods in which the time available for exercise is limited (e.g., personal, family, or business conflicts). During such periods, the goal of physical training may be to simply maintain (rather than improve) physical performance. Similarly, certain special populations may desire to maintain performance for prolonged periods, namely athletes (during the competitive season and off-season) and military personnel (during deployment). In general populations, endurance performance can be maintained for up to 15 weeks when training frequency is reduced to as little as 2 sessions per week or when exercise volume is reduced by 33–66% (as low as 13–26 minutes per session), as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size (at least in younger populations) can be maintained for up to 32 weeks with as little as 1 session of strength training per week and 1 set per exercise, as long as exercise intensity (relative load) is maintained; whereas, in older populations, maintaining muscle size may require up to 2 sessions per week and 2–3 sets per exercise, while maintaining exercise intensity. Our primary conclusion is that exercise intensity seems to be the key variable for maintaining physical performance over time, despite relatively large reductions in exercise frequency and volume.",
        "prediction": "Nearly every person who exercises regularly faces times when they have less time to work out due to personal, family, or business reasons. During these times, the goal of physical training may shift to maintaining performance instead of improving it. This is especially true for certain groups like athletes, who want to keep up their performance during both competition season and off-season, and military personnel during their time in service. Research shows that most people can keep their endurance performance for up to 15 weeks with just 2 training sessions each week or by reducing their exercise volume by 33-66%. This means exercising for as little as 13-26 minutes each session, as long as the intensity, which refers to the level of effort measured by heart rate, is kept high. For maintaining strength and muscle size, younger individuals can do this with only 1 session of strength training each week and 1 set for each exercise, provided they also keep the intensity high. In contrast, older individuals may need to work out 2 times a week and do 2-3 sets per exercise while still maintaining intensity. The key takeaway is that exercise intensity is essential for sustaining physical performance over time, even with significant cuts to how often and how much one exercises.",
        "reference": "Physically active people encounter periods with limited time available for exercise. During such periods, the goal of training may be to simply maintain physical performance. Some people may also desire to maintain performance for prolonged periods, like athletes during competitive season or militaries during deployment. Endurance performance can be maintained for up to 15 weeks with 2 trainings per week, or with a reduction of 13–26 minutes per training, as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size can be maintained for up to 32 weeks with 1 strength training per week and 1 set per exercise, as long as exercise intensity (load) is maintained. In older populations, maintaining muscle size may require up to 2 trainings per week and 2–3 sets per exercise, while maintaining exercise intensity. Exercise intensity seems to be the key variable for maintaining physical performance, even with reduction of frequency and volume (number of exercises, sets, etc.)",
        "metrics": {
          "SARI": 32.565221330692715,
          "BLEU": 0.09737290684835807,
          "FKGL": 13.7
        }
      },
      {
        "source": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients--primarily protein and carbohydrate--in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been postulated that the timing of nutritional consumption may be more important than the absolute daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. Theoretically, consuming the proper ratio of nutrients during this time not only initiates the rebuilding of damaged muscle tissue and restoration of energy reserves, but it does so in a supercompensated fashion that enhances both body composition and exercise performance. Several researchers have made reference to an anabolic “window of opportunity” whereby a limited time exists after training to optimize training-related muscular adaptations. However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Not only is nutrient timing research open to question in terms of applicability, but recent evidence has directly challenged the classical view of the relevance of post-exercise nutritional intake with respect to anabolism.",
        "prediction": "Nutrient timing is a popular strategy related to nutrition that focuses on when to consume different nutrients, mainly protein and carbohydrates, around exercise sessions. Some people believe that this approach can lead to significant changes in body composition, which refers to the percentage of fat, bone, water, and muscle in the body. It has even been suggested that when you eat can be more important than how much you eat over the course of a day. The time after exercise is generally seen as the most important period for nutrient timing. According to theory, eating the right mix of nutrients during this time not only starts the recovery of damaged muscle tissue and replenishes energy supplies but does so in a way that enhances body composition and exercise performance. Many researchers refer to an anabolic, which means muscle-building, \"window of opportunity\" suggesting that there is a short period after training to maximize muscle benefits. However, the significance and even the existence of this post-exercise window can depend on various factors. Nutrient timing research may not always apply to everyone, and recent findings have questioned the traditional belief that post-exercise food intake is essential for muscle building.",
        "reference": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients around an exercise session. Some people say that this approach can produce dramatic improvements in body composition. It has even been hypothesized that the timing may be more important than the total daily intake of nutrients. The post-exercise period is considered the most important part of nutrient timing. In theory, consuming the proper ratio of nutrients during this time allows the rebuilding of damaged muscle tissue and restoration of energy reserves, but also enhances both body composition and exercise performance. Researchers have made reference to an anabolic “window of opportunity” (a limited time after training to optimize muscular adaptations). However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Research is uncertain about the applicability of nutrient timing. Besides, the relevance of post-exercise nutritional intake concerning anabolism (increasing muscle mass) has been challenged by recent evidence.",
        "metrics": {
          "SARI": 33.15370823934215,
          "BLEU": 0.18192882924189993,
          "FKGL": 14
        }
      },
      {
        "source": "Lack of time is among the more commonly reported barriers for abstention from exercise programs. The aim of this review was to determine how strength training can be most effectively carried out in a time-efficient manner by critically evaluating research on acute training variables, advanced training techniques, and the need for warm-up and stretching. When programming strength training for optimum time-efficiency we recommend prioritizing bilateral, multi-joint exercises that include full dynamic movements (i.e. both eccentric and concentric muscle actions), and to perform a minimum of one leg pressing exercise (e.g. squats), one upper-body pulling exercise (e.g. pull-up) and one upper-body pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights based on training goals, availability, and personal preferences. Weekly training volume is more important than training frequency and we recommend performing a minimum of 4 weekly sets per muscle group using a 6–15 RM loading range (15–40 repetitions can be used if training is performed to volitional failure). Advanced training techniques, such as supersets, drop sets and rest-pause training roughly halves training time compared to traditional training, while maintaining training volume. However, these methods are probably better at inducing hypertrophy than muscular strength, and more research is needed on longitudinal training effects. Finally, we advise restricting the warm-up to exercise-specific warm-ups, and only prioritize stretching if the goal of training is to increase flexibility.",
        "prediction": "Lack of time is one of the most common reasons people do not participate in exercise programs. This review aims to find the best ways to do strength training efficiently by examining research on important training factors, advanced training methods, and the importance of warming up and stretching. To make strength training more time-efficient, we suggest focusing on exercises that work on both sides of the body at the same time and involve multiple joints, which means they require the use of larger muscle groups. This includes performing at least one leg exercise (like squats), one exercise for pulling movements of the upper body (like pull-ups), and one exercise for pushing movements of the upper body (like bench press). You can use machines or free weights depending on your training goals, what's available, and what you prefer. The total volume of training in a week is more important than how often you train, and we recommend doing at least four sets for each muscle group each week, using a load that allows for 6 to 15 repetitions. You may do 15 to 40 repetitions if you push yourself to your limits. Advanced training techniques, such as supersets (doing two exercises back-to-back without rest), drop sets (reducing weight after reaching fatigue), and rest-pause training (taking short breaks during sets), can cut training time in half compared to traditional methods while keeping the training volume the same. However, these techniques are likely more effective at increasing muscle size than building muscle strength, and further research is needed to understand long-term effects of these methods. Lastly, we recommend limiting the warm-up to exercises specific to the workout and only focusing on stretching if your training goal is to improve flexibility.",
        "reference": "Lack of time is one of the most common reason for not exercising. The aim of this review was to determine how strength training can be time-efficient by evaluating research on training variables, advanced training techniques, and the need for warm-up and stretching. We recommend choosing bilateral (with the two arms or legs), multi-joint (multiple joints involved) exercises that include full dynamic movements and to perform a minimum of one leg pressing exercise (e.g. squats), one pulling exercise (e.g. pull-up) and one pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights. Weekly training volume (number of exercises, sets, etc.) is more important than training frequency and we recommend a minimum of 4 weekly sets per muscle group using a load that permits to do 6 to 15 repetitions. Advanced training techniques like combining exercises with no rest or shortening the rest time while using lighter weights can cuts the training time in half, while maintaining training volume. However, these methods are probably better to increase muscle mass than strength. Finally, we advise exercise-specific warm-ups, and stretching only if the goal of training is to increase flexibility.",
        "metrics": {
          "SARI": 35.55836060722622,
          "BLEU": 0.1419957753565262,
          "FKGL": 13.3
        }
      },
      {
        "source": "Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequency on a volume-equated basis. Meta-regression analysis of non-volume-equated studies showed a significant effect favoring higher frequencies, although the overall difference in magnitude of effect between frequencies of 1 and 3+ days per week was modest. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "prediction": "Training frequency is an important factor in how muscles grow in response to structured weightlifting exercises. This paper aimed to review and analyze various studies that looked into how the number of training sessions each week affects muscle growth. The findings indicated that there was no significant difference in muscle growth between those who trained more frequently and those who trained less, as long as the total amount of exercise was the same. An additional analysis of studies that did not match the total amount of exercise showed that training more often did have a positive effect, but the difference in muscle growth between training once a week and three or more times a week was small. In conclusion, there is strong evidence that how often someone trains does not have a significant effect on muscle growth if the total amount of exercise is the same. Therefore, individuals can choose how many training sessions to have each week for each muscle group based on what they prefer.",
        "reference": "Training frequency is an important variable for hypertrophy (increasing muscle mass) in strength training. The purpose of this paper was to conduct a review and analysis of multiple studies investigating the effects of weekly training frequency on hypertrophy. Results showed no significant difference between higher and lower frequency with the same training volume (number of exercises, sets, etc.). Analysis of studies with different training volumes showed better results for higher frequencies, although the difference between frequencies of 1 and 3+ days per week was small. In conclusion, there is strong evidence that resistance training frequency does not significantly impact muscle hypertrophy when training volume is the same. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "metrics": {
          "SARI": 32.465000031029064,
          "BLEU": 0.1570166952391243,
          "FKGL": 13
        }
      },
      {
        "source": "A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Forced repetitions/drop sets, supersets, and heavy negatives, in particular, have been purported to enhance the hypertrophic response to resistance exercise. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to resistance training programs.",
        "prediction": "A variety of specialized training techniques have been recommended to increase muscle growth. Forced repetitions, also known as drop sets, supersets, and heavy negatives, are believed to improve the body’s response to resistance exercise. This article will examine how these techniques may help in promoting muscle growth and offer insights into how they can be applied in resistance training programs.",
        "reference": "A variety of training techniques have been suggested to support muscle growth. Forced repetitions/drop sets (use lighter weight or receiving help to continue the set), supersets (combining exercises with no rest), and heavy negatives (lowering phase of the lift), are pretended to enhance hypertrophy (increasing muscle mass). This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to strength training.",
        "metrics": {
          "SARI": 31.86260932772364,
          "BLEU": 0.16608955932792996,
          "FKGL": 12.3
        }
      },
      {
        "source": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. Although both groups are known to display impressive muscularity, it is not clear which method is superior for hypertrophic gains. It has been shown that many factors mediate the hypertrophic process and that mechanical tension, muscle damage, and metabolic stress all can play a role in exercise-induced muscle growth. Therefore, the purpose of this paper is twofold: (a) to extensively review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and (b) to draw conclusions from the research as to the optimal protocol for maximizing muscle growth.",
        "prediction": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Metabolic stress refers to the accumulation of byproducts from exercise in the muscles, which can promote muscle growth. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. High-intensity loads mean they lift heavier weights, while lengthy rest periods allow their muscles to recover fully before lifting again. Although both groups are known to display impressive muscularity, which means a well-developed physique, it is not clear which method is superior for hypertrophic gains. Hypertrophic gains refer to an increase in muscle size. It has been shown that many factors mediate, or influence, the hypertrophic process and that mechanical tension, muscle damage, and metabolic stress all can play a role in exercise-induced muscle growth. Mechanical tension refers to the force exerted on muscles when lifting weights, muscle damage occurs when muscles are worked to their limit, and metabolic stress is the byproduct accumulation mentioned earlier. Therefore, the purpose of this paper is twofold: to extensively review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and to draw conclusions from the research as to the optimal protocol for maximizing muscle growth.",
        "reference": "Increasing muscle mass is the goal of many people who lift weights. Research is lacking, however, as to the optimal way to induce muscle growth. Bodybuilders train with moderate loads and short rest intervals. Powerlifters, on the other hand, train with high-intensity loads and lengthy rest periods. Although both groups are known to display impressive muscularity, it is not clear which method is superior for increasing muscle mass. It has been shown that many factors mediate the process of muscle growth and that mechanical tension, muscle damage, and metabolic stress all can play a role in muscle growth. Therefore, the purpose of this paper is to review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and to draw conclusions from the research as to the optimal way to induce muscle growth.",
        "metrics": {
          "SARI": 56.41111074454498,
          "BLEU": 0.42119017832037287,
          "FKGL": 12.9
        }
      }
    ]
  ],
  "aiir_llama_run_1_prompt": [
    {
      "SARI": 39.3414448977919,
      "BLEU": 0.18975710966265105,
      "FKGL": 8.607428571428569
    },
    [
      {
        "source": "Abstract   Novel technological advances in mobile devices and applications can be exploited in wildfire confrontation, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. This work describes an innovative mobile application for wildfire information management that operates on Windows Phone devices and acts as a complementary tool to the web-based version of the AEGIS platform for wildfire prevention and management. Several tasks can be accomplished from the AEGIS App, such as routing, spatial search for closest facilities and firefighting support infrastructures, access to weather data and visualization of fire management data (water sources, gas refill stations, evacuation sites etc.). An innovative feature of AEGIS App is the support of these tasks by a digital assistant for artificial intelligence named Cortana (developed by Microsoft for Windows Phone devices), that allows information utilization through voice commands. The application is to be used by firefighting personnel in Greece and is potentially expected to contribute towards a more sophisticated transferring of information and knowledge between wildfire confrontation operation centers and firefighting units in the field.",
        "prediction": "This passage talks about using new mobile technology to help fight wildfires. A special mobile app is created for Windows Phone devices. This app works with the AEGIS platform, which helps with wildfire prevention and management. Users can do many tasks with the AEGIS App. They can find the quickest routes, search for nearby facilities and support for firefighting, get weather updates, and see important fire management data like water sources and evacuation sites. \n\nA key feature of the AEGIS App is a digital assistant called Cortana. This assistant helps users access information using voice commands. The app is designed for firefighting teams in Greece. It aims to improve how information and knowledge are shared between firefighting centers and teams in the field.",
        "reference": "Novel mobile devices and applications can be used in wildfire confrontation by helping users to access data and information and coordinate personnel and vehicles. This paper describes a new Windows Phone application for wildfire information management which is complementary to the web version of the AEGIS platform for wildfire prevention and management. The AEGIS App can be used for searching for the closest firefighting infrastructures, as well as access to weather data, and visualization of water sources, gas refill stations, evacuation sites, etc. The AEGIS App supports these tasks through voice commands with Microsoft Cortana (AI digital assistant for Windows Phone).  The application is designed for firefighters in Greece but it can help communication between wildfire coordination and local firefighting units.",
        "metrics": {
          "SARI": 40.00649526860155,
          "BLEU": 0.09104605131561645,
          "FKGL": 7.5
        }
      },
      {
        "source": "In recent years, mobile technologies have developed and applied in education fields, and some mobile emerging carriers with mobile technologies include a personal digital assistant (PDA), smart phone, and e-book. Some of the mobile carriers combines context-aware technologies or involve into wireless network environments to make effective use through the combination of learning scenarios and technologies, and then provide students new learning experiences differed from the past learning. In view of the application of mobile technology in education, some previous studies have addressed that mobile learning is a meaningful learning that can improve the interaction between students and the situations and reach the purposes of learning. In the main stream of mobile learning, using mobile carriers with suitable learning methods or strategies in mobile learning activities for different students to enhance learning have gradually become a important and concern issue. The purpose of this study is to investigate the learning achievement and learning attitude of elementary school students on a campus plant learning activity when they use mobile carriers and competitive learning strategies. The experimental results show that the competitive learning group of students have better learning performance than non-competitive learning group of students. After completing the learning activity, the two groups of students presented high positive attitudes towards learning.",
        "prediction": "In recent years, mobile technologies have grown in education. Some mobile tools include personal digital assistants, smartphones, and e-books. These tools often use context-aware technologies and work in wireless networks. This combination helps create new learning experiences. Mobile learning is important because it improves interaction between students and their learning environment. Using mobile tools with effective learning methods is a key issue for different students. This study looks at how well elementary school students learn and feel about a plant learning activity when using mobile tools and competitive learning strategies. The results show that students in the competitive learning group did better than those in the non-competitive group. Both groups had a positive attitude towards learning after the activity.",
        "reference": "Mobile technologies, such as personal digital assistants (PDA), smartphones, and e-books, are increasingly used in the educational field. Some mobile carriers combine learning scenarios and context-aware technologies or wireless network environments providing students with new learning experiences. Mobile learning is effective as it can improve the interaction between students and situations. An important question in the mobile learning field is how to enhance learning through the use of mobile carriers with suitable learning methods in student activities. This study investigates learning achievement and attitude of elementary school students when they use mobile carriers and competitive learning strategies during campus plant learning activities. The competitive learning group of students has better learning performance than the non-competitive one. After the activity, both groups of students were positive about learning.",
        "metrics": {
          "SARI": 44.78598501429087,
          "BLEU": 0.17644977975000348,
          "FKGL": 10.8
        }
      },
      {
        "source": "This paper proposes an electrophysiological wireless patient monitoring system which integrates a Wireless ECG signal transmitter, GPS device and a mobile phone to acquire physiological signals and transmit them to a local server via Bluetooth wireless technology. Four kinds of monitor units were specially designed for a wireless communication, including a control center , a local monitor unit, mobile devices (personal digital assistant; PDA), and a Web page (for both patient and doctor). The use of various monitor units is created to fulfill different medical personnel requirements and wishes. This application was developed to promote the mobility and flexibility for the patients and also for the medical personnel, which further will improve both the quality of health care and lifestyle of the patient.",
        "prediction": "This paper introduces a wireless system for patient monitoring. It uses a Wireless ECG signal transmitter, a GPS device, and a mobile phone. This system collects physiological signals and sends them to a local server using Bluetooth. Four types of monitor units were created for wireless communication. These include a control center, a local monitor, mobile devices like PDAs, and a web page for patients and doctors. Different monitor units are designed to meet the needs of medical staff. The goal of this application is to improve mobility and flexibility for both patients and medical workers. This will enhance health care quality and the patient's lifestyle.",
        "reference": "This paper suggests a system to monitor patients wirelessly using a device that transmits ECG signals, a GPS device, and a mobile phone. The signals are sent to a local server via Bluetooth technology. Four types of monitor units were created for wireless communication: a control center, a local monitor unit, mobile devices such as personal digital assistants (PDA), and a web page accessible to both patients and doctors. Various monitor units are used to satisfy different medical personnel needs. This application was developed to boost the quality of healthcare and patient lifestyle by increasing mobility and flexibility for both patients and medical personnel.",
        "metrics": {
          "SARI": 55.62727215209168,
          "BLEU": 0.3808706672497578,
          "FKGL": 9.7
        }
      },
      {
        "source": "As various kinds of output devices emerged, such as highresolution printers or a display of PDA(Personal Digital Assistant), the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging image with high quality. One of the largest problems on image enlargement is the exaggeration of the jaggy edges. To remedy this problem, we propose a new interpolation method, which uses artificial neural network to determine the optimal values of interpolated pixels. The experimental results are shown and evaluated. The effectiveness of our methods is discussed by comparing with the conventional methods.",
        "prediction": "As new output devices like high-resolution printers and PDAs came out, the need for high-quality image conversion grew. This paper presents a new way to enlarge images clearly. A big problem with image enlargement is the rough edges that appear. To fix this, we suggest a new method that uses artificial neural networks to find the best values for the new pixels. We show and evaluate our experimental results. We compare how well our method works with traditional methods.",
        "reference": "The rise of output devices like high-resolution printers and PDA displays has increased the need for high-quality resolution conversion. The paper proposes a new method to make images bigger while maintaining high quality. The main issue with enlarging images is that jagged edges can become exaggerated. To solve this problem, we suggest a new method that helps us to estimate the value of the newly generated pixels using a neural network. The experiment's results are presented and analyzed. We evaluate the effectiveness of our methods by comparing them to traditional approaches.",
        "metrics": {
          "SARI": 42.3069048174012,
          "BLEU": 0.1704651574661775,
          "FKGL": 7.3
        }
      },
      {
        "source": "A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours. For them, everyday tasks like navigating through a train or metro network map becomes demanding. We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant, non-invasive sound. This technique was implemented inside a Personal Digital Assistant (PDA) portable device. In this implementation, colour information is extracted from the input image and categorised according to how human observers segment the colour space. This information is subsequently converted into sound and sent to the user via speakers or headphones. In the original implementation, it is possible for the user to send its feedback to reconfigure the system, however several features such as these were not implemented because the current technology is limited.We are confident that the full implementation will be possible in the near future as PDA technology improves.",
        "prediction": "Many people have trouble seeing or telling colors apart. This makes it hard for them to read train or metro maps. We created a new way to help these users by turning color information into sound. This method works inside a portable device called a Personal Digital Assistant, or PDA. The device takes a picture and gets the color information. It then organizes this information like how people see colors. Next, the device changes the colors into sounds. Users can hear these sounds through speakers or headphones. In the original version, users could give feedback to change the system, but some features were not included because of limits in technology. We believe that in the near future, better PDAs will allow for full features.",
        "reference": "A significant percentage of people have trouble differentiating or seeing colors. Simple tasks like using a train or subway map can be difficult for them. We introduce a new method to help visually impaired users perceive color information using non-invasive, pleasant sounds generated from everyday stimuli. We put this technique into a portable device called a Personal Digital Assistant (PDA). In this implementation, we extract color information from the input image and organize it based on how people perceive color. Then, the information is changed into sound and sent to the user through speakers or headphones. In the first version, the user could give feedback to change the system, but we couldn't add all the features due to technology limitations. We believe that we can add all the features soon as PDA technology advances.",
        "metrics": {
          "SARI": 50.60941119986365,
          "BLEU": 0.21715393009588632,
          "FKGL": 6.9
        }
      },
      {
        "source": "Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients but also incurs a great financial cost to the society. A medicine decision support system in a Personal Digital Assistant (PDA), with a barcode reader, can provide an overview of the patientsu0027 complete medicine use, and detect unsuitable drugs and drug combinations. Focusing on the elderly, our aim was to evaluate if a mobile medicine decision support system with a barcode reader is useful and user-friendly for nurses in home care. The participants received a comprehensive overview from the patientsu0027 medicine use and noted drug-drug interactions, therapeutic duplications and warnings for drugs unsuitable for elderly people. The nurses regarded that the decision support system increased prevention and safety, was useful and user-friendly. Our findings suggest that most of the content and functions were regarded as important. Therefore, this decision support system might be a useful tool for district nurses.",
        "prediction": "Using medicines the wrong way can lead to more hospital visits for older people. This not only causes pain for patients but also costs a lot of money for society. A mobile medicine decision support system with a barcode reader can help nurses see all the medicines a patient uses. It can also find unsafe drugs and drug combinations. We focused on older patients to see if this mobile system is helpful and easy to use for nurses at home. The nurses got a full view of the patients' medicine use and saw issues like drug-drug interactions, duplicate therapies, and warnings for medicines not suitable for older people. The nurses felt the system improved safety and was helpful and easy to use. Our results show that most features were seen as important. So, this system could be a valuable tool for district nurses.",
        "reference": "Misusing medicines can make older people more likely to end up in the hospital. This causes patient suffering and financial cost to society. A support system for medicine decision, with a bar code reader, in a PDA can help identify inappropriate medication use and combinations by providing a complete overview of the patient's medication The goal was to evaluate if a mobile medicine decision support system with a barcode reader is helpful and easy to use for home care nurses working specially with older patients. The participants were provided with a complete overview of the patient's medication usage, and identified any drug interactions, duplicated medications, and any warnings about drugs that are not suitable for elderly patients. The nurses found the decision support system to be helpful, and easy to use, and improving the prevention and safety measures. Our results indicate that the majority of the content and features were considered significant. So, this system could be helpful for district nurses.",
        "metrics": {
          "SARI": 50.735946179354784,
          "BLEU": 0.20565266582274777,
          "FKGL": 7.1
        }
      },
      {
        "source": " :[0],\"access to patient appointment schedules can help clinicians manage time and problems better. Many large health care organizations manage clinical appointment scheduling via enterprise resource scheduling systems which are poorly accessible by clinicians. Also, staff other than the clinicianu0027s personal assistant(s) may manage scheduling, making it difficult for clinicians to stay informed of changes. Many a clinician today uses a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician.",
        "prediction": "Access to patient appointment schedules helps clinicians manage their time and issues better. Many large healthcare organizations use complex systems to schedule appointments, making it hard for clinicians to access them. Sometimes, staff other than a clinician's assistant handle scheduling, which can lead to missed updates. Many clinicians use personal digital assistants (PDAs) with basic calendar features. Our project \"PalmOversite\" shows that it's possible to connect enterprise appointment schedule information to a PDA calendar, making it easier for clinicians to access schedule information.",
        "reference": "Knowing patients' appointment schedules can help clinicians manage time and problems more effectively. Healthcare organizations often use enterprise resource scheduling systems for clinical appointment scheduling, but these systems may not be easily accessible by clinicians. Moreover, scheduling may not only be handled by the clinician's assistant(s), creating challenges for clinicians to stay updated on any changes. Many clinicians today use a personal digital assistant (PDA) with a basic calendar. Our project called 'PalmOversite' shows the practicability of integrating enterprise appointment schedule information into a PDA calendar. This makes it easier for clinicians to access their schedule information.",
        "metrics": {
          "SARI": 38.51483195623378,
          "BLEU": 0.14270691676562317,
          "FKGL": 13.3
        }
      },
      {
        "source": "Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. Currently, a SES employee visits each health establishment to collect this information by hand, process it and type it into an electronic medical record system  this :[81],\"paper, we describe the development and implementation of a personal digital assistant (PDA)-based electronic system to collect, verify and upload monthly bacteriology data into the PIH-EMR. After an initial implementation period, we performed a pilot study to test the use of this system. We completed a baseline assessment in two health districts and then implemented the electronic system in one of the districts while the control site continued to use the paper-based system during the same period. The PDA-based system had a processing time of 6.2 days, significantly lower than measurements for both the baseline [54.8] and control sites [64.4] (both pu003c0.0001). It was also able to reduce the frequency of discrepancy from 10.1% to 2.8% (pu003c0.0001) and receive positive feedback from the users. Finally, the system’s cost would be recuperated in three months from time savings due to increased work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors and costs.",
        "prediction": "Partners In Health and its partner group in Lima, Peru, Socios En Salud, help many patients with multidrug-resistant tuberculosis in Peru. They work with the Peruvian National TB Program. Monthly tests are important for this treatment. A worker from Socios En Salud visits health centers to collect test information by hand and then types it into an electronic medical record.\n\nIn this study, we created and used a digital assistant system to collect, check, and upload monthly test data into the electronic record. After starting the system, we tested it in a pilot study. We looked at two health areas first. Then, we used the electronic system in one area while the other area continued with paper.\n\nThe digital system worked faster, needing only 6.2 days compared to the 54.8 days in the baseline and 64.4 days in the control area. It also reduced mistakes from 10.1% to 2.8%. Users gave positive reviews. The cost of the system would be paid back in three months because of saved time and better work. We will study this system more to see how it affects delays, mistakes, and costs.",
        "reference": "PIH (Partners In Health) and SES (Socios En Salud) provide treatment for most of the MDR-TB (multidrug-resistant tuberculosis) patients in Peru along with the Peruvian NTP (National TB Program). Monthly bacteriology tests are necessary for this treatment and must be collected from various health establishments located throughout the city. An employee from SES is currently going to each health establishment and collecting monthly bacteriology test results by hand, processing them, and typing them into an electronic medical record system. In this paper, we explain how we made a digital system using a personal digital assistant (PDA) to gather, check, and upload monthly bacteriology data into the PIH-EMR (Partners In Health Electronic Medical Record).  After an initial period of putting the system into action, we conducted a test study to evaluate the effectiveness of using this system. We first evaluated the current system in two health districts, then introduced the electronic system in one district while the other continued using the paper-based system during the same time. The PDA-based system took only 6.2 days to process, which was significantly lower than both the baseline (54.8) and control sites (64.4) (both less than 0.0001). The PDA-based system reduced differences from 10.1% to 2.8% and received positive feedback from users. The system would pay for itself in three months due to increased efficiency and time savings. A larger study will be conducted to assess the impact of this system on delays, errors, and costs.",
        "metrics": {
          "SARI": 43.423670533131826,
          "BLEU": 0.13717631009604733,
          "FKGL": 6.4
        }
      },
      {
        "source": "In order to facilitate knowledge transfer between specialists and generalists and between experts and novices, and to promote interdisciplinary communication, there is a need to provide methods and tools for doing so. This interdisciplinary research team developed and evaluated a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage/tele-consultation when the presenting problem was chest pain. The combined human factors methods of cognitive work analysis during the requirements-gathering phase and ecological interface design during the design phase were used to develop the DST. A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period. During this time, the DST was used by the nine nursing coordinators who provide tele-triage/tele-consultation 24/7. This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses, its potential for use by nurses less experienced in cardiac care, and for its potential use in an interdisciplinary team environment.",
        "prediction": "To help specialists and generalists share knowledge better, we need tools and methods. A research team created a decision support tool (DST) that works on a personal digital assistant (PDA) for cardiac tele-triage and tele-consultation, especially for chest pain cases. They used two methods: cognitive work analysis to gather needs and ecological interface design to create the tool. A pilot clinical trial was done at a major cardiac care hospital over three months. During this time, nine nursing coordinators used the DST for tele-triage and tele-consultation around the clock. The trial showed that the DST design was effective and helpful for advanced cardiac care nurses. It also showed promise for less experienced nurses and for use in teams from different fields.",
        "reference": "To improve communication between specialists, generalists, experts, and novices and encourage interdisciplinary collaboration, we need to develop methods and tools. An interdisciplinary research team created and tested a decision support tool (DST) on a personal digital assistant (PDA) to aid cardiac tele-triage/tele-consultation for patients with chest pain. To create the DST (Decision Support Tool), we used both cognitive work analysis and ecological interface design methods during the requirements-gathering and design phases. A trial was done for 3 months at a quaternary cardiac care hospital During the trial, nine nursing coordinators used DST for 24/7 tele-triage/tele-consultation. This clinical trial confirmed that the design is useful for advanced cardiac care nurses, as well as for less experienced nurses and interdisciplinary teams.",
        "metrics": {
          "SARI": 53.10473939160164,
          "BLEU": 0.26387760758528017,
          "FKGL": 10
        }
      },
      {
        "source": "This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Since conventional computer-assisted language learning is often boring, it exploits a cyber pet game to increase the learner’s interest. M-CALL runs on a personal digital assistant with public wireless LAN for mobile learning. It grows a cyber pet by solving problems of Korean language learning. Korean Proficiency Test (KPT), a nationally certified Korean language test, was used as problem sets. It consists of cyber pet game, mobile learning courseware, mobile learning system, and mobile tutoring. It provides various functions for Korean language learning. Currently, the prototype M-CALL was designed and partly implemented between mobile PDA and personal computer.",
        "prediction": "This paper is about M-CALL. M-CALL is a mobile language learning tool for people who want to learn Korean. Traditional language learning programs can be boring. M-CALL uses a cyber pet game to make learning fun. It works on a personal digital assistant and uses public Wi-Fi for mobile learning. Learners take care of a cyber pet by solving Korean language problems. These problems come from the Korean Proficiency Test, a test that shows how well someone knows Korean. M-CALL includes a cyber pet game, mobile learning tools, and mobile tutoring. It has many features to help learn Korean. The M-CALL prototype is partially created to work between a PDA and a personal computer.",
        "reference": "M-CALL is a mobile courseware for Korean language learners. It incorporates a cyber pet game to make it more engaging for learners than traditional computer-assisted language learning methods. M-CALL is a mobile learning system that uses a personal digital assistant with public wireless LAN (Computer network in small geographic areas). A cyber pet is raised while solving problems related to Korean language learning. The KPT, a nationally recognized exam for Korean language proficiency, served as the source material for the problem sets. It includes a virtual pet game, a mobile course for learning, a mobile learning platform, and mobile tutoring. It offers multiple features for learning Korean. Right now, a preliminary version of M-CALL has been developed and partially implemented for use on both mobile PDAs and personal computers.",
        "metrics": {
          "SARI": 46.828777127730525,
          "BLEU": 0.16270597034090797,
          "FKGL": 7.7
        }
      },
      {
        "source": "This study compared text entry performance of two stylus-driven soft keyboards for use in hand-held computing devices: the QWERTY and the T9. Participants transcribed text presented on a computer screen into a personal digital assistant (PDA) using a stylus and one of these two keyboards. We introduced a new psychophysical technique for measuring transcription rate that provides a composite measure of speed and accuracy. Using this technique, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparent superior physical characteristics. An ancillary experiment demonstrated that the poorer performance of the T9 layout may have resulted from an increase in visual scanning time due to perceptual grouping of the letters on the keys. Together these findings imply that the QWERTY keyboard layout remains the most effective of the currently available designs for stylus tapping on soft keyboards.",
        "prediction": "This study looked at how well two stylus keyboards work on hand-held devices: the QWERTY and the T9. Participants typed text from a computer screen into a personal digital assistant using a stylus and one of these keyboards. We used a new method to measure typing speed that combines speed and accuracy. We found the maximum typing speed for each keyboard. The results showed that the QWERTY keyboard had faster typing speeds than the T9, even though the T9 seemed better designed. An extra test suggested that the T9's lower speed might be due to longer time spent looking at the letters on the keys. Overall, these findings suggest that the QWERTY keyboard is the best choice for stylus typing on soft keyboards.",
        "reference": "This study compared two onscreen keyboards, the QWERTY and the T9, for text entry performance on hand-held computing devices. Participants used a stylus to input text on a PDA using either a QWERTY or T9 keyboard in this study. We created a new way to measure transcription rate that considers both speed and accuracy together. We used this method to determine the fastest possible typing speed for each keyboard. The study found that the QWERTY keyboard had a higher transcription rate than the T9 keyboard, even though the T9 keyboard was physically better. Another test showed that the T9 keyboard's lower performance might be because the grouped letters on its keys require more visual scanning time. Overall, these results suggest that, among the current options, the QWERTY keyboard layout is still the most efficient for typing on touch screens with a stylus.",
        "metrics": {
          "SARI": 51.65915835416514,
          "BLEU": 0.20223495912720166,
          "FKGL": 7.8
        }
      },
      {
        "source": "The proliferation of mobile devices such as smart phones and Personal Digital Assistant (PDA) opens new ways for developing mobile E-commerce systems (so called Mobile-commerce or M-commerce system). In Mcommerce systems, we still see many common components that we have in standard E-commerce applications, such as web servers and database servers. However, these new applications raise some unique challenges. For example, the limitations of mobile devices (e.g., small screen size and reduced CPU performance) implies that software development is partly different from desktop applications. Questions which are posed when designing M-commerce system include: “What kind of information do the users want to access via mobile devices?”, “How can we provide useful applications with limited user input facility?”, “How can we test a distributed M-commerce system?” In this paper, we discuss about some software development issues for mobile commerce systems from our experiences while developing an M-commerce system for tourists. Our prototype, easyHotel, is an useful software that allows booking hotel rooms via mobile phones.",
        "prediction": "Mobile devices like smartphones and PDAs are changing how we do mobile E-commerce, also called M-commerce. M-commerce systems have some of the same parts as regular E-commerce systems, like web servers and database servers. However, they have unique challenges. Mobile devices have limits, such as small screens and less powerful CPUs. This means software development is different than for desktop applications. When designing M-commerce systems, we ask questions like: What information do users want on mobile devices? How can we create useful apps with limited user input? How do we test a distributed M-commerce system? In this paper, we share our experiences developing an M-commerce system for tourists. Our prototype, easyHotel, helps users book hotel rooms through their mobile phones.",
        "reference": "The increasing use of mobile devices like smartphones and PDAs provides opportunities for the development of new mobile e-commerce systems, also known as M-commerce (Mobile-commerce) systems. M-commerce systems share several components with traditional E-commerce applications, such as web servers and database servers. However, these new applications present some distinct challenges. For example, due to mobile devices' limitations, such as smaller screens and weaker processors (also called CPUs), software development for mobile applications is partially different from desktop applications. Designing an M-commerce system involves asking questions such as: \"What information do users want on mobile devices?\", \"How can we create useful apps with limited user input?\", and \"How do we test a distributed M-commerce system?\" In this paper, we share our experiences developing an M-commerce system for tourists and discuss software development issues for such systems. Our software prototype, easyHotel, is a helpful application enabling users to book hotel rooms through their mobile phones.",
        "metrics": {
          "SARI": 61.85535004643328,
          "BLEU": 0.34693109544186157,
          "FKGL": 8
        }
      },
      {
        "source": "It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As emerging mobile platforms such as different kinds of Personal Digital Assistant (PDA) tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common approaches for content adaptation are automated conversion and explicit specification of adapted content, with a trade-off between quality and development/maintenance effort. As alternative avoiding this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework in the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development/maintenance effort.",
        "prediction": "Web content needs to be adjusted for mobile devices because they have limitations. Different mobile devices, like Personal Digital Assistants (PDAs), have different abilities. We believe that changes should be made based on each device. There are two common ways to adapt content: automatic conversion and manual changes. Each method has a balance between quality and the effort needed to develop and maintain it. To avoid these issues, we suggest a simple framework for adapting content. This framework is based on the object-oriented WebComposition model and its XML system, WCML. We use this approach in an example application to show how object-oriented content can lower development and maintenance efforts.",
        "reference": "It is widely recognized that information, such as web content, should be adjusted for mobile platforms to accommodate the limitations of mobile environments. As new mobile platforms, such as various types of PDAs, differ significantly in their capabilities, we propose that adaptation should be tailored to each platform. Common methods for content adaptation involve automated conversion or explicitly specifying adapted content, balancing the trade-off between quality and the effort required for development and maintenance. Instead of making this trade-off, we suggest using a straightforward object-oriented framework for content adaptation. To make this framework more accessible for use on the web, we build our approach on the object-oriented WebComposition model and its implementation, WCML, which is an application of XML. We use our object-oriented framework in an example application to show how specifying platform-adapted content using an object-oriented approach can lower development and maintenance costs.",
        "metrics": {
          "SARI": 43.88453159483147,
          "BLEU": 0.16676025056075694,
          "FKGL": 9.8
        }
      },
      {
        "source": "In monitoring a patientu0027s real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in timely-context aware manner. In this paper a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device — PSD) used in the Healthcare systems are bulky in design and are not flexible and comfortable for the elderly patients. Presented is a unique flexible, as well as detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage and processing of patient related data, which ensures periodic logging of patient data without saturating communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smart-phone and the ECG and temperature acquisition apparatus is implemented Bluetooth. The presented Data Management System — Periodic Model (DMS — PM) is designed to manage wireless interface of sensor units with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a Smart phone making use of the existing GSM network.",
        "prediction": "In monitoring a patient's vital signs using Body Area Networks (BAN), important data is shared with doctors. It is crucial that this data is sent quickly and is relevant. This paper describes a system for patients with heart problems focusing on the design of the sensing device and how it communicates. Current wearable devices for health care are often big and uncomfortable for older patients. We introduce a flexible and detachable Patient Sensing Device (PSD) for better comfort. We also explain a method for transmitting, storing, and processing patient data, which keeps track of data without overloading networks. A working prototype has been created to send Electrocardiogram (ECG) signals and body temperature, with the ability to add more vital signs later. Communication between a smartphone and the ECG/temperature device is done using Bluetooth. The Data Management System - Periodic Model (DMS - PM) is made to manage the wireless connection of sensors with the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or smartphone using the GSM network.",
        "reference": "Medical practitioners can access a wealth of data sources by using Body Area Networks (BAN) to monitor a patient's real-time vital signs. A BAN refers to a network of wearable or implantable devices worn on or attached to the human body for various purposes. Timely and context-aware delivery of data is crucial. This paper presents a system specifically designed for patients with cardiac disorders, focusing on the design of the sensing device and communication scheme selected. Many of the current wearable physiological devices used in healthcare systems, known as Patient Sensing Devices (PSD), are bulky and uncomfortable for elderly patients due to their inflexible design. We introduce a unique Patient Sensing Device (PSD) that is both flexible and detachable, designed for patient comfort. We also propose a model for transmitting, storing, and processing patient-related data that ensures regular logging of data without overwhelming communication networks. We have developed and implemented a proof-of-concept prototype that can transmit a patient's Electrocardiogram (ECG) signal and body temperature, potentially expanding to other vital signs. Communication between a mobile smartphone and the ECG and temperature acquisition apparatus is achieved via Bluetooth. The Data Management System - Periodic Model (DMS - PM) introduced here is created to facilitate the wireless connection between sensor units and the patient database at a Medical Service Provider (MSP) through a Personal Digital Assistant (PDA) or a smartphone, utilizing the existing GSM network.",
        "metrics": {
          "SARI": 56.307563132814764,
          "BLEU": 0.3144165223329406,
          "FKGL": 11.9
        }
      },
      {
        "source": "Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. Current trend integrates usage of mobile terminals such as smart PDA (personal digital assistant) in a learning system. Such devices can communicate with wireless mobile terminal and realize the intelligence exchange with its surroundings [1]. A LOCH (Language Learning Outside the Classroom) system named One Day Trip is developed to assist overseas students to learn Japanese in real life situations with the help of a PDA. This paper proposes the implementation with Knowledge Management (KM) concepts in the system to aid the teacher and learners in the learning environment.",
        "prediction": "The rise of wireless and mobile technology helps learning in one space. Now, mobile devices like PDAs (personal digital assistants) are used in learning systems. These devices can communicate with other mobile devices and share information with their surroundings. A system called LOCH (Language Learning Outside the Classroom) named One Day Trip is made for foreign students to learn Japanese in real-life situations using a PDA. This paper suggests using Knowledge Management (KM) ideas in the system to support both teachers and students in learning.",
        "reference": "Wireless and mobile technology progress has allowed more platforms to facilitate learning within a unified space. The current trend is to use mobile devices, such as smart PDAs, in learning systems. These devices can communicate wirelessly with mobile terminals and exchange information with the surrounding environment One Day Trip is a language learning system called LOCH (Language Learning Outside the Classroom), designed to help foreign students learn Japanese in real-life situations using a PDA. This paper suggests incorporating Knowledge Management (KM) concepts in the One Day Trip system to support both the teacher and learners in the learning environment.",
        "metrics": {
          "SARI": 57.93789367656854,
          "BLEU": 0.3602341426036702,
          "FKGL": 9.9
        }
      },
      {
        "source": "In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko personal digital assistant, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Instead of porting each application to individual platforms, which would prove to be extremely time consuming because of other APIu0027s and networking protocols, we created the Vassar College Messaging Layer. This layer abstracts the Cybiko networking structure and interface, allowing Cybiko applications to be ported to other platforms with minimal code alteration.",
        "prediction": "In recent years, we made apps to teach wired and wireless networking. These apps were made for the Cybiko personal digital assistant, which is a cheaper option than other devices. Sadly, the Cybiko PDA is no longer available. Because of this, we had to move the apps to other platforms to keep them useful. Instead of changing each app for different platforms, which would take a lot of time, we created the Vassar College Messaging Layer. This layer simplifies the Cybiko networking system. It allows Cybiko apps to be moved to other platforms with little code change.",
        "reference": "In recent years, we have created applications for teaching both wired and wireless networking. These applications are designed for the Cybiko personal digital assistant, which offers a cost-effective alternative to other hardware options. The Cybiko PDA was discontinued recently. Consequently, these applications needed to be adapted to other platforms to maintain their usefulness. To avoid the time-consuming task of porting each application to various platforms due to different APIs and networking protocols, we developed the Vassar College Messaging Layer. This layer simplifies the Cybiko networking structure and interface, making it easier to adapt Cybiko applications to different platforms without needing to make significant changes to the code.",
        "metrics": {
          "SARI": 45.80437800952728,
          "BLEU": 0.27771728906157517,
          "FKGL": 7.5
        }
      },
      {
        "source": "Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful (and thus more useful), machines. On the other side of this border we find the Personal Digital Assistant  :[66],\"our view, to be useful as an extension of the usersu0027 private sphere, a machine must at least have enough functionality and resources to create trustworthy digital signatures (to speak for the user, as it were). A less resourceful machine can merely act as a memory prothesis, helping the owner remembering addresses and phone  are :[121],\"designed to be tamper resistant, and as such they seem ideal as a minimal machine. However, trustworthy digital signatures can not be created by smartcards alone, simply because the user does not know what is given to the card for  :[163],\"order to be trusted--that is, being able to make trustworthy digital signatures--a smartcard must be supported by some infrastructure outside the card proper. We explore what must be included in such an infrastructure, and demonstrate that trustworthy digital signatures can in fact be made using a standard  :[211],\"argue that based on this fact, nontrivial distributed systems can be constructed by utilizing smartcards; a nontrivial system is one where holders of smartcards are \"first class citizens\". Asserting that a smartcard can act as a trusted machine gives new opportunities for designers of user-centric  that smartcards are :[257],\"here to stay, finding ways to apply them in constructive manners is prudent.",
        "prediction": "Smartcards are different from other hardware. They cannot communicate securely with their users. This makes it hard to use their good features. We look at the area between smartcards and stronger machines. On the stronger side, we find Personal Digital Assistants. To be useful, a machine should be able to create trustworthy digital signatures. This means it can represent the user. A weaker machine can only help the user remember things like addresses and phone numbers. \n\nSmartcards are designed to resist tampering. This makes them good for basic tasks. However, they cannot create trustworthy digital signatures by themselves. This is because users don't know what information the card gets. To trust a smartcard, it needs support from external systems. We examine what this support should include. We show that it is possible to create trustworthy digital signatures using common tools. \n\nBased on this, we believe that smartcards can help build complex systems. In these systems, smartcard holders have important roles. Saying that a smartcard is a trusted device opens new chances for designers. Since smartcards are likely to remain useful, we should find smart ways to use them.",
        "reference": "Smartcards lack the capability to communicate with the user in a secure manner, unlike other hardware classes. (Smart cards are small, portable devices that have a special chip inside. They can store and process information securely.) Without a way to keep the owner updated, it becomes challenging to take advantage of the useful features of smart cards. We look at how smart cards and powerful machines can be combined to make even better devices. We believe that Personal Digital Assistants (PDAs) are more useful than smartcards because they can extend the user's private sphere. PDAs have enough functionality and resources to create trustworthy digital signatures, which can speak for the user. A less resourceful machine can serve as a memory aid, assisting the owner in remembering addresses and phone numbers, such as [121]. These machines are designed to be tamper-resistant, making them suitable for a minimalistic purpose. To create trustworthy digital signatures, a smartcard needs support from infrastructure outside the card itself. This is because the user is not aware of what information is given to the card, making it impossible for the card to be trusted solely on its own. We investigate the necessary components of this infrastructure and show that reliable digital signatures can indeed be created using a standard method [211]. We propose that smartcards can be utilized to build nontrivial distributed systems, where smartcard holders are considered important participants or \"first class citizens\". Claiming that smartcards can function as trusted machines opens up fresh possibilities for designers focused on user-centric approaches. It is evident that smartcards are here to stay, so it is wise for designers to explore and implement them in meaningful ways.",
        "metrics": {
          "SARI": 41.47285904804357,
          "BLEU": 0.08561854933031482,
          "FKGL": 6
        }
      },
      {
        "source": "In this paper we present the main features of a system that we have developed and that provides a new kind of tele-assistance service to elderly people.apart from the functionalities offered by the current tele-assistance services, also provides a high quality, anywhere and at any time assistance. We rate the assistance as high quality because allows the monitoring of vital signs and a local detection of anomalous situations by the device that the person carries in real time. Therefore, it sends alarms autonomously when  this paper we :[84],\"explain the three main functionalities offered by AINGERU: universal assistance anywhere and at any time, monitoring of vital signs irrespective of location or time and remote monitoring by allowing authorized external persons to consult data about monitored persons using the Internet. Moreover we show how those functionalities are achieved by making use of PDAs (Personal Digital Assistant), wireless communication, Semantic Web, Web services and agent technologies.",
        "prediction": "In this paper, we describe a system we developed for tele-assistance for elderly people. This system is different from current services. It offers high-quality help anytime and anywhere. It monitors vital signs and detects problems in real time. The device alerts caregivers automatically. We highlight three main functions of our system: universal support anywhere, vital signs monitoring at any time, and remote access for authorized people to view data online. We explain how we achieve these functions using PDAs, wireless communication, Semantic Web, Web services, and agent technologies.",
        "reference": "This paper introduces a new tele-assistance service for elderly people that goes beyond existing services by offering high-quality assistance anywhere and anytime. Our system provides high-quality assistance because it allows real-time monitoring of vital signs and local detection of anomalous situations through the device carried by the person. We explain the three main functions of AINGERU in this paper: universal assistance anywhere and at any time, monitoring vital signs regardless of location or time, and remote monitoring that allows authorized external individuals to access data about monitored persons via the internet. We use PDAs, wireless communication, Semantic Web (aims to enhance the World Wide Web by adding meaning to the information available online), Web services, and agent technologies to achieve the aforementioned functionalities.",
        "metrics": {
          "SARI": 47.316211232196686,
          "BLEU": 0.18000281373486926,
          "FKGL": 10.6
        }
      },
      {
        "source": "Objective: To evaluate the effectiveness of a personal digital assistant (PDA)–based clinical decision support system (CDSS) on nonsteroidal anti-inflammatory drug (NSAID) prescribing safety in the outpatient  :[26],\"The design was a randomized, controlled trial conducted in a university-based resident clinic. Internal medicine residents received a PDA-based CDSS suite. For intervention residents, the CDSS included a prediction rule for NSAID-related gastrointestinal risk assessment and treatment recommendations. Unannounced standardized patients (SPs) trained to portray musculoskeletal symptoms presented to study physicians. Safety outcomes were assessed from the prescriptions given to the SPs. Each prescription was reviewed by a committee of clinicians blinded to participant, intervention group assignment, and baseline or follow-up  :[108],\"Prescriptions were judged as safe or unsafe. :[26],\"The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control  :[135],\"At baseline, the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar (0.27 vs. 0.29, p u003e 0.05). Controlling for baseline performance, intervention participants prescribed more safely than controls after receiving the CDSS (0.23 vs. 0.45 [F = 4.24, p u003c 0.05]). With the CDSS, intervention participants documented more complete assessment of patient gastrointestinal risk from  :[198],\"Participants provided with a PDA-based CDSS for NSAID prescribing made fewer unsafe treatment decisions than participants without the CDSS.",
        "prediction": "The study aimed to check if a personal digital assistant (PDA) helps doctors prescribe nonsteroidal anti-inflammatory drugs (NSAIDs) safely. It was a controlled trial at a university clinic with internal medicine residents. The residents used a PDA that had a clinical decision support system (CDSS). This CDSS had a tool to assess gastrointestinal risks of NSAIDs and gave treatment advice. \n\nStandardized patients acted out musculoskeletal symptoms and visited doctors involved in the study. The safety of prescriptions was reviewed by a group of doctors who did not know which group the participants belonged to. They decided if prescriptions were safe or unsafe. \n\nThe main goal was to see if the group using the CDSS had fewer unsafe NSAID prescriptions compared to the control group. Before the study, both groups had similar rates of unsafe prescriptions. After using the CDSS, the group with the PDA prescribed more safely. They also did a better job of assessing patients' gastrointestinal risks. \n\nIn summary, doctors who used the PDA-based CDSS for NSAID prescribing made fewer unsafe choices than those who did not use it.",
        "reference": "We did a study to test if a clinical decision support system (CDSS) on a personal digital assistant (PDA) could improve the safety of prescribing nonsteroidal anti-inflammatory drugs (NSAIDs) in outpatient clinics. We conducted a randomized, controlled trial in a university-based resident clinic. Residents in internal medicine were given a CDSS suite based on PDA. Intervention residents were provided with a CDSS that had a prediction rule for assessing gastrointestinal risk related to NSAIDs and recommendations for treatment. Trained individuals, known as standardized patients (SPs), who were equipped with musculoskeletal symptoms, interacted with the physicians participating in the study without prior notification. The study evaluated the safety outcomes of prescriptions given to standardized patients portraying musculoskeletal symptoms. A committee of clinicians who didn't know about the participant, intervention group assignment, or the baseline or follow-up data reviewed each prescription. They determined whether a prescription was safe or unsafe. The study aimed to see if the PDA-based CDSS would result in a reduction of unsafe prescribing of NSAIDs. The main outcome measured the difference in unsafe prescribing between the intervention and control groups. At the beginning of the study, both groups had a similar proportion of unsafe prescriptions. (0.27 vs 0.29) After controlling for the baseline performance, it was found that those who received the CDSS suite prescribed nonsteroidal anti-inflammatory drugs more safely than those who didn't (0.23 vs 0.45, with statistical significance). Participants who had access to the CDSS through a PDA documented a more comprehensive assessment of gastrointestinal risk in patients. Moreover, the presence of the CDSS resulted in fewer unsafe treatment decisions compared to participants who did not have access to the CDSS.",
        "metrics": {
          "SARI": 47.49784063037063,
          "BLEU": 0.14760356242310163,
          "FKGL": 9.1
        }
      },
      {
        "source": "Recently pedestrian navigation has been studied by many researchers, which provides pedestrians with a similar function of car navigation. On the other hand, a 3dimensional (3D) map, which is one of major themes in machine vision research, has been utilized as a simulation tool in city and landscape planning, and other engineering fields. The 3D maps will give more intuitive information compared to conventional 2-dimensional (2D) ones. In this paper we first propose pedestrian navigation based on 3D maps, and describe technologies required and its use situations. Secondly we compare effectiveness of 2D and 3D maps for navigation by object search experiments under conditions: 3D maps with and without texture; display sizes corresponding to a mobile phone and PDA (Personal Digital Assistant). The experimental results show that 3D maps with texture on mobile phone display size, and 3D maps without texture on PDA display size are superior to 2D maps in search time and error rate. From these results, we can say the effectiveness of 3D maps in pedestrian navigation.",
        "prediction": "Recently, many researchers have studied pedestrian navigation. This helps people find their way like car navigation does. A 3-dimensional (3D) map is important in machine vision research. It is used for city and landscape planning and in engineering. 3D maps provide clearer information than 2-dimensional (2D) maps. In this paper, we propose using 3D maps for pedestrian navigation. We explain the required technologies and how they can be used. We also compare 2D and 3D maps in terms of effectiveness. We performed object search experiments using 3D maps with and without texture, and mobile phone and PDA display sizes. The results show that 3D maps with texture on mobile displays, and 3D maps without texture on PDA displays work better than 2D maps. This shows that 3D maps are effective for pedestrian navigation.",
        "reference": "Recently many researchers have studied pedestrian navigation, which is similar to car navigation but designed for pedestrians. A 3D map is widely used in machine vision research and various engineering fields, such as city and landscape planning, as a simulation tool. 3D maps are more intuitive than 2D maps. In this paper, we suggest using 3D maps for pedestrian navigation and explain the necessary technology and possible scenarios for its application. We will conduct object search experiments to compare the effectiveness of 2D and 3D maps for navigation. The experiments will consider the following conditions: 3D maps with and without texture, and display sizes equivalent to a mobile phone and a Personal Digital Assistant (PDA). The results of the experiment suggest that 3D maps with texture on a mobile phone display and 3D maps without texture on a PDA display are better than 2D maps in terms of search time and error rate. Based on these results, we can conclude that 3D maps are effective for pedestrian navigation.",
        "metrics": {
          "SARI": 52.74594339901684,
          "BLEU": 0.3236897128108152,
          "FKGL": 8
        }
      },
      {
        "source": "Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments, that are characterized by low bandwidth communication facilities and ad hoc connection/disconnection to stationary systems. They can be used for information retrieval and information filtering, in which case they evaluate replies and return only the relevant data. Mobile agents as a metaphor of active objects are created on a mobile device such as a Personal Digital Assistant (PDA), will be launched into the information galaxy and are fulfilling the mobile user ‘s task on the services available on networked stationary systems. One transmission channel for these itinerant agents is email. This paper introduces Active M 3 as an example of an active mail framework, which can be regarded as a first approach in authoring mobile agents in a graphical interactive manner. Active M 3 integrates two known concepts: active mail and multimedia mail.",
        "prediction": "Mobile agents are a new idea in client-server computing. They work well in mobile setups, which often have slow internet and unstable connections. Mobile agents help find and filter information. They check responses and send back only what is important. Mobile agents act like active objects created on portable devices like PDAs. They travel in the information world to complete tasks for mobile users using services from fixed systems. One way these agents communicate is through email. This paper talks about Active M 3, which is an example of active mail. It is a first step in creating mobile agents using a graphical interface. Active M 3 combines two concepts: active mail and multimedia mail.",
        "reference": "Mobile agents are a competitive concept in client-server computing, particularly well-suited for mobile environments. These environments are characterized by limited bandwidth communication and the frequent connection and disconnection to stationary systems. They can be utilized for information retrieval and filtering, where they assess responses and provide only the pertinent data. Mobile agents act like active objects that can be created on a mobile device like a PDA. They are sent out to perform tasks on the networked stationary systems and fulfill the user's needs. Email serves as a transmission channel for these mobile agents. This paper presents Active M 3, which is an active mail framework that allows users to create mobile agents using a graphical interface. Active M3 is a framework that combines two concepts: active mail and multimedia mail.",
        "metrics": {
          "SARI": 49.81016461818284,
          "BLEU": 0.2582718793441929,
          "FKGL": 6.6
        }
      },
      {
        "source": "Digital Video Broadcasting — Handheld (DVB-H) is a technology developed as an extension of Digital Video Broadcasting — Terrestrial (DVB-T) with additional features that make it suitable for delivery to mobile devices such as phones and Personal Digital Assistant (PDAs). This paper deals with the conditions of receiving digital video broadcast type services in mobile handheld devices. The DVB-H standard is introduced and the key technology elements on data link layer and physical layer are discussed in some details. Finally the DVB-H terminal and network design is described.",
        "prediction": "Digital Video Broadcasting - Handheld (DVB-H) is a technology that builds on Digital Video Broadcasting - Terrestrial (DVB-T) to improve delivery to mobile devices like phones and PDAs. This paper looks at how to receive digital video broadcast services on handheld devices. It introduces the DVB-H standard and explains important parts of the data link layer and physical layer. It also describes the design of DVB-H terminals and networks.",
        "reference": "DVB-H is a technology based on DVB-T that is designed for delivering content to mobile devices like phones and PDAs. This paper discusses how digital video broadcast services can be received on mobile handheld devices. The introduction of the DVB-H standard includes a discussion of key technology elements on the data link layer and physical layer, providing some detailed information. This paper also covers the design of DVB-H terminals and networks.",
        "metrics": {
          "SARI": 59.47695056891153,
          "BLEU": 0.3707470249183508,
          "FKGL": 11
        }
      },
      {
        "source": "Summary form only given, as follows. We present a design for a low cost but powerful and high speed communications device powered by IDTu0027s RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus compatible IDT ATM NIC. The device could be used as a two way pager, a set-top box, an Internet terminal, a personal digital assistant or as a video phone. The RV4640 is a very affordable 64 bit RISC processor that executes 175 dhrystone MIPS at 133 MHz. The RV4640 can be connected to a System Interface Chip which will provide I/O and memory control. The IDT77903 ATM card is a full duplexed 25 Mbps NIC with a PCI bus interface and it costs less than $100 (US). Integrating this card into our device as a network interface will make highspeed videoconferencing and multimedia applications easily implementable.",
        "prediction": "We designed a low-cost, fast communication device. It uses the IDT RV4640 processor. This device can work very quickly because of the compatible IDT ATM NIC. It can be a two-way pager, set-top box, internet terminal, digital assistant, or video phone. The RV4640 is a cheap 64-bit RISC processor that runs at 133 MHz and executes 175 dhrystone MIPS. It connects to a System Interface Chip for I/O and memory control. The IDT77903 ATM card is a 25 Mbps NIC with PCI bus interface and costs under $100. This card helps to easily create high-speed video calls and multimedia applications.",
        "reference": "Summary provided in the following. We propose a low-cost and high-speed communication device that uses IDT's RV4640 as the processing engine. This device is designed with IDT's RV4640 as the processing engine and can achieve high speed due to the PCI bus (a type of computer bus that simplifies the communication between computer components by providing a standardized interface) compatible IDT ATM NIC. This device can serve multiple purposes such as a two-way pager, set-top box, Internet terminal, personal digital assistant, or video phone. The RV4640 is a low-cost 64-bit RISC processor that runs at 133 MHz and can execute 175 dhrystone MIPS. The RV4640 can connect to a System Interface Chip for I/O (a specialized integrated circuit designed to facilitate communication between a computer system and its external devices) and memory control. The IDT77903 ATM card, which costs less than $100 (US), is a NIC with a PCI bus interface that is capable of full duplexing at 25 Mbps. Using the IDT77903 ATM card as a network interface in our device will make it easy to implement high-speed videoconferencing and multimedia applications.",
        "metrics": {
          "SARI": 56.08970193685505,
          "BLEU": 0.22670209394251062,
          "FKGL": 7
        }
      },
      {
        "source": "Dr. Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework, which serves as the foundation for the NET natural user interface. The paper discusses MiPadu0027s design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates compelling user advantages for wireless personal digital assistant (PDA) devices, MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in todayu0027s PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built in roller to navigate, the tapping action narrows the number of possible instructions for spoken word understanding. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition is performed. The Dr Who CSR engine uses a unified CFG and n-gram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager.",
        "prediction": "Dr. Who is a Microsoft research project. It aims to create a speech-focused system for user interaction. This system supports the NET natural user interface. The paper talks about the design and development of MiPad. It also compares MiPad with the current pen-based PDA interface. \n\nMiPad is an app that shows great benefits for wireless PDAs. It uses continuous speech recognition (CSR) and spoken language understanding (SLU). This allows users to do many tasks using voice and touch. MiPad helps solve the issue of using small styluses or tiny keyboards on PDAs. \n\nUnlike a cellphone, MiPad does not rely only on speech. It has a microphone that turns on when a user selects a field. When a user taps the screen or rolls a built-in device, it helps understand spoken words better. \n\nMiPad works on a Windows CE Pocket PC and connects to a Windows 2000 computer for speech recognition. The Dr. Who CSR engine uses a combined CFG and n-gram language model. The Dr. Who SLU engine uses a strong chart parser and a dialog manager based on plans.",
        "reference": "Dr. Who is a Microsoft research project that aims to create a speech-focused system for interacting with technology. This system will serve as the foundation for a natural user interface called NET. The paper discusses MiPad's design and implementation, which are still in progress, as well as a preliminary user study comparing it to existing pen-based PDA interfaces. MiPad is a prototype application that shows how wireless personal digital assistants (PDAs) can benefit from integrating continuous speech recognition (CSR) and spoken language understanding (SLU) technologies. This allows users to perform common tasks using a multimodal interface and wireless technology. It aims to solve the issue of having to use small styluses or type on tiny keyboards on modern PDAs. MiPad does not rely solely on speech interaction, unlike a cellular phone. MiPad has a built-in microphone that turns on when a field is selected. When a user taps the screen or utilizes a built-in roller for navigation, this action reduces the range of potential instructions for spoken word comprehension. MiPad is currently operating on a Windows CE Pocket PC, with speech recognition being carried out on a Windows 2000 machine. The Windows CE Pocket PC is a PDA that runs the Windows Embedded Compact operating system. The Dr Who CSR engine combines a CFG and an n-gram language model. The Dr Who SLU engine uses a chart parser and a plan-based dialog manager.",
        "metrics": {
          "SARI": 54.67750088672566,
          "BLEU": 0.3013860732452062,
          "FKGL": 6.8
        }
      },
      {
        "source": "Little is known about how viable new mobile applications emerge in complex commercial environments. Research in the sociology of technology suggests that the evolution of new applications is a process of social interaction between multiple constituencies, aiming to create a common definition of mobile technology problems and solutions that is obvious only in retrospect. This theory is illustrated by the early evolution of the personal digital assistant (PDA). According to this view, established technological communities attempt to impose their own definitions of key problems and solutions on new mobile technology, thereby forcing newer players to provide a \"total system\" that completely redefines a new application area.",
        "prediction": "Not much is known about how new mobile apps come about in complex business settings. Studies in technology sociology show that new apps develop through social interaction among different groups. They work together to find a shared understanding of mobile tech problems and solutions, which is often clear only after the fact. This idea is seen in how personal digital assistants (PDAs) first grew. Established tech communities try to set their definitions of important problems and solutions for new mobile tech. This pushes newcomers to create a complete system that completely changes how we see a new app area.",
        "reference": "We don't know much about how new mobile apps succeed in complicated business environments. Research in the sociology of technology indicates that the development of new applications involves social interaction among different groups. These groups work together to establish a shared understanding of mobile technology challenges and solutions, which often becomes clear only in hindsight. The PDA's early evolution exemplifies this theory. This idea suggests that existing technology groups try to define the problems and solutions for new mobile tech, which puts pressure on new players to create a total system that redefines the application area.",
        "metrics": {
          "SARI": 47.54893569257896,
          "BLEU": 0.2436282432068827,
          "FKGL": 9.7
        }
      },
      {
        "source": "In the ubiquitous environment, mobile terminals like personal digital assistant or cellular phone are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. Extracting images and text from Web content, u-PaV generates a Macromedia Flash file, which is a popular multimedia format. Although the presentation of Web content depends on the terminal type, the Web interface of mobile terminals is designed almost the same as that of personal computers. However, comparing mobile and desktop terminals, some difference in device characteristics, such as screen resolution or sound faithfulness, affect the presentation on the terminal. Moreover, when changing the access style between interactive and passive, the presentation should be adjusted for the terminal type because the lacking information is usually not perceived after transformation. To adjust the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content by the logotype of the keyword. Third, we segment the picture of the content into pieces and present one after another using visual effects. These procedures can be applied to any type of terminal.",
        "prediction": "In our everyday world, people use mobile devices like smartphones and tablets to browse the Internet. We created a system called u-PaV that changes web content into a format easy for mobile viewing. This system takes images and text from the web and makes a Macromedia Flash file, which is a popular way to show multimedia. \n\nMobile devices and personal computers have similar web interfaces, but they differ in some ways. For example, screen size and sound quality can change how content looks and sounds. Also, when moving from interactive browsing to passive viewing, the presentation needs to fit the device type since some information may not be noticed after changes are made.\n\nTo help adjust the display of web content for different devices, we created a method for visual presentation. First, we use keyword analysis to change the background color based on the emotions of the web page. Second, we highlight important words with a special font. Third, we break up images into pieces and show them one at a time with visual effects. These steps can work on any mobile device.",
        "reference": "In the ubiquitous environment, devices like personal digital assistants or cell phones are used to access the Internet. We created a system called ubiquitous/universal passive viewer (u-PaV) that converts Web content into a format suitable for easy viewing on mobile devices. u-PaV takes text and images from the Web and creates a multimedia file in Macromedia Flash format, a widely used multimedia format. Mobile devices and personal computers have similar Web interfaces, although they may look different depending on the device. Mobile and desktop terminals have some differences in device characteristics, such as screen resolution and sound quality, which can affect how content is presented on the device. When switching between interactive and passive access styles, the presentation needs to be adjusted for the device type because some information may be lost during transformation and not noticed by the user. We developed a visual presentation method to customize how Web content is displayed based on the device type. First, we analyze keywords to capture the emotional tone of a web page and adjust the screen's background color accordingly. Then, we use the keyword's logo to highlight the main subject of the content. Finally, we divide the content's image into sections and present them one by one using visual effects. These steps can be used on any type of device.",
        "metrics": {
          "SARI": 43.53530703736211,
          "BLEU": 0.17686038186508526,
          "FKGL": 8.6
        }
      },
      {
        "source": "We have developed two-way location aware system, named as “The Beijing Explorer”, which exchanged positioning information and users’ situation to one another using a PDA (Personal Digital Assistant) with built-in wireless LAN and a GPS (Global Positioning System) receiver in real time. Users can see their position and their chats on the screen of a PDA using the system real-timely. The system was used for the guidance of the Palace Museum (Forbidden City) in Beijing, China. We carried on experiments two times using the system. The results of experiments show that the service using positioning data and sharing contents were valuable and interesting. The correct positioning information is important for the guidance system.",
        "prediction": "We created a two-way location-aware system called “The Beijing Explorer.” It shares position data and user information in real time using a PDA with wireless LAN and GPS. Users can view their location and chats on the PDA. The system was tested for guiding visitors at the Palace Museum in Beijing, China. We conducted two experiments with the system. The results showed that using position data and sharing content was valuable and interesting. Accurate positioning is crucial for the guidance system.",
        "reference": "We created a location-aware system called \"The Beijing Explorer\" that uses a PDA with wireless LAN and GPS to exchange positioning and user information in real-time. Users can view their position and chats in real-time on the PDA screen when using the system. The system was used to guide visitors at the Palace Museum (Forbidden City) in Beijing, China. We conducted two experiments using the system. The experiment results indicate that the service was useful and engaging because it shared positioning data and content. Accurate positioning information is crucial for the guidance system.",
        "metrics": {
          "SARI": 60.66639962825193,
          "BLEU": 0.37917760189833544,
          "FKGL": 9
        }
      },
      {
        "source": "A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. It is personal and private in an unprecedented manner, and its modus operandi is such that network and power failures will not be rare. Designing distributed systems where PDAs are treated as \"first class citizens\" is a  private :[59],\"assets (electronic money, keys for authentication and opening doors) will be stored in PDAs. Ownership and control of these :[59],\"assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private  :[105],\"introduce the \"open-ended argument\" to describe the design strategy we used for designing a system that is designed to reveal information to the user (as opposed to hide it). We argue and show that when systems are designed this way, the user (a human) is better able to control the system and his personal data, as he can make better decisions than the system itself based on qualitative assessment of the provided information. The system we have designed and implemented under this design guidelines is presented and discussed.",
        "prediction": "A private machine, like a Personal Digital Assistant (PDA), is very different from regular machines. It is personal and private in a new way, and it will often face network and power issues. We want to design systems where PDAs are treated as important tools. Important items, like electronic money and keys, will be stored in PDAs. Users should own and control these items and their storage. This should be clear in system design. \n\nWe used an \"open-ended argument\" to create a system that shows information to users instead of hiding it. We believe that when systems are built this way, users can control their data better. They can make smarter choices than the system based on the information it gives them. We will show and discuss the system we created based on these design ideas.",
        "reference": "A Personal Digital Assistant (PDA) used as a private machine, is fundamentally distinct from traditional machines because it is a personalized device. It offers an unprecedented level of personal and private experience, and its operation is designed to handle network and power failures, which are expected to occur frequently. In designing distributed systems, PDAs are given high importance and treated as \"first class citizens\". This means that important assets like electronic money and authentication keys will be stored in PDAs. The user should retain ownership and control of these assets and the media that stores and communicates them. This principle should be incorporated into the design of private systems. We introduced the concept of the \"open-ended argument\" to describe our design strategy, which focuses on creating systems that are intended to provide information to the user rather than hiding it. We argue and demonstrate that when systems are designed in this manner, the user (a human) gains improved control over both the system and their personal data. By enabling users to make decisions based on qualitative assessment of the information provided, they are better equipped to make informed choices compared to relying solely on the system's automated decisions. We have created a system based on the design guidelines and we will explain it.",
        "metrics": {
          "SARI": 44.069968913036135,
          "BLEU": 0.13363627233309602,
          "FKGL": 6.2
        }
      },
      {
        "source": "Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the development and initial evaluation of a personal digital assistant (PDA)-based decision support system (DSS) for pediatric depression screening in ages 8 to 18 years of age by pediatric advanced practice nurse (APN) students. Three aspects are described: selection of depression screening instrument; integration of the instrument into the PDA; and quantitative (usage) and qualitative (focus group) evaluation. Only one third of eligible patients were screened. Twenty percent of those screened were identified as at risk for mood disorder. The barriers to screening identified through focus groups included a lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, as well as perceived or real cultural barriers. Suggestions for educational, research, and interventions to integrate clinical based PDA-based screening are discussed.",
        "prediction": "Depression is not often recognized in kids. This paper talks about a new tool using a personal digital assistant (PDA) to help check for depression in kids ages 8 to 18. It covers three main points: choosing a way to screen for depression, putting that method into the PDA, and checking how well it works through usage and focus groups. Only one third of eligible kids were screened for depression. Of those screened, 20 percent were found at risk for mood disorders. Focus groups pointed out obstacles to screening, such as lack of time, knowledge, protocols for intervention, referral resources, problems with PDA use, and comfort level of mentors. The paper also suggests ways to improve education, research, and practices for using PDA screening in clinical settings.",
        "reference": "Depression often goes unnoticed in many pediatric settings. This paper aims to explain the creation and early assessment of a decision support system (DSS) that uses personal digital assistants (PDAs) for screening pediatric depression among children aged 8 to 18 years, carried out by pediatric advanced practice nurse (APN) students. Three aspects are discussed: choosing a depression screening tool, incorporating the tool into the PDA, and evaluating it through quantitative (usage) and qualitative (focus group) assessments. Only one third of eligible patients were screened. 20% of the screened patients were found to be at risk for a mood disorder. Barriers to screening were identified through focus groups, including lack of time, knowledge, intervention protocol, referral resources, PDA usability issues, preceptor motivation and comfort, and cultural barriers. The paper discusses suggestions for integrating PDA-based screening into clinical practice through education, research, and interventions.",
        "metrics": {
          "SARI": 37.8181712789863,
          "BLEU": 0.1729992588943134,
          "FKGL": 9.2
        }
      },
      {
        "source": "This study explored the potential of the application of wireless and mobile computing technologies to be used in improving the efficiency of patient care and education and future developments in information and communication technologies to support healthcare professionals and medical students in healthcare research, medical education and training. The design used for this study was a systematic review of published materials obtained from EMBASE and MEDLINE online databases, and the Cochrane Library database, including personal observations. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistant with expected growth of more than 75% by year-end 2007. In addition, wireless and mobile computing technologies allows Personal Digital Assistant to connect directly to networks or the Internet. Studies relating to processes of patient care and should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies is only beginning to take its first step in improving patient care and education. They have shown a positive impact on patient safety, health care efficiency, and ultimately patient satisfaction.",
        "prediction": "This study looked at how wireless and mobile technologies can help improve patient care and education. It also focuses on how these tools can support healthcare workers and medical students in research and training. The study reviewed published materials from online databases like EMBASE, MEDLINE, and the Cochrane Library, along with personal observations. Over 50% of healthcare workers and medical students now use Personal Digital Assistants, and this is expected to rise to over 75% by the end of 2007. Wireless and mobile technologies allow these devices to connect to networks or the Internet. Studies on patient care show that mobile technologies can save time. Wireless and mobile technologies are just starting to improve patient care and education. They have been shown to positively affect patient safety, healthcare efficiency, and patient satisfaction.",
        "reference": "This study examined how wireless and mobile computing technologies can enhance patient care and education. It also explored how advancements in information and communication technologies can support healthcare professionals and medical students in research, education, and training. We conducted a systematic review for this study, analyzing published materials from databases such as EMBASE, MEDLINE, and the Cochrane Library, as well as personal observations. Over 50% of healthcare professionals and medical students currently use Personal Digital Assistants, and this number is expected to grow by more than 75% by the end of 2007. Wireless and mobile computing technologies enable Personal Digital Assistants to connect directly to networks or the Internet. Studies should examine whether mobile computing technologies can save time in patient care processes. Wireless and mobile computing technologies are still in the early stages of improving patient care and education. These technologies have positively impacted patient safety, healthcare efficiency, and patient satisfaction.",
        "metrics": {
          "SARI": 60.12690046432188,
          "BLEU": 0.41920243450885786,
          "FKGL": 10.9
        }
      },
      {
        "source": "Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore practical training is an effective learning opportunity for developing the practical nursing care ability of nursing students. Moreover, at hospitals, which are important training facilities, with regard to medical safety, the use of learning tools that produce electrical waves is not possible. So, we created a learning support environment that facilitates the imagination of nursing techniques, and enables effective preparation, review, and learning at anytime and anywhere using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named \"digital nursing dictionary\" that we developed and the evaluation of the practices using it.",
        "prediction": "Nursing students learn a lot from practical training. This training lets them see real medical practice and work with patients. Practical training helps students build their nursing skills. Hospitals are key places for this training. However, for safety reasons, some learning tools that use electrical waves cannot be used there. To help, we created a learning support system. This system uses a portable digital assistant (PDA) to help students learn nursing techniques anywhere, anytime. This paper talks about the \"digital nursing dictionary\" we made and how we tested its use in training.",
        "reference": "Nursing students can learn through practical training and by interacting with patients in real medical settings. Practical training is effective for developing the practical nursing care ability of nursing students. Moreover, hospitals, which are important training facilities, do not allow the use of learning tools that produce electrical waves due to medical safety concerns. We developed a learning support environment that helps nursing students visualize and practice techniques. It allows for convenient preparation, review, and learning using a portable digital assistant (PDA) device, enabling anytime and anywhere practical training. We developed an educational tool called the 'digital nursing dictionary' and evaluated its effectiveness, as outlined in this paper.",
        "metrics": {
          "SARI": 35.46101672101935,
          "BLEU": 0.11523432382585053,
          "FKGL": 7.8
        }
      },
      {
        "source": "Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. Manual reconstruction of web pages for these devices would ease the problem; however, the large variation of display capabilities will greatly increase the burden of web page designers since they have to customize a web page for each possible display device. In this paper, we propose a document segmentation and presentation system. The system automatically divides a web document into a number of logical segments based on the screen size and the structure and content of the document. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.",
        "prediction": "Mobile devices have small screens and low resolution. This makes it hard to use web applications. Web pages can be too big for devices like PDAs and WebTV. Changing web pages for each device is difficult for designers. In this paper, we suggest a new system. This system splits a web document into smaller sections based on screen size and its content. It also provides extra information like summaries to help users navigate. The system shows these sections clearly to use the screen effectively for finding information.",
        "reference": "Small screens and low resolutions on mobile devices are a major obstacle for the widespread adoption of web applications in a wireless environment. PDA screens are often too small to display entire web pages. Low-resolution devices like WebTV face the same problem. Manually reconstructing web pages for these devices would help, but the wide range of display capabilities would make it challenging for web page designers as they would have to customize each web page for every possible display device, increasing their workload. This paper suggests a system for segmenting and presenting documents. The system divides a web document into logical segments automatically, using information about the screen size, document structure, and content. The system also extracts summaries and overviews to make navigation easier. The system optimizes the use of the screen for information finding by showing the segments and structure of a web document.",
        "metrics": {
          "SARI": 37.876617348446764,
          "BLEU": 0.045812076283263656,
          "FKGL": 6.4
        }
      },
      {
        "source": "Take 8-bit microcontroller (C8051F005) as the nucleus and study a kind of voltage harmonic monitor device that satisfies the long-range monitor. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). This device has two work modes: ”native” and ”remote”. It also can communicate with monitor center through telephone line, serial port, IC card, PDA (Personal Digital Assistant), etc.",
        "prediction": "Use the 8-bit microcontroller C8051F005 to create a voltage harmonic monitor for long-distance checking. The device uses the Fast Fourier Transform algorithm for harmonic analysis. It has two operating modes: native and remote. It can communicate with a monitor center using a telephone line, serial port, IC card, or PDA.",
        "reference": "Study a voltage harmonic monitor device for long-range monitoring using the 8-bit microcontroller C8051F005 as the core. The harmonic analysis uses FFT. The device has two modes: \"native\" and \"remote\". It can communicate with a monitoring center via different methods such as telephone line, serial port, IC card, and PDA (Personal Digital Assistant).",
        "metrics": {
          "SARI": 58.371579460705846,
          "BLEU": 0.336612104988354,
          "FKGL": 9.3
        }
      },
      {
        "source": "With network and small screen device improvements, such as wireless abilities, increased memory and CPU speeds, users are no longer limited by location when accessing on-line information. We are interested in studying the effect of users switching from a large screen device, such as a desktop or laptop to use the same web page on a small device, in this case a PDA (Personal Digital Assistant). We discuss three common transformation approaches for display of web pages on the small screen: Direct Migration, Linear and Overview. We introduce a new Overview method, called the Gateway, for use on the small screen that exploits a user’s familiarity of a web page. The users in an initial study prefer using the Gateway and Direct Migration approach for web pages previously used on the large screen, despite the common Linear approach used by many web sites.",
        "prediction": "With better networks and small screen devices, like wireless connections and faster memory and CPU speeds, users can access online information from anywhere. We want to study how users feel when they switch from big screens, like desktops or laptops, to small devices, like PDAs. We will look at three ways to show web pages on small screens: Direct Migration, Linear, and Overview. We will introduce a new Overview method called the Gateway that helps users who know a web page. In an early study, users preferred the Gateway and Direct Migration methods for web pages they had used on big screens, even though many websites use the Linear method.",
        "reference": "Improvements in network and small-screen devices, such as wireless capabilities, faster CPU speeds, and increased memory, have eliminated location restrictions for accessing online information. We want to study the impact of users switching from a desktop or laptop to a PDA (Personal Digital Assistant) when accessing the same web page. We are looking at three common ways to display web pages on small screens: Direct Migration, Linear, and Overview. Gateway is our new Overview method for small screens that use the user's familiarity with a web page. Many websites use the Linear method, but an initial study showed that users prefer using the Gateway and the Direct Migration method for web pages.",
        "metrics": {
          "SARI": 47.308130619330946,
          "BLEU": 0.257748501650617,
          "FKGL": 8.3
        }
      },
      {
        "source": "The limitations and constraints of mobile systems need to be adequately addressed in software development. We have been developing a taxonomy of risks based on SEIu0027s risk questionnaire and applied it during the development of a negotiation support system for a Personal Digital Assistant (PDA). In our planned research, we will explore how we can better integrate existing risk management strategies and Agile Methods (AM).",
        "prediction": "Mobile systems have limits that must be considered in software development. We created a list of risks using SEI's risk questionnaire. We used this list while developing a negotiation support system for a Personal Digital Assistant (PDA). In our research, we will look at how to improve the use of risk management strategies with Agile Methods (AM)",
        "reference": "During software development, restrictions due to mobile systems need to be addressed. Based on SEI's risk questionnaire, we developed a classification of risks and applied it while developing a negotiation support system for a PDA. Risk management strategies and Agile Methods and how we can integrate them better will be studied in our future research.",
        "metrics": {
          "SARI": 42.216423575644804,
          "BLEU": 0.17894202876520773,
          "FKGL": 8.9
        }
      },
      {
        "source": "Expansions of mobile services and private data have required increased level of protection. Speaker recognition, one of the biometric technologies, arises lots of research interests for its simple, cheap and convenient characteristics. In this paper, a robust speaker recognition system which facilitates reliable authentication with multi-channel voices is presented. A large multi-channel corpus, including mobile phone, personal digital assistant (PDA), telephone and microphone, is collected to evaluate the system performance.",
        "prediction": "Mobile services and private data need better protection. Speaker recognition is a biometric technology that is cheap and easy to use. This paper shows a strong speaker recognition system for reliable authentication with voices from different sources. We collected a large dataset of voices from mobile phones, PDAs, telephones, and microphones to test how well the system works.",
        "reference": "The growth of mobile services and private data necessitates higher levels of protection. Speaker recognition, a technology using the voice of users to identify them, leads to a lot of research interest because of its simple, cheap, and convenient characteristics. This work presents a powerful speaker recognition system that makes the authentification with multi-channel voices easier. Data are collected on mobile phones, PDAs, telephones, and microphones to evaluate the system's performance.",
        "metrics": {
          "SARI": 51.64934193655929,
          "BLEU": 0.20394797042455703,
          "FKGL": 8.9
        }
      },
      {
        "source": "This paper presents a method to generate unique and nevertheless highly random pseudonyms in a distributed environment. More precisely, each user can now generate his pseudonym locally in his personal security environment, e.g. in his smart card or his personal digital assistant. There is no need for any information interchange between issuing parties or global data (especially keys), except unique identifiers for each user and each device of the system. Additionally the holder can prove, that he generated a specific pseudonym without revealing his identity and he can reveal his identity by disclosing the pseudonym. Whereas the verifier of a disclosed pseudonym can be sure, that the presenter of the pseudonym is the holder of the pseudonym (i.e. the person which originally generated it). The identifier of the user and the identifier of the useru0027s device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form.",
        "prediction": "This paper shows a way to create unique and random pseudonyms. Users can create their pseudonyms locally on their own devices like smart cards or digital assistants. There is no need to share information between different parties or have global data, except for unique identifiers for each user and device. Users can prove they made a specific pseudonym without showing their identity. They can also reveal their identity by sharing the pseudonym. The verifier of a pseudonym can trust that the person showing it is the original creator. User identifiers and device identifiers will help generate unique pseudonyms. To keep users anonymous, both identifiers will be stored in an encrypted form within the pseudonym.",
        "reference": "This work presents a method to generate unique and random pseudonyms in a distributed system. Every user can now create his pseudonym locally in their own personal security environment, e.g. in his chip card or PDA. No information exchange is required between issuing parties or global data, except for unique identifiers for each user and device in the system. The holder can also prove that he generated a distinct pseudonym without revealing his identity; by divulging it, he can reveal his identity. In that case, the verifier of a divulgated pseudonym can be sure that the one who presents the pseudonym is holding it. (i.e. the person that originally created it) The user identifier and device's identifier will be used to create unique pseudonyms but to ensure anonymity with pseudonyms, both identifiers will be stored within the pseudonym in encrypted form.",
        "metrics": {
          "SARI": 38.07283804980322,
          "BLEU": 0.17996323507027248,
          "FKGL": 8.9
        }
      },
      {
        "source": "In todayu0027s mobile information society, location-based services play an increasingly important role. These services are to be accessed by users with a mobile end device for the use of city maps, route planning, navigation, or traffic information. Mobile end devices, however, do not have computing power or storage capacity comparable to that of a personal computer or laptop. These deficits can be bypassed by employing special methods in development of the respective applications. This paper outlines the experience gained in the development of a prototype for route computation in public transport networks to be used on a personal digital assistant. Subdivision of the route computation process into various steps for minimizing the computation expenditure on the mobile end device plays a significant role.",
        "prediction": "In today's mobile information society, location-based services are very important. Users access these services from mobile devices for city maps, route planning, navigation, and traffic information. Mobile devices do not have the same computing power or storage as personal computers or laptops. We can solve this problem using special methods in application development. This paper shares experiences from creating a prototype for route calculation in public transport networks. We divide the route calculation into steps to reduce the work needed on the mobile device.",
        "reference": "Location-based service's role in today's mobile information society is increasing . Users can access city maps, route planning, navigation, or traffic information with their mobile devices. Compared to laptops or personal computers, mobile devices do not have the same computing power or storage capacity. This shortage can be overcome by using specific methods during the development of the respective application. This paper shows the gained experience from developing a prototype designed for a PDA that calculates routes in public transport networks. Dividing the route computation process into multiple steps to reduce the computational load on the mobile device is crucial.",
        "metrics": {
          "SARI": 57.190091957432585,
          "BLEU": 0.291198503846174,
          "FKGL": 11.1
        }
      },
      {
        "source": "DIANE is a digital assistant system that aims to fasten the doctor access to various informations at the hospital such as health care facility, medical records, and also human resource data. The fasten access could be achieved by implementing face recognition and live streaming as part of the digital assistant system.",
        "prediction": "DIANE is a digital assistant system designed to help doctors quickly access important information in hospitals. This includes health care facilities, medical records, and staff data. It achieves fast access by using face recognition and live streaming technology.",
        "reference": "DIANE is a digital assistant system that allows the doctor a faster access patient and hospital information. This could be done with face recognition and live streaming.",
        "metrics": {
          "SARI": 54.57720982720983,
          "BLEU": 0.21170876705481304,
          "FKGL": 10.6
        }
      },
      {
        "source": "Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time.",
        "prediction": "Digital assistants are becoming more common in our lives. Users can do many tasks quickly with these assistants.",
        "reference": "Digital assistants are becoming popular in our daily lives. It will allow users to do multiple tasks in a faster way.",
        "metrics": {
          "SARI": 46.00210324196126,
          "BLEU": 0.15094813209726435,
          "FKGL": 6.8
        }
      },
      {
        "source": "As extensive experimental research has shown individuals suffer from diverse biases in decision-making. In our paper we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, managerial decisions which are based on different levels of organizational complexity and different incentive systems suffer from biases known from descriptive decision theory. The results illustrate how biases in combination with each other and in different organizational contexts affect organizational performance. We find that, contrary to intuition, some combinations of biases significantly improve organizational performance while these biases negatively affect organizational performance when they occur separately. This might evoke considerations whether decision-making should be as rational as possible.",
        "prediction": "Research shows that people face many biases when making decisions. In our study, we look at how these biases affect managers when they make decisions together and how this impacts the organization. Our simulations show that when managers decide based on the complexity of their organization and the rewards they get, they still experience biases. The results show that some biases can work together to improve performance, even though they can harm performance on their own. This raises questions about whether decision-making should aim to be as rational as possible.",
        "reference": "Research showed that people suffer from diverse biases (disproportionate weight in favor of or against an idea) in decision-making. We analyzed the effects of decision-making biases of managers on organizational performance. In the simulations, managers decisions which are based on organizational skills and rules, suffer from several known biases. The results illustrate how combined biases in different contexts affect organizational performance. Contrary to expectations, some combinations of biases improve organizational performance. However, when they occur separately, they decrease performance. This raises questions about the importance of rationality in decision-making.",
        "metrics": {
          "SARI": 33.60733978800727,
          "BLEU": 0.09342545269678061,
          "FKGL": 10.3
        }
      },
      {
        "source": "Big data, the enhanced ability to collect, store and analyze previously unimaginable quantities of data in tremendous speed and with negligible costs, delivers immense benefits in marketing efficiency, healthcare, environmental protection, national security and more. The central tenets of the current privacy framework, the principles of data minimization and purpose limitation, are severely strained by the big data technological and business reality. As we increasingly interact with these artificial agents in unsupervised settings, with no human mediators, their seeming autonomy and increasingly sophisticated functionality and behavior, raises legal and philosophical questions. The focus on the machine is a distraction from the debate surrounding data driven ethical dilemmas, such as privacy, fairness and discrimination. The machine may exacerbate, enable, or simply draw attention to the ethical challenges, but it is humans who must be held accountable. Policymakers should seek to devise agreed-upon guidelines for ethical data analysis and profiling. Such guidelines would address the use of legal and technical mechanisms to obfuscate data; criteria for calling out unethical, if not illegal, behavior; categories of privacy and non-privacy harms; and strategies for empowering individuals through access to data in intelligible form.",
        "prediction": "Big data means collecting, storing, and analyzing a huge amount of data very quickly and cheaply. It helps many areas like marketing, healthcare, the environment, and national security. However, big data challenges the rules about privacy, especially the ideas of using less data and limiting its use. As we use more artificial agents without human help, we face legal and ethical issues. Focusing too much on machines distracts us from important problems like privacy, fairness, and discrimination. Machines can make these issues worse, but humans are the ones responsible. Policymakers need to create clear rules for ethical data use. These rules should cover how to hide data, how to identify bad behavior, the types of privacy harms, and how to help people understand and access their data.",
        "reference": "Big data (the ability to collect, store and analyze huge quantities of data in a really fast and cheap way) offers immense benefits in marketing, healthcare, environment, national security and more. The principles of privacy, limitation of data collection and use are stretched by the business and technological reality of big data. Our increasing interactions with these technologies without human supervision, and their growing autonomy, functionality and behavior raises legal and philosophical questions. The focus on the machine is a distraction from the debate on ethical dilemmas about data like privacy, fairness and discrimination. The machine may influence the ethical challenges, but the humans must remain responsible. Responsible people should find guidelines for ethical data analysis and collection. These guidelines would deal with mechanisms to protect data, criteria about unethical or illegal behavior, privacy harms, and strategies to give people access to data.",
        "metrics": {
          "SARI": 37.32022714953307,
          "BLEU": 0.09561071737429332,
          "FKGL": 10.7
        }
      },
      {
        "source": "Amazon's Echo, and Apple's Siri have drawn attention from different user groups; however, these existing commercial VUIs support limited language options for users including native English speakers and non-native English speakers. we conducted a usability study of the Google Home Smart Speaker with 20 participants including native English and non-native English speakers to understand their differences in using the Google Home Smart Speaker. The findings show that compared with their counterparts, the native English speakers had better and more positive user experiences in interacting with the device. It also shows that users' English language proficiency plays an important role in interacting with VUIs. The findings from this study can create insights for VUI designers and developers for implementing multiple language options and better voice recognition algorithms in VUIs for different user groups across the world.",
        "prediction": "Amazon's Echo and Apple's Siri are popular voice user interfaces (VUIs). However, they have limited language options for users, both native and non-native English speakers. We studied the Google Home Smart Speaker with 20 participants, including native and non-native English speakers. Our findings showed that native English speakers had a better and more positive experience using the device. We also found that a user's English skills are important for interacting with VUIs. This study can help VUI designers and developers create more language options and improve voice recognition for different users around the world.",
        "reference": "The VUIs (voice user interface) like Amazon’s Echo or Apple’s Siri are popular nowadays. However, they have limited language options for users. We studied the usability of the Google Home Smart Speaker with native and non-native English speakers to understand their differences in using this device. The results show that the native English speakers had a better experience with the device than the non-native. It also shows that language skills play an important role. The results of this study can help VUI developers for improving language options and voice recognition in the devices.",
        "metrics": {
          "SARI": 51.82982784493009,
          "BLEU": 0.30840303173756395,
          "FKGL": 10.6
        }
      },
      {
        "source": "the smart speakers cannot distinguish human voice from machine voice. a method to identify which of human or machine is sending voice commands to a smart speaker is desired. to prevent such machine-voice based attacks to a smart speaker in absence of residents, we propose a system consisting of a speaker and a microphone array to detect the existence of a human nearby, supposing it can be incorporated in a smart speaker in the future.",
        "prediction": "Smart speakers cannot tell the difference between human voices and machine voices. We need a way to know if a voice command comes from a human or a machine. To stop machine voice attacks when no one is home, we suggest a system with a speaker and a microphone array. This system can check if a human is close by. We hope it can be added to smart speakers in the future.",
        "reference": "The smart speakers cannot differentiate between human voice and machine voice. A method to identify if the voice command is from a human or a machine is required. We propose a system composed of a speaker and microphones to detect the presence of humans. It could help to prevent attacks on a smart speaker with a machine voice in absence of residents",
        "metrics": {
          "SARI": 39.256088796830426,
          "BLEU": 0.1277566936460511,
          "FKGL": 5.4
        }
      },
      {
        "source": "As smart speakers with voice interaction capability permeate continuously in the world, more and more people will gradually get used to the new interaction medium–voice. Although speech recognition, natural language processing (NLP) have been greatly improved over the past few years, users still may encounter errors from time to time like “cannot understand”, “no requested audio resource (such as music)”, which can frustrate users. Therefore, when an error message is reported, it is vital that the smart speaker gives an effective and proper response. However, currently the response strategies adopted by leading smart speaker brands in China differed mainly on two dimensions: “apology or not” and “humor or neutral”. We explored user’s preference of response strategies under two error scenarios——“cannot understand” and “no requested audio resource”. Two dependent variables (satisfaction and perceived sincerity of response) were measured. The results showed that participants were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no significant impact on the perception of satisfaction and sincerity. But in the “cannot understand” scenario, humorous expression decreased perceived sincerity.",
        "prediction": "Smart speakers that use voice are becoming common. More people are getting used to talking to devices. Even though speech recognition and natural language processing have improved, users may still see errors like \"cannot understand\" or \"no requested audio resource.\" These errors can be frustrating. When an error happens, it is important for the smart speaker to respond well. \n\nTop smart speaker brands in China have different response strategies. They mainly differ in two ways: whether they apologize or not, and if they use humor or stay neutral. We looked at what users prefer in two error situations - \"cannot understand\" and \"no requested audio resource.\" We measured satisfaction and perceived sincerity of the responses.\n\nThe results showed that users felt more satisfied and saw higher sincerity when the smart speaker apologized in both error situations. In the \"no requested audio resource\" case, humor did not affect satisfaction or sincerity. However, in the \"cannot understand\" case, using humor made the speaker seem less sincere.",
        "reference": "Smart speakers are becoming popular around the world and people will get used to this new way of interacting with the voice. Even with the progress of speech recognition and natural language processing (interactions between computers and human language), users may still get errors like “cannot understand” or “no requested audio resource” which can frustrate users. So, it is important that the smart speaker gives an effective and proper response during an error message. The responses of the most popular brand of smart speakers are based on 2 elements, apology and humor. We studied user’s preference in two error scenarios : “cannot understand” and “no requested resource” Satisfaction of the user and perceived sincerity of the response were measured. The results showed that users were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no impact on the satisfaction or sincerity. But in the “cannot understand” scenario, humor decreased perceived sincerity.",
        "metrics": {
          "SARI": 42.01966157010307,
          "BLEU": 0.1874319311076099,
          "FKGL": 8.6
        }
      },
      {
        "source": "A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process (specifically, drug molecules, compounds in high throughput screening libraries, combinatorial chemistry building blocks and nondrug molecules). Significant differences were observed between the pharmacophore profiles obtained for the drug molecules and those obtained for the high-throughput screening compounds, which appear to be closely related to the nondrug pharmacophore distribution. It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the property-based optimization of compound selection and library design processes, thus improving the odds of success in lead discovery projects.",
        "prediction": "A pharmacophore analysis was used to study different types of compounds important for drug discovery. This includes drug molecules, compounds in high throughput screening, combinatorial chemistry building blocks, and nondrug molecules. There were clear differences between the pharmacophore profiles of drug molecules and those from high-throughput screening. These differences seem linked to the distribution of nondrug pharmacophores. The analysis of pharmacophore profiles could help improve how we choose and design compounds. This could increase the chances of success in lead discovery projects.",
        "reference": "A pharmacophore analysis investigated and compared different compounds of the drug discovery process. Significant differences were observed between the pharmacophore profiles of the drug molecules and the high-throughput screening compounds, which appear to be related to the nondrug pharmacophore distribution. It is suggested that the analysis could be used as an additional tool for the optimization of compound selection and library design processes..",
        "metrics": {
          "SARI": 25.876833511698056,
          "BLEU": 0.09157037449663791,
          "FKGL": 9.8
        }
      },
      {
        "source": "Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been previously designed for this express purpose, and provides a layered hierarchy of meaningful terms which can be linked to.",
        "prediction": "Annotating bioassay protocols with semantic web vocabulary helps make experiment descriptions readable by machines. Protocols are written in clear scientific English, which limits software analysis. With a good ontology, key information can be captured as facts shown in semantic web triples (subject, predicate, object). This annotation allows assays to be searched, grouped, tagged, and assessed in many ways, similar to other parts of drug discovery informatics. The BioAssay Ontology (BAO) was created for this purpose and offers a structure of useful terms that can be linked.",
        "reference": "Annotation of bioassay protocols with web vocabulary can make experiment descriptions machine-readable. Protocols use concise scientific English which prevents most of analysis by software algorithms. There is a sufficient ontology, so the pertinent information can be written using semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated. The BioAssay Ontology (BAO) has been designed for this purpose, and provides many meaningful terms.",
        "metrics": {
          "SARI": 42.655568482208835,
          "BLEU": 0.2604647076870416,
          "FKGL": 12.4
        }
      },
      {
        "source": "Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Subsequent bioinformatic analysis is typically done with the help of open source tools, where each application performs a single step towards the final result. This situation leaves the bioinformaticians with the tasks to combine the tools, manage the data files and meta-information, document the analysis, and ensure reproducibility.",
        "prediction": "Next generation sequencing NGS creates large datasets with billions of reads and thousands of samples. Bioinformatic analysis usually uses open source tools, where each tool does one step of the work. This means bioinformaticians must combine the tools, manage data files, keep track of meta-information, document their work, and ensure results can be repeated.",
        "reference": "Next generation sequencing (NGS) produces datasets of billions of reads and thousands of samples. The analysis is typically done with open source tools, performing a single step towards the final result. The bioinformaticians have to combine the tools, manage the files and information, document the analysis, and ensure reproducibility.",
        "metrics": {
          "SARI": 36.72537065400592,
          "BLEU": 0.2699859496569013,
          "FKGL": 10.3
        }
      },
      {
        "source": "SPSS Clementinel2.0 statistical software was used to mine the association rules between Etiology and traditional Chinese medicine (TCM), Syndromes and TCM, Symptoms and TCM. The classic Apriori algorithm is useful to mine cases of influenza treated by contemporary famous old Chinese medicine.",
        "prediction": "SPSS Clementine 2.0 is a statistical tool used to find rules between causes, traditional Chinese medicine (TCM), syndromes, and symptoms. The Apriori algorithm helps to study influenza cases treated by well-known traditional Chinese medicine.",
        "reference": "SPSS Clementinel2.0 statistical software was used to extract data and find relations between traditional Chinese medicine and other medicine. The classic Apriori algorithm is useful to find cases of influenza treated by old Chinese medicine.",
        "metrics": {
          "SARI": 31.555342769574384,
          "BLEU": 0,
          "FKGL": 8.9
        }
      },
      {
        "source": "Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype and phenotype in humans and other species. a Bayesian framework for genotype estimation for mixtures of multiple bacteria, named as Genetic Polymorphisms Assignments (GPA) has reduced the false discovery rate (FDR) and mean absolute error (MAE) in single nucleotide variant (SNV) identification.",
        "prediction": "Finding antimicrobial resistant (AMR) bacteria in metagenomics samples is very important for public health and food safety. Next-generation sequencing (NGS) technology helps us identify genetic differences and understand how genes relate to traits in humans and other species. A method called Genetic Polymorphisms Assignments (GPA) uses a Bayesian approach to better estimate genotypes in mixed bacteria. This method reduces false discoveries and errors in identifying single nucleotide variants (SNVs)",
        "reference": "Identifying bacterias that are resistant to antimicrobial in samples of genetic material collected from the environment is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype (genetic code of an individual) and phenotype (physical characteristics of an indvidual) in humans and other species. The Genetic Polymorphism Assignements (GPA) is a framework which determines the genetic constitution of multiple bacteria while reducing the number of false positive results and improving the accuracy of calculations when identifying the variation of a single nucleotide.",
        "metrics": {
          "SARI": 40.46390148831115,
          "BLEU": 0.18333601796503587,
          "FKGL": 13.6
        }
      },
      {
        "source": "The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (altering the genetic sequence) and gene regulation (without altering the genetic sequence). These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "prediction": "The CRISPR/Cas9 system is a new tool for gene editing and controlling genes. It allows changes to the genetic code and can also regulate genes without changing the code. To use this system, we need to design special molecules called single guide RNAs (sgRNAs). These sgRNAs must be efficient and specific. However, designing them is difficult because we have to think about many factors.",
        "reference": "The CRISPR/Cas9 system is a powerful technology for gene editing and gene regulation. These applications require the design of single guide RNAs (which are enzymes that can delete, insert or alter DNA) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.",
        "metrics": {
          "SARI": 36.95747827155413,
          "BLEU": 0.21923758090853152,
          "FKGL": 7.1
        }
      },
      {
        "source": "The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals; it might improve the health of individuals and communities, and, if successful, might be seen as a public good. However, enhancing future generations will require In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. Remarkably, the necessary involvement of women in an enhancing scenario has not been discussed by its proponents. The present discourse on moral obligations of future generations, although not referring to women, seems to imply that women might be required, morally, if not legally, to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. Despite the lack of a unified feminist conclusion in the discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.",
        "prediction": "Gene editing might help solve problems in health care. It could make future generations healthier. This aligns with public health goals and could benefit individuals and communities. If it works, it could be seen as a good thing for society. To enhance future generations, we may need In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. However, the role of women in this process has not been discussed by those who support it. The current talks about moral duties to future generations do not mention women specifically, but they suggest that women might need to use IVF to have children. Enhancing future generations will involve women, unless we develop artificial wombs. These are tough issues that need to consider both women and men. There is a need to clearly define women's roles in discussions about gene editing, even though there is no single feminist view on the pros and cons of changing the human genome.",
        "reference": "Gene editing could be used to solve issues related to health care allocation. Improving the health of future generations might coincide with public health goals. However, enhancing future generations will require In Vitro Fertilisation (IVF) and Pre-implantation Genetic Diagnosis. The necessary involvement of women in an enhancing scenario has not been discussed. The discourse on moral obligations of future generations seems to imply that women might be required to reproduce with IVF. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. There are no unified feminist conclusion about the merits and risks of human genome modification. There is also an urgent need to clarify the role of women in this scenario.",
        "metrics": {
          "SARI": 28.94914176804239,
          "BLEU": 0.19303938736572873,
          "FKGL": 9.3
        }
      },
      {
        "source": "CRISPR-Cas is a tool that is widely used for gene editing. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins, which are powerful molecules that inhibit the CRISPR-Cas system, may have the potential to promote better utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. Additionally, more in-depth research on these proteins would help researchers to better understand the co-evolution of bacteria and phages. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "prediction": "CRISPR-Cas is a popular tool for gene editing. But it can cause unexpected effects over time. Anti-CRISPR proteins block the CRISPR-Cas system. They may help improve its use in gene editing and gene therapy. More research on these proteins can help scientists learn about how bacteria and phages evolved together. It is important to gather and combine data on different types of anti-CRISPRs.",
        "reference": "CRISPR-Cas is a tool used for gene editing. However, unintended genetic modifications may occur. Anti-CRISPR proteins (molecules that inhibit the CRISPR-Cas system) may improve utilization of the CRISPR-Cas system in gene editing, especially for gene therapy. More research on these proteins would help to understand the co-evolution of bacteria and bacteriophages (virus that infects and replicates within bacteria). It is necessary to collect and integrate data on various types of anti-CRISPRs.",
        "metrics": {
          "SARI": 36.798923915982265,
          "BLEU": 0.2236283534120311,
          "FKGL": 6.2
        }
      },
      {
        "source": "CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its function of gene perturbation, a variety of gene editing techniques have been developed to achieve different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen.",
        "prediction": "CRISPR is a popular area of research because it is efficient and specific in editing DNA. Different gene editing techniques have been developed based on how CRISPR works. Scientists can cut and fix DNA at specific points. They can remove, repair, silence, or activate genes. CRISPR/Cas9 is easy to prepare and use, making it a strong tool for high-content screening.",
        "reference": "CRISPR is a popular research area known for its efficiency and specificity in editing DNA sequences. A variety of gene editing techniques have been developed for different aims. The target locations of a DNA strain can be precisely broken and repaired, during which the genes can be removed, repaired, silenced, or activated. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it a powerful tool.",
        "metrics": {
          "SARI": 25.46604674259081,
          "BLEU": 0.14574902024138983,
          "FKGL": 6.8
        }
      },
      {
        "source": "The CRISPR-Cpf1 system has been successfully applied in genome editing. However, target efficiency of the CRISPR-Cpf1 system varies among different gRNA sequences. Using machine learning technology, a SVM model was created to predict target efficiency for any given gRNAs. the first web service application, CRISPR-DT (CRISPR DNA Targeting), to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and specificity is available.",
        "prediction": "The CRISPR-Cpf1 system is used for genome editing. The success of this system varies with different gRNA sequences. Machine learning was used to make a model called SVM that predicts how well gRNAs will work. There is now a web service called CRISPR-DT. This tool helps users design the best gRNAs for the CRISPR-Cpf1 system by looking at both target efficiency and specificity.",
        "reference": "The CRISPR-Cpf1 system (a DNA-editing technology) has been successfully applied in gene editing. However target efficiency of the CRISPR-Cpf1 system varies among different gRNA (piece of RNA that serves as a guide to DNA-targeting enzymes) sequences. Using machine learning technology, a support-vector machine model (algorithm used for data classification) was created to predict the target efficiency for any given gRNAs. CRISPR-DT (CRISPR DNA Targeting) is available. It is the first web service application to help users design optimal gRNAs for the CRISPR-Cpf1 system by considering both target efficiency and target specificity (recognition mecanism).",
        "metrics": {
          "SARI": 23.510321591401976,
          "BLEU": 0.09900351855144227,
          "FKGL": 5.8
        }
      },
      {
        "source": "Emerging self-driving vehicles are vulnerable to different attacks due to the principle and the type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via vehicular ad hoc networks (VANETs). VANETs add new threats to self-driving vehicles that contribute to substantial challenges in autonomous systems. These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for self-driving and semi self-driving cars. The hybrid detection system relies on the Back Propagation neural networks (BP), to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.",
        "prediction": "Self-driving cars use special communication systems that connect them to each other and to the surrounding environment. This connection makes them open to different types of attacks. One main type of communication these cars use is called vehicular ad hoc networks, or VANETs. VANETs bring new dangers that challenge self-driving systems. They can be attacked in various ways, like Sybil attacks, Denial of Service (DoS), black hole, grey hole, and wormhole attacks. \n\nIn this study, we suggest a smart security system to protect the communication of self-driving and semi self-driving cars. This system uses Back Propagation neural networks (BP) to find Denial-of-Service (DoS) attacks, which are common in VANETs. Our tests show that the BP-IDS can effectively spot harmful vehicles in self-driving and semi self-driving cars.",
        "reference": "Self-driving vehicles are vulnerable to different attacks because of their communication systems. These vehicles use external communication via a network. This network add new threats to self-driving vehicles that contribute to the challenges in autonomous systems. With these communication systems, self-driving vehicles are vulnerable to many types of malicious attacks on the network. In this paper, we propose an intelligent security system to secure external communications for self-driving cars. The system will detect a common type of attack in the network: Denial-of-Service (attack trying to make network resource unavailable). The results show that the intrusion detection system is capable of identifying malicious vehicles in self-driving vehicles.",
        "metrics": {
          "SARI": 25.877411755642605,
          "BLEU": 0,
          "FKGL": 9.5
        }
      },
      {
        "source": "Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications. When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, might become deadlocked, or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols, while ensuring safety.",
        "prediction": "Self-driving vehicle technology is improving quickly and will be important for future transportation. A key challenge for self-driving cars on public roads is to work safely with other vehicles. They need to use sensors and communication to do this. When self-driving cars try to be in the same space at the same time, they can crash, get stuck, or brake hard, which can be unsafe or uncomfortable for passengers. This paper looks at how self-driving cars can safely handle merge points, where two lanes meet. We introduce a safe protocol for these merge points called Autonomous Vehicle Protocol for Merge Points. This protocol allows self-driving cars to communicate and use their sensors to work with both other self-driving cars and human-driven cars. Our simulations show that this traffic protocol allows for more cars to pass through while still keeping safety in mind.",
        "reference": "Self-driving vehicle technologies are progressing and are expected to play a significant role in the future of transportation. A main challenge for self-driving vehicles on public roads is the safe cooperation, collaboration and communication with other vehicles. When self-driving vehicles are in the same area, they might collide, become blocked, or suddenly brake and make uncomfortable or unsafe the passengers. We study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other vehicles (self-driving or human-driven). Our simulation results show that our traffic protocol leads to a better traffic flow, while ensuring safety.",
        "metrics": {
          "SARI": 31.909950646433483,
          "BLEU": 0.13276629209863178,
          "FKGL": 9.1
        }
      },
      {
        "source": "This paper provides a holistic study of how stock prices vary in their response to financial disclosures across different topics. Thereby, we specifically shed light into the extensive amount of filings for which no a priori categorization of their content exists. For this purpose, we utilize an approach from data mining - namely, latent Dirichlet allocation - as a means of topic modeling. This technique facilitates our task of automatically categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings from U.S. companies. We then evaluate the subsequent stock market reaction. Our empirical evidence suggests a considerable discrepancy among various types of news stories in terms of their relevance and impact on financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit rating, but also for disclosures regarding business strategy, the health sector, as well as mergers and acquisitions. Our results yield findings that benefit managers, investors and policy-makers by indicating how regulatory filings should be structured and the topics most likely to precede changes in stock valuations.",
        "prediction": "This paper studies how stock prices change in response to financial news. We look at many filings that do not have clear categories. To do this, we use a data mining method called latent Dirichlet allocation for topic modeling. This helps us automatically classify over 70,000 regulatory 8-K filings from U.S. companies. We then check how the stock market reacts. Our findings show that different types of news have varying impacts on the financial markets. For example, we see significant changes in stock prices due to earnings results, credit ratings, business strategy, health sector news, and mergers and acquisitions. Our results help managers, investors, and policy-makers understand how to organize regulatory filings and which topics can affect stock prices.",
        "reference": "This paper studies how stock prices vary with financial disclosures. We particularly study the many uncategorized filings. We use latent Dirichlet allocation for topic modeling. This helps automatically predict the categories based on the content of 70,000 regulatory 8-K filings from U.S. companies, which announce significant events relevant to shareholders. We then evaluate the stock market reaction. Evidence suggests a large difference among types of news stories in relevance and impact on financial markets. For example, we find unusual returns in response to disclosures about earnings results, credit rating, business strategy, the health sector, mergers and acquisitions. Our results are useful for managers, investors and policy-makers. They show how to structure regulatory filings and which topics influence stock valuations.",
        "metrics": {
          "SARI": 57.34897453312888,
          "BLEU": 0.3716052324810953,
          "FKGL": 9.1
        }
      },
      {
        "source": "A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a multi-objective Convolutional Neural Network (CNN) to identify the robot body in the image and estimate 3D positions of the joints by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - Kuka LBR iiwa, which has a significantly different appearance and an additional joint. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.",
        "prediction": "A big problem with deep learning is the lack of data for training. There are some datasets available for common tasks like item recognition and self-driving cars, but the data is limited for industrial robots. In earlier work, we trained a Convolutional Neural Network (CNN) to find robot bodies in images and estimate 3D joint positions using only 2D images. This was only for robots made by Universal Robots (UR). In this study, we expand our method to work with a new robot arm called Kuka LBR iiwa, which looks different and has an extra joint. Instead of collecting big datasets again, we gathered several smaller datasets with a few hundred images each. We used transfer learning techniques on the CNN that was trained on UR robots to adapt it to the new robot with different shapes and looks. We have shown that transfer learning works well in this field. It requires smaller, well-organized training datasets, trains faster, and achieves similar accuracy to the original method, even improving it in some areas.",
        "reference": "Limited data available for training affects the use of deep learning techniques. Datasets are available for problems such as item recognition and classification for self-driving cars, but very limited for industrial robotics. A multi-objective Convolutional Neural Network (CNN) was first trained to identify the robot in the image and find 3D positions of the joints using just a 2D image. But this only used robots made by Universal Robots (UR). Here we work with a new robot arm - Kuka LBR iiwa which looks very different and has an additional joint. We collect a number of smaller datasets and use transfer learning techniques on the CNN trained on UR robots to adapt it to a robot that looks different. Transfer learning requires smaller training datasets, learns faster and reaches similar or even better accuracy.",
        "metrics": {
          "SARI": 44.22168096609063,
          "BLEU": 0.23721892021086066,
          "FKGL": 9.6
        }
      },
      {
        "source": "We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments. The model is a ResNet-18 variant, which is fed in images from the front of a simulated F1 car, and outputs optimal labels for steering, throttle, braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.",
        "prediction": "We present Ignition, a neural network for training self-driving cars in simulations. The model is a ResNet-18 version. It takes images from the front of a simulated F1 car and gives labels for steering, throttle, and braking. We do not train the model to see road features like track edges or distances to other cars. Instead, we show that the network can learn these features on its own.",
        "reference": "Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments. This ResNet-18 variant model receives images from the front of a simulated car and uses them to steer, throttle, or brake appropriately. The model is not explicitly trained to detect road features such as track outline or distance to other cars, as these features can automatically be calculated by the network.",
        "metrics": {
          "SARI": 43.5083043744497,
          "BLEU": 0.22240435160089933,
          "FKGL": 6.2
        }
      },
      {
        "source": "As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to deep neural network (DNN) based automatic speech recognition (ASR) system. When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. Then, the second DNN for primary acoustic modeling employs both the bottleneck features tossed from the first DNN and the acoustics features. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "prediction": "Humanoid robots make noise when they move. This noise comes from motors, fans, and other parts. It makes it hard for them to understand speech. This paper presents a new speech recognition system that works well even with this noise. We use the on/off state of the motor as extra information to help find important input features. \n\nWe look at bottleneck features, which have worked well in deep neural networks for speech recognition. First, we use the motor on/off data along with sound features in the first deep neural network for early sound modeling. Then, the second deep neural network uses both the bottleneck features from the first network and the sound features. \n\nWhen we tested our method on the TIMIT database, we found an 11% improvement in the phoneme error rate compared to traditional systems.",
        "reference": "Internal noise generated by motors, fans and mechanical components severely decreases speech recognition accuracy, when the humanoid robot is moving or shaking its body. This paper presents a novel speech recognition system robust to ego-noise for humanoid robots. On/off state of the motor is used to find relevant input features. Bottleneck features have been successfully applied to automatic speech recognition (ASR) systems based on deep neural networks (DNN). The motor on/off state data and acoustic features are used as the input of the first deep neural network (DNN) for preliminary acoustic modeling. The second DNN for primary acoustic modeling uses both bottleneck features from the first DNN and acoustic ones. The proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, experimental results show that improvement (11% relative) is achieved by our algorithm over the conventional systems.",
        "metrics": {
          "SARI": 32.27883003261148,
          "BLEU": 0.17775488180323817,
          "FKGL": 6.8
        }
      },
      {
        "source": "Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model is made of residual convolutional blocks with hierarchical dilated skip connections joined in steps. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.",
        "prediction": "Recognizing traffic signs with smart systems can greatly lower accidents worldwide. Self-driving cars have made it important to automatically recognize traffic and handheld signs on main roads. Many machine learning methods like Random Forest and SVM, as well as deep learning models, have been suggested for classifying traffic signs. While these models perform well on specific data sets, they struggle with various Traffic Sign Recognition tests. In this paper, we introduce a new all-in-one model that performs better across multiple tests compared to the top models. Our model uses residual convolutional blocks with special connections added in steps. With this approach, we achieve 99.33% accuracy in recognizing German signs and 99.17% accuracy in Belgian traffic signs. Additionally, we propose a new method called dilated residual learning, which uses very little memory and processing power.",
        "reference": "If intelligent systems recognize traffic signs, it can reduce car accidents. For self-driving cars, we need automatic recognition of traffic and hand signs. Machine learning techniques  could be used to classify traffic signs. They perform well on a particular dataset, but fail on multiple traffic sign recognition test datasets. We proposed a new and better method. This model is based on enhanced convolutional neural networks (a deep-learning approach often used for image recognition). This model was more than 99% correct on German and Belgian sign recognition datasets. This new model is very low in both memory and computational complexity.",
        "metrics": {
          "SARI": 37.70461929878242,
          "BLEU": 0,
          "FKGL": 9.7
        }
      },
      {
        "source": "In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN for the end-to-end control is designed to map a single front-facing camera image to a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.",
        "prediction": "In this paper, we show a method for teaching self-driving cars using transfer learning. This method allows a convolutional neural network (CNN) trained in one area to work in another. A regular CNN takes an image from a front camera and gives a steering command. To make transfer learning possible, we add a new task module that also measures lane departure level (LDL). This module uses output from the last convolutional layer. \n\nThe CNN from the first area is called the source network. This network trains a second module called the target network, which also uses the output from the source network's last layer. The target network gives a steering command for a new area. The steering commands from both networks are combined based on LDL, and this combined command is used to drive the car in the new area. \n\nTo test our method, we used two simulators: TORCS for the source area and GTAV for the target area. Our results show that this method works better than other methods for safe and stable car control.",
        "reference": "A transfer learning method to control self-driving cars is proposed, in which a convolutional neural network (CNN) is trained on one domain then used for the same task in a different domain. A conventional CNN is designed to map a single front-facing camera image to control a car. CNN produces a steering command and a lane departure level (LDL, mechanism designed to warn the driver when the vehicle begins to move out of its lane). To enable the transfer learning, a new task module takes the output of the last convolutional layer as input. The CNN trained on the source domain is then used to train the target network. It takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are merged and used for car control in the target domain. Two simulators were used to evaluate the proposed method. The proposed method is more stable and safer for car control.",
        "metrics": {
          "SARI": 36.683610304436215,
          "BLEU": 0.1809207463877521,
          "FKGL": 7.2
        }
      },
      {
        "source": "This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market, based on a Limit Order Book (LOB) such as is common in major stock exchanges. Construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Research aimed at understanding the dynamics of this new style of financial market is hampered by the fact that no operational real-world exchange is ever likely to allow experimental probing of that market while it is open and running live, forcing researchers to work primarily from time-series of past trading data. Similarly, university-level education of the engineers who can create next-generation automated trading systems requires that they have hands-on learning experience in a sufficiently realistic teaching environment. BSE as described here addresses both those needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHuB.",
        "prediction": "This paper talks about the Bristol Stock Exchange (BSE). BSE is a simple model of a central financial market. It uses a Limit Order Book (LOB) like major stock exchanges. BSE was created because many financial markets are now automated. Human traders are being replaced by fast automated trading systems. \n\nResearchers want to study how these new markets work. However, real exchanges do not allow experiments while they are open. This limits research to looking at past trading data. \n\nTo train engineers for future trading systems, students need hands-on experience. BSE meets both needs. It has been used for teaching and research at a top UK university since 2012. The BSE program code is free and available on GitHub.",
        "reference": "This paper describes the Bristol Stock Exchange (BSE), a minimal simulation of a centralised financial market based on a Limit Order Book (LOB), a type of order to buy or sell a security at a specific price or better. BSE was built because most of the financial markets became automated. Research tries to understand the dynamics of this automated financial markets but Trainee engineers also need a realistic learning experience to build automated trading systems. BSE has been successfully used for both teaching and research since 2012. BSE code is freely available on GitHuB.",
        "metrics": {
          "SARI": 48.76267872079077,
          "BLEU": 0.16850774826661288,
          "FKGL": 6
        }
      },
      {
        "source": "Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. First, at DC, Multi-Layer Perceptron (MLP) is used to predict the probabilities of contents to be requested in specific areas. To reduce the car-DC delay, MLP outputs are logged into MEC servers attached to roadside units. Second, in order to cache entertainment contents stylized for car passengers’ features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.",
        "prediction": "When self-driving cars are common and passengers feel safe, they will want new ways to have fun. Our simulations show we can predict entertainment content with 98.04% accuracy and reduce delays. However, getting content from the Data Center (DC) can slow down service because of delays in car-to-DC communication. To solve this, we suggest a deep learning method for caching entertainment in self-driving cars using Multi-access Edge Computing (MEC). First, at the DC, we use a Multi-Layer Perceptron (MLP) to predict what content people will want in specific areas. This helps reduce the delay by saving MLP predictions in MEC servers by the road. Next, we use a Convolutional Neural Network (CNN) to predict the age and gender of passengers. This helps the car know what content to download and save. Finally, we set up a deep learning caching system for self-driving cars to improve entertainment services and reduce downloading delays. We use a Block Successive Majorization-Minimization (BS-MM) technique to solve this problem.",
        "reference": "Once self-driving cars become reality and passengers no longer worry about it, they will need to find other forms of entertainment. Prediction accuracy was 98.04% for contents to be cached for self-driving cars. Such an approach can minimize delay. However, high communication delay can slow retrieval of entertainment content from Data Centers (DC). Self-driving cars can use deep learning caching to address these challenges. It is deployed on the Multi-access Edge Computing (MEC) structure that moves services from a centralized cloud to the network edges and closer to the customer. Multi-Layer Perceptron (MLP), a type of artificial neural network, is used to predict the content requested in specific areas. To reduce delay, MLP outputs are sent to MEC servers for roadside units. A Convolutional Neural Network (CNN) is used to predict passengers’ age and gender to cache adapted entertainment content. The self-driving car can therefore identify the contents to be downloaded and cached. Deep learning caching is formulated for the self-driving car to enhance entertainment services and minimize content downloading delay. A Block Successive Majorization-Minimization (BS-MM) technique solves the problem.",
        "metrics": {
          "SARI": 42.319514318753804,
          "BLEU": 0.18025611148155068,
          "FKGL": 9.1
        }
      },
      {
        "source": "Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges.",
        "prediction": "Moral responsibility is important in automated decision-making. This includes things like self-driving cars and kidney exchanges.",
        "reference": "Moral responsibility is a major concern in automated decision-making, from self-driving cars to kidney transplants.",
        "metrics": {
          "SARI": 25.530916070131752,
          "BLEU": 0,
          "FKGL": 13.5
        }
      },
      {
        "source": "A key problem for autonomous car navigation is the understanding, at an object level, of the current driving situation. Addressing this issue requires the extraction of meaningful information from on-board stereo imagery by classifying the fundamental elements of urban scenes into semantic categories that can more easily be interpreted and be reflected upon (streets, buildings, pedestrians, vehicles, signs, etc.). A probabilistic method is proposed to fuse a coarse prior 3D map data with stereo imagery classification. A novel fusion architecture based on the Stixel framework is presented for combining semantic pixel-wise segmentation from a convolutional neural network (CNN) with depth information obtained from stereo imagery while integrating coarse prior depth and label information. The proposed approach was tested on a manually labeled data set in urban environments. The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to what is obtained from the semantic pixel-wise segmentation of a CNN alone.",
        "prediction": "A big challenge for self-driving cars is understanding the current driving situation. To solve this, we need to get important information from on-board stereo images. This involves classifying key parts of city scenes into categories like streets, buildings, pedestrians, vehicles, and signs. \n\nWe suggest a method that combines basic 3D map data with stereo image classification. We introduce a new way to merge semantic pixel-by-pixel analysis from a convolutional neural network (CNN) with depth information from stereo images. This method also considers basic depth and label data.\n\nWe tested this approach on a data set with labels in city areas. The results showed that our method improved the classification accuracy of urban scene elements compared to using CNN pixel-wise segmentation alone.",
        "reference": "A key problem for autonomous car navigation is the understanding of the environment while driving. It needs to extract information from the sensors and cameras and classify the elements into categories so it can be easily analysed (streets, buildings, pedestrians, vehicles, signs, etc.). A method is to fuse 3D map data with classification of the sensors and cameras. This system combine image segmentation (simplify the representation of an image to facilitate its analysis) and information obtained from sensors and cameras while integrating prior information. This approach was tested on manually entered data in urban environments. The results show that the classification of elements is more accurate with this method than the image segmentation alone.",
        "metrics": {
          "SARI": 38.46992089557186,
          "BLEU": 0.12314163558910085,
          "FKGL": 10.4
        }
      },
      {
        "source": "This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to the development of self-driving systems is the utilization of ROS which is an open-source middleware framework used in the development of robot applications. On the other hand, the popular approach in the automotive industry is the utilization of MATLAB/Simulink which is software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables to create functionalities of ROS-based robots in MATLAB/Simulink. However, it is not been fully utilized in the development of self-driving systems yet because there are not enough samples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.",
        "prediction": "This paper presents a benchmark for self-driving systems using MATLAB and Simulink along with the Robot Operating System (ROS). ROS is a free tool used to develop robot applications. MATLAB/Simulink is common in cars for modeling and simulating designs. This paper connects ROS with MATLAB/Simulink to help create features for self-driving robots. However, this connection is not fully used in self-driving development yet. There aren't enough examples for self-driving, and it's hard for developers to work together. To help, we offer a benchmark suite for a ROS-based self-driving system called Autoware. Autoware is well-known free software that has many self-driving tools. Our benchmark includes MATLAB/Simulink examples found in Autoware. These examples aid in designing self-driving systems using MATLAB/Simulink.",
        "reference": "This paper proposes MATLAB/Simulink tests for an open-source self-driving system based on Robot Operating System. One approach to the development of self-driving systems is the utilization of Robot Operating System (open-source software framework used in the development of robot applications). But the popular approach in the automotive industry is the utilization of MATLAB/Simulink (software for modeling, simulating, and analyzing). MATLAB/Simulink provides an interface that enables to create functionalities of robots (based on robot operating system). However, it is not fully utilized in the development of self-driving systems yet because there are not enough samples, and it is difficult for developers to adopt co-development. We provide MATLAB/Simulink tests for a self-driving system (based on robot operating system) called Autoware. Autoware is a popular open-source software that provides a complete set of self-driving modules. The provided tests contain MATLAB/Simulink samples available in Autoware. They help to design self-driving systems (based on robot operating system) using MATLAB/Simulink.",
        "metrics": {
          "SARI": 23.841420339529343,
          "BLEU": 0.12035475415243224,
          "FKGL": 9.1
        }
      },
      {
        "source": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. One approach to the development of self-driving systems is the use of ROS; however, the system used in the automotive industry is typically designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with ROS-based systems. To allow the two to be used in tandem, it is necessary to rewrite the C++ code and incorporate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, thereby improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "prediction": "Managing self-driving systems is getting more complicated as the technology grows. One way to develop self-driving systems is by using ROS. But in the car industry, systems are usually built with MATLAB/Simulink. This software can simulate and test models for self-driving. However, these models do not work with ROS systems. To make them work together, developers need to rewrite C++ code and add it to the ROS system, which slows down development. The new framework allows models from MATLAB/Simulink to be used in a ROS-based self-driving system. This improves the efficiency of development. Our tests of the new framework showed that it has real potential.",
        "reference": "The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. Self-driving systems use Robot Operating System (open-source software framework used in the development of robot applications). However, in the automotive industry, the system is designed using MATLAB/Simulink, which can simulate and evaluate the models used for self-driving. These models are incompatible with systems based on Robot Operating System. To use both, it is necessary to incorporate new code into the system based on Robot Operating System, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a self-driving system, which improve development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.",
        "metrics": {
          "SARI": 25.780983309992934,
          "BLEU": 0.13825510832464946,
          "FKGL": 7.9
        }
      },
      {
        "source": "Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. Deploying self-driving cars for testing in regular traffic is a common, but costly and risky method, which has already caused fatalities. As a safer alternative, virtual tests, in which self-driving car software is tested in computer simulations, have been proposed. One cannot hope to sufficiently cover the huge number of possible driving situations self-driving cars must be tested for by manually creating such tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for systematically testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car software, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "prediction": "Making self-driving cars safe is very important. Right now, there is no clear way to test them. Testing self-driving cars in real traffic is common but can be expensive and dangerous. Some tests have even led to deaths. A safer way is to use virtual tests. In these tests, self-driving car software is checked using computer simulations. It is too hard to create enough tests for all possible driving situations by hand. To solve this, we created AsFault. AsFault is a tool that automatically makes virtual tests for self-driving car software. We used AsFault to test the lane-keeping feature of an AI self-driving car. It created scenarios where the car could drive off the road. You can watch a video of AsFault in action at https://youtu.be/lJ1sa42VLDw",
        "reference": "Ensuring the safety of self-driving cars is important, but there is no standard way to test them. Testing self-driving cars in regular traffic is costly and risky, and has already caused fatalities. As a safer alternative, virtual tests (self-driving car software tested in computer simulations) have been proposed. To cover the huge number of possible driving situations, self-driving cars must be tested in manually created tests. We developed AsFault, a tool for automatically generating virtual tests for testing self-driving car software. We demonstrate AsFault by testing the lane keeping feature of a self-driving car software with scenarios going off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw",
        "metrics": {
          "SARI": 35.42898071730691,
          "BLEU": 0.23320530739934162,
          "FKGL": 6.2
        }
      },
      {
        "source": "With the developing of the technology on self-driving, more and more L3 driverless vehicles are launched in market, people get opportunities to experience the self-driving cars in their daily life. As a result, there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can not only improve the driving experience, but also accelerate the process of self-driving commercialization. This paper discusses eye-movement interaction, voice interaction and gesture interaction in self-driving car, and analyzes the technology, advantages and disadvantages of the existing interaction modes, and prospect the future development trend of self-driving human-computer interaction.",
        "prediction": "As technology for self-driving cars improves, more L3 driverless vehicles are available. People can try these cars in their daily lives. This creates a bigger demand for self-driving experiences. Good human-computer interaction can make driving better and help self-driving technology grow faster. This paper looks at three types of interaction in self-driving cars: eye movement, voice, and gestures. It analyzes the technology, pros, and cons of each method and explores future trends in self-driving human-computer interaction.",
        "reference": "Self-driving technology is developing, more and more driverless vehicles are launched in market and accessible for people in their daily life. So there is a growing demand for the autopilot experience. Natural and efficient human-computer interaction can improve the driving experience and accelerate the process of self-driving commercialization. This paper discusses interactions with the eye, voice and gesture in self-driving car, analyzes the technology, the interaction modes, and considers the future of self-driving human-computer interaction.",
        "metrics": {
          "SARI": 32.93614904334476,
          "BLEU": 0.12640464548498698,
          "FKGL": 10.6
        }
      },
      {
        "source": "In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on highfidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. The state of the art in driving scenario generation, as adopted by some of the front-runners of the self-driving car industry, still relies on human input [1]. In this paper we propose to automate the process using Bayesian optimization to generate adversarial self-driving scenarios that expose poorly-engineered or poorly-trained self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy using vision-based imitation learning we obtain safer self-driving behavior.",
        "prediction": "Self-driving cars are becoming more common on roads. They promise to make transportation safer and more efficient. To ensure they are reliable, we need thorough software tests. These tests work best on high-quality simulators. In these simulators, self-driving cars interact with other vehicles and pedestrians. It's very important to test self-driving software in many difficult driving situations. \n\nSome leading companies in the self-driving car industry still use human input to create driving scenarios. In this paper, we suggest automating this process. We use Bayesian optimization to create challenging driving situations. These scenarios reveal weaknesses in self-driving cars that are poorly designed or trained. They also increase the chance of crashes with simulated pedestrians and vehicles. \n\nBy adding these scenarios to the training data and fine-tuning the self-driving policy with vision-based imitation learning, we can achieve safer self-driving behavior.",
        "reference": "Self-driving vehicles have become common on public roads, with the promise of a safe and efficient mode of transport. To be reliable, these vehicles need many software tests on simulators with interactions of multiple vehicles and pedestrians. It is essential that self-driving software is evaluated with many different challenging driving scenarios. The driving scenario generation still needs human contribution. We propose to automate the process and generate conflicting self-driving that expose weaknesses of self-driving policies, and increase the risk of collision with simulated pedestrians and vehicles. By incorporating the generated scenarios into the parameters of the self-driving policy, and by adjusting the policy with the imitation learning, we obtain safer self-driving behavior.",
        "metrics": {
          "SARI": 39.479294164785486,
          "BLEU": 0.13378853936860344,
          "FKGL": 10.1
        }
      },
      {
        "source": "At the brink of the introduction of self-driving vehicles, only little is known about how potential users perceive them. This is especially true for self-driving vehicles deployed in public transport services. In this study, the relative preferences for a trip with a self-driving bus is assessed compared to a trip with a regular bus, based on a stated preference experiment. Based on the responses of 282 respondents from the Netherlands and Germany, a discrete choice model is estimated as a Mixed Logit model including attitudes towards trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. This is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. Findings from this study further suggest that the popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences towards automated public transport services are expected to evolve along with the transition from demonstration pilots to their deployment in regular operations.",
        "prediction": "Many people do not know much about self-driving vehicles. This is especially true for self-driving buses used in public transport. This study looks at how people feel about taking a self-driving bus compared to a regular bus. The study includes feedback from 282 people in the Netherlands and Germany. A special model called Mixed Logit was used to analyze the data. It looks at attitudes like trust in self-driving vehicles and interest in technology. The results show that people prefer self-driving buses for short trips. The value of travel time is twice as high for self-driving buses compared to regular buses for these short rides. The study also found that people like self-driving buses less when there is a human worker on board or if the bus follows fixed routes. People who are more interested in technology or trust automated vehicles view self-driving buses more positively. Preferences for automated public transport may change as they begin regular service after demonstrations.",
        "reference": "Little is known about how potential users perceive self-driving vehicles. This is especially true for self-driving vehicles in public transport services. In this study, the preferences between a trip in a self-driving bus and a regular bus were compared. The results of 282 participants were studied, including trust in self-driving vehicles and interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. The preference is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. The popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences for automated public transport services are expected to evolve with their deployment in regular operations.",
        "metrics": {
          "SARI": 31.27105142466972,
          "BLEU": 0.2641915076102504,
          "FKGL": 8.9
        }
      },
      {
        "source": "In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. The fundaments of human dignity, autonomous decision-making, and democracies are shaking, but I believe that they need to be vigorously defended, as they are not only core principles of livable societies, but also the basis of greater efficiency and success.",
        "prediction": "The digital revolution is changing our economy and society. This change brings big benefits but also serious risks. After automating production and creating self-driving cars, society is next. We face an important choice. We can have a society where actions are controlled from the top and carried out through pressure or manipulative tools like targeted ads, or we can choose a society where decisions are made freely and together. Modern information and communication technology allows both options, but the second choice has economic and strategic advantages. Our values, like human dignity and democracy, are under threat. We must protect them because they are important for good societies and can lead to greater success.",
        "reference": "The on-going digital revolution will dramatically transform our economy and societal institutions. But this also presents risks for our society. Automation of society is coming. We must choose either between a society in which the actions are determined top-down by coercion or manipulative technologies (such as personalized ads) or in which decisions are taken freely and cooperatively. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits. Foundations of human dignity, autonomous decision-making, and democracies are weakening. These core principles of society and the basis of greater efficiency and success need to be defended.",
        "metrics": {
          "SARI": 35.00160312142775,
          "BLEU": 0.10716289919729598,
          "FKGL": 10
        }
      },
      {
        "source": "This paper addresses the financial markets problem space with particular emphasis on trading systems seen in banking today. With the advent of modern computing, cross-regional trading transactions can now be executed within milliseconds, a reality that is impossible without the advancements of software systems technology. We then move on and discuss the anatomy of a trading system and how it fits in with the banks ecosystem of vital inter-working components.",
        "prediction": "This paper looks at financial markets, focusing on trading systems used in banks. Today, thanks to modern computers, trading can happen across regions in just milliseconds. This speed is made possible by advanced software systems. Next, we will discuss what a trading system is and how it works with the important parts of a bank's ecosystem.",
        "reference": "This paper addresses problems of trading systems in banking. Modern computing allows cross-regional trading transactions within milliseconds. We then discuss how a trading system fits in with the bank ecosystem.",
        "metrics": {
          "SARI": 39.76642689650465,
          "BLEU": 0,
          "FKGL": 7.6
        }
      },
      {
        "source": "In this paper we present queueing-theoretical methods for the modeling, analysis, and control of autonomous mobility-on-demand MOD systems wherein robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first cast an autonomous MOD system within a closed Jackson network model with passenger loss. The theoretical insights are used to design a robust, real-time rebalancing algorithm, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study of New York shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles roughly 70% of the size of the current taxi fleet operating in Manhattan. Finally, we extend our queueing-theoretical setup to include congestion effects, and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to the problem of system-wide coordination of autonomously driving vehicles, and provides one of the first characterizations of the sustainability benefits of robotic transportation networks.",
        "prediction": "In this paper, we discuss methods to understand and manage autonomous mobility-on-demand (MOD) systems. These systems use self-driving vehicles to transport people in cities and to reposition themselves for good service. We model an autonomous MOD system using a closed Jackson network with passenger loss. We develop a strong, real-time rebalancing algorithm and test it in New York City with an eight-vehicle robot setup. Our case study shows that about 8,000 robotic vehicles can meet Manhattan's current taxi demand. This is about 70% of the current taxi fleet size. We also look at how traffic congestion affects our models and how rebalancing vehicles can impact overall traffic. This paper offers a clear approach to coordinating self-driving vehicles and highlights the benefits of robotic transport networks for sustainability.",
        "reference": "Queueing models are used for autonomous mobility-on-demand MOD systems. A queueing model is constructed so that queue lengths and waiting time can be predicted. In MOD systems,  robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure quality of service. We first cast an autonomous MOD system within a closed Jackson network model,a class of queueing network, with passenger loss. A real-time rebalancing algorithm is applied to a case study of New York City. It is implemented on an eight-vehicle mobile robot testbed. The current taxi demand in Manhattan can be met with about 8,000 robotic vehicles. It represents roughly 70% of the size of the current taxi fleet in Manhattan. Congestion effects are also included to study the impact of autonomously rebalancing vehicles on overall traffic jams. This paper describes the sustainability benefits of robotic transportation networks.",
        "metrics": {
          "SARI": 36.00983253343378,
          "BLEU": 0.16379494113305268,
          "FKGL": 10.7
        }
      },
      {
        "source": "Many model based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. However sensor based robot control overcomes many of the difficulties of uncertain models and unknown environments which limit the domain of application of the previous methods. Furthermore, for service and manipulation tasks, it is more suitable to study the interaction between the robot and its environment at the contact point using the sensor based control, rather than specifying the joint positions and velocities required to achieve them.",
        "prediction": "Many techniques have been suggested for using humanoid robots in service tasks. These include teleoperation, learning from demonstration, and imitation. However, using sensors for robot control solves many problems with unclear models and unknown settings. This makes the previous methods harder to use. For service and manipulation tasks, it is better to focus on how the robot interacts with its environment at the contact point with sensors. This approach is more effective than just setting joint positions and speeds.",
        "reference": "Model based techniques exist to apply domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. Sensor based robot control overcomes difficulties of uncertain models and unknown environments. Interaction between the robot and its environment using the sensor based control is important.",
        "metrics": {
          "SARI": 31.699703450315486,
          "BLEU": 0.09106545584404602,
          "FKGL": 9.6
        }
      },
      {
        "source": "Inaccurate information, in the field of library and information science, is often regarded as a problem that needs to be corrected or simply understood as either misinformation or disinformation without further consideration. Misinformation and disinformation, however, may cause significant problems for users in online environments, where they are constantly exposed to an abundance of inaccurate and/or misleading information. This paper aims to establish conceptual groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of cues to deception, as means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related yet distinct sub-categories of information. Misinformation is a multifaceted concept, more complex than simply being inaccurate or incomplete, and disinformation does not always entail misinformation.",
        "prediction": "In library and information science, inaccurate information is a problem to fix. This type of wrong information can be called misinformation or disinformation. Misinformation and disinformation can lead to big issues for users online. They often see too much misleading information. This paper looks at the links between information, misinformation, and disinformation. We also discuss signs that show when something might be false. Misinformation and disinformation are connected but are not the same. Misinformation is more complex than just being wrong or incomplete. Disinformation does not always include misinformation.",
        "reference": "Inaccurate information is often regarded as a problem that needs to be corrected or simply understood as misinformation or disinformation without further consideration. Misinformation and disinformation may cause problems online because users are constantly exposed to inaccurate and/or false information. This paper aims to establish preliminary work for future research by examining the relationships among information, misinformation, and disinformation. Our analysis extends to a discussion of means for detecting misinformation and disinformation. We argue that misinformation and disinformation are related but distinct sub-categories of information. Misinformation is more complex than simply being inaccurate or incomplete, and disinformation does not always involve misinformation.",
        "metrics": {
          "SARI": 31.200369496205067,
          "BLEU": 0.20638290576698526,
          "FKGL": 10.7
        }
      },
      {
        "source": "The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.",
        "prediction": "Social media is an easy way for people to communicate. However, it also spreads false information. This misinformation can harm the public. We create a system that uses both group knowledge and machine power to find false information. Our main idea is to: (1) automatically organize user expertise based on what they post and (2) connect experts to suspected false information. By sending the false information to the right experts, we can gather their opinions to assess its truth and help correct it. This paper focuses on finding experts for identifying false information. We suggest a method that uses tags to organize the expertise of social media users. Tests on real data show that our method works well for finding experts to identify false information on social media.",
        "reference": "The growth of social media leads to better communication between people, but also propagation of misinformation. The wide spread of misinformation over social media have bad consequences for public interest. We design a framework to help identify misinformation. The idea is to index the expertise of users and to match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, they can judge the credibility of information, and help refute misinformation. In this paper, we focus on finding appropriate experts for misinformation identification. We propose a method to index the expertise of users with tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding for misinformation identification.",
        "metrics": {
          "SARI": 25.776548663305853,
          "BLEU": 0.06220964797796078,
          "FKGL": 8.8
        }
      },
      {
        "source": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation in support of such research. This paper aims to address the aforementioned challenges by conceptualizing misinformation and enabling the interoperability of misinformation. In particular, a representation and a model of misinformation are proposed through surveying, synthesizing, and explicating existing work in the field. The ontology-supported misinformation model can not only guide future misinformation research but also lay the foundation for building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a formal methodology for managing misinformation in a digital library, and suggest future research directions related to the misinformation model.",
        "prediction": "Research on misinformation is very important. There are two big problems in this area. First, there are not enough theories to explain misinformation. Second, there is not enough misinformation to study. This paper tries to solve these problems by defining misinformation and helping different systems work together about it. We provide a new way to represent and model misinformation based on what we found in existing studies. Our model, supported by a clear structure, can help guide future research on misinformation. It also helps create a digital library for misinformation. This will improve how we share, manage, and reuse knowledge about misinformation. We also offer a method for managing misinformation in a digital library and suggest ideas for future research related to our model.",
        "reference": "The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the rare misinformation data. This paper aims to address these challenges by conceptualizing misinformation and being able to completely understand and work on misinformation. A representation and a model of misinformation are proposed through existing work in the field. The model can guide future misinformation research and help building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a methodology for managing misinformation in a digital library, and suggest future research directions.",
        "metrics": {
          "SARI": 24.757323027845786,
          "BLEU": 0.10444157117745796,
          "FKGL": 9.3
        }
      },
      {
        "source": "Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to the event. We define as misinformation any false or inaccurate information that is spread either intentionally or unintentionally. In this paper we study the problem of misinformation identification in OSNs, and we focus in particular on the Twitter social network. Based on user and tweets characteristics, we build a misinformation detection model that identifies suspicious behavioral patterns and exploits supervised learning techniques to detect misinformation. Our extensive experimental results on 80294 unique tweets and 59660 users illustrate that our approach effectively identifies misinformation during emergencies. Furthermore, our model manages to timely identify misinformation, a feature that can be used to limit the spread of the misinformation.",
        "prediction": "Online social networks (OSNs) are popular for sharing information. During emergencies, news often spreads quickly on these networks. Unfortunately, misinformation also spreads. Misinformation is false or incorrect information that can be shared on purpose or by mistake. In this study, we look at how to identify misinformation on OSNs, especially on Twitter. We create a model to detect misinformation based on user and tweet features. This model looks for strange behavior patterns and uses supervised learning to find misinformation. We tested our model on 80,294 unique tweets and 59,660 users. Our results show that our method can effectively spot misinformation during emergencies. Our model can also find misinformation quickly, which helps to slow down its spread.",
        "reference": "Social media have become popular means of information sharing. The spread of news regarding emergency events is common in social media but so is the spread of misinformation. Misinformation is any false or inaccurate information that is spread intentionally or not. In this paper we study the problem of misinformation identification in social media, and we focus in particular on Twitter. We build a misinformation detection model that identifies suspicious behavioral patterns and detect misinformation. We found 80294 unique tweets and 59660 users. It illustrates that our approach identifies misinformation during emergencies. Our model manages to timely identify misinformation, it can be used to limit the spread of the misinformation.",
        "metrics": {
          "SARI": 26.03982420454895,
          "BLEU": 0.11681934090410089,
          "FKGL": 10.2
        }
      },
      {
        "source": "The advent of WWW changed the way we can produce and access information. Recent studies showed that users tend to select information that is consistent with their system of beliefs, forming polarized groups of like-minded people around shared narratives where dissenting information is ignored. In this environment, users cooperate to frame and reinforce their shared narrative making any attempt at debunking inefficient. Such a configuration occurs even in the consumption of news online, and considering that 63% of users access news directly form social media, one hypothesis is that more polarization allows for further spreading of misinformation. Along this path, we focus on the polarization of users around news outlets on Facebook in different European countries (Italy, France, Spain and Germany). First, we compare the page posting behavior and the users interacting patterns across countries and observe different posting, liking and commenting rates. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities generated around specific pages.",
        "prediction": "The rise of the World Wide Web changed how we create and access information. Recent studies show that people often choose information that matches their beliefs. This leads to groups of like-minded individuals who ignore differing views. In this setting, users work together to support their shared ideas, making attempts to correct false information less effective. This trend is also seen in how people read news online. Since 63% of users get news from social media, one idea is that more division leads to more spreading of false information. We focus on how users become polarized around news pages on Facebook in countries like Italy, France, Spain, and Germany. First, we compare how often pages post content and how users interact in each country. We notice different rates of posting, liking, and commenting. Second, we look at how users tend to interact with certain pages, showing selective exposure and the rise of polarized communities around specific pages.",
        "reference": "The WWW changed the ways we can produce and access information. Users tend to select information that agrees with their beliefs. They form polarized groups of like-minded people sharing narratives that ignore contradicting information. Here, users reinforce their shared narratives and can resist debunking This is the case with online news. Because 63% of users access news from social media, polarization could increase misinfomation spread. We focus on polarisation of Facebook news consumers in different European countries. Posting, liking and commenting rates differ across countries. We look at how users interact with different pages and polarized communities form around specific pages.",
        "metrics": {
          "SARI": 45.624703619165494,
          "BLEU": 0.13067361060497104,
          "FKGL": 8.2
        }
      },
      {
        "source": "We find that Italy is the most polarized country, followed by France, Germany and lastly Spain. Finally, we present a variation of the Bounded Confidence Model to simulate the emergence of these communities by considering the usersu0027 engagement and trust on the news. Our findings suggest that trust in information broadcaster plays a pivotal role against polarization of users online.",
        "prediction": "Italy is the most divided country. Next is France, then Germany, and lastly Spain. We show a new version of the Bounded Confidence Model to see how these groups form. We look at how users engage with news and their trust in it. Our results show that trust in news sources helps reduce online polarization among users.",
        "reference": "Italy is the most polarized country, then France, Germany and Spain. A variant of the Bounded Confidence Model simulates the emergence of these communities by considering user engagement and trust in the news. Trust in an information source plays a major role against polarization of users online.",
        "metrics": {
          "SARI": 36.88469433159366,
          "BLEU": 0.1342272794043834,
          "FKGL": 4.2
        }
      },
      {
        "source": "Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative strategies to contrast misinformation. Simultaneously, they allow the spread of misinformation by empowering individuals to self-select the narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. A precise theoretical understanding of such trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs unbiasedly based on the arrival of new information, while a fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. We show that this simple confirmation bias mechanism can generate permanent opinion polarisation. Furthermore, the model results in states where unbiased agents behave \"as if\" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. We derive analytic results for the distribution of individual agents’ beliefs, explicitly demonstrating the aforementioned trade-off between confirmation bias and social connectivity, which we further validate against US county-level data on the impact of Internet access on the formation of beliefs about global warming. Our findings indicate that confirmation bias in small doses may actually result in improved accuracy across individuals by preserving information diversity in a social network. However, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups.",
        "prediction": "Social media lets users see many different opinions. We talk about what this means for policies. We point out the problems with debunking lies and suggest better ways to fight misinformation. Social media also helps misinformation spread. This happens because people choose which stories to follow. They can do this actively by looking for news that agrees with their beliefs or passively through personalized news feeds. We still need a better understanding of these trade-offs. \n\nWe created a simple model that shows how people in a group change their beliefs when they get new information. Most people update their beliefs fairly. But some users stick to their old beliefs and reject news that doesn’t match. This can lead to strong divisions in opinions. \n\nOur model shows that even fair-minded people can act biased because their biased friends limit their access to different information. We analyzed how beliefs spread and found a trade-off between confirmation bias and social connections. We tested our findings using data from US counties about how Internet access affects beliefs about global warming. \n\nOur results suggest a little confirmation bias can actually help people be more accurate by keeping information diverse in a social group. But if confirmation bias goes too far, accuracy goes down as biased people only share limited information.",
        "reference": "Social media give users opportunities to interact with different opinions. We discuss the meaning of our model for policy and the downsides of myth-busting We suggest alternatives to fight misinformation. But misinformation is spread via social media because individuals can search for information that confirms their beliefs and personalized news algorithms may supply it. Theoretical understanding of the balance between opportunities and misinformation is incomplete. We introduce a stylized social learning model. In this model, most participants in a network update their beliefs based on new information, while others show confirmation bias and can reject news that differs from their beliefs. This simple confirmation bias mechanism can create lasting opinion polarisation. Besides, in the model, unbiased people behave “as if” biased because biased neighbours limit their access to diverse information. With our model, we show the trade-off between confirmation bias and social connectivity. We validated the model using the data on how internet access influences beliefs on global warming. Small confirmation bias may even improve accuracy across individuals by keeping information diversity in a social network. But large confirmation bias lowers accuracy as biased individuals limit information flow to others.",
        "metrics": {
          "SARI": 45.14575083429986,
          "BLEU": 0.11016795819041907,
          "FKGL": 8.5
        }
      },
      {
        "source": "Superintelligence is a potential type of future artificial intelligence (AI) that is significantly more intelligent than humans in all major respects. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. Meanwhile, the prospect of superintelligence is the subject of major ongoing debate, which includes a significant amount of misinformation. Superintelligence misinformation is potentially dangerous, ultimately leading bad decisions by the would-be developers of superintelligence and those who influence them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: strategies to prevent the spread of superintelligence misinformation and strategies to correct it after it has spread. In general, misinformation can be difficult to correct, suggesting a high value of strategies to prevent it. The strategies proposed can be applied to lay public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "prediction": "Superintelligence is a type of future artificial intelligence that is smarter than humans in every way. If created, superintelligence could change everything, with effects that could be very good or very bad. There is a lot of discussion about superintelligence, but also a lot of wrong information. This wrong information can be harmful and lead to poor choices by those who want to create superintelligence. This paper looks at ways to fight superintelligence misinformation. It discusses two kinds of strategies: one to stop misinformation from spreading and one to fix it after it has spread. Misinformation can be hard to correct, so strategies to prevent it are very important. The strategies suggested can help raise awareness about superintelligence, improve AI education, and build agreement among experts.",
        "reference": "Superintelligence is a potential future artificial intelligence (AI) significantly more intelligent than humans. Superintelligence could be a major event, with potential consequences that are beneficial or catastrophic. The prospect of superintelligence is the subject of major debate, which includes a lot of misinformation. Superintelligence misinformation could be dangerous, and lead to bad decisions by the people that could work on the project. This paper searches for strategies to counter superintelligence misinformation. Two strategies are examined: to prevent the spread of misinformation and to correct it after it has spread. Misinformation can be difficult to correct, so preventing it seems to be the best strategy. The strategies proposed can be used to bring public attention to superintelligence, AI education programs, and efforts to build expert consensus.",
        "metrics": {
          "SARI": 32.658239638441195,
          "BLEU": 0.16844326543519683,
          "FKGL": 10.6
        }
      },
      {
        "source": "The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\\\\Omega(2^{\\\\log^{1-\\\\epsilon}n^4})$ in polynomial time unless $NP \\\\subseteq DTIME(n^{\\\\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",
        "prediction": "Online misinformation can create public panic and cause serious economic damage. The misinformation containment problem seeks to limit how misinformation spreads in social networks by using competing campaigns. We analyze this problem when many cascades can happen. First, we present a formal model for how information spreads in multiple cascades and explain a key concept called cascade priority. Second, we show that the misinformation containment problem cannot be solved easily without specific conditions being met. Third, we introduce different types of cascade priority that appear in real social networks. Finally, we create new algorithms to address the misinformation containment problem. Our experimental results show that the proposed algorithms are effective.",
        "reference": "The online misinformation could cause public panic and serious economic damages. The goal of misinformation containment is to limit the spread of misinformation in online social networks by launching campaigns to counter misinformation. We present the first analysis of the misinformation containment problem with a random number of cascades (people observe the actions of others and then make the same choice) allowed. First, we provide a diffusion model and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of Ω(2log1−ϵn4) in polynomial time unless $NP \\subseteq DTIME(n^{\\polylog{n}})$ Third, we introduce several types of cascade priority that are frequently seen in social networks. Finally, we design algorithms for solving the misinformation containment problem. The proposed algorithm is effective and shows encouraging results.",
        "metrics": {
          "SARI": 34.297509930850325,
          "BLEU": 0.223526072503073,
          "FKGL": 12.2
        }
      },
      {
        "source": "Yet contemporary work in Philosophy argues provisional belief in conspiracy theories is—at the very—least understandable (because conspiracies occur) and if we take an evidential approach—judging individual conspiracy theories on their particular merits—belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine the kinds of evidence typically associated with conspiracy theories, showing that the evidential problems typically associated with conspiracy theories are not unique to such theories. As such, if there is a problem with the conspiracy theorist’s use of evidence, it is one of principle: is the principle which guides their use of evidence somehow in error? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for a scepticism of conspiracy theories based purely on their use of evidence.",
        "prediction": "Some modern philosophers say it is reasonable to believe in conspiracy theories because conspiracies do happen. They suggest we should look at each conspiracy theory based on its own evidence. I will discuss the types of evidence linked to conspiracy theories and show that the issues with their evidence are not different from other types of evidence. The real problem might be the principle that guides how conspiracy theorists use evidence. I argue that we should not doubt conspiracy theories just because of how they use evidence.",
        "reference": "To believe in conspiracy theories is understandable as conspiracies occur and if we take an evidential approach, belief in such theories turns out to be warranted in a range of cases. Drawing on this work, I examine evidence associated with conspiracy theories, showing that the evidential problems are not unique to conspiracy theories. The problem with the use of evidence of conspiracy theorist could be the principle which guides their use of it. I argue that there is no ground to be suspicious of conspiracy theories as a whole, because those are based on evidence.",
        "metrics": {
          "SARI": 33.09653491163826,
          "BLEU": 0.09804255348628381,
          "FKGL": 11.3
        }
      },
      {
        "source": "Conspiracy theories have gained much academic and media attention recently, due to their large impact on public events. However, little is known about how conspiracy theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis--the Zika virus outbreak. Using a mixed-methods approach including content analysis and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the Zika crisis, the conditions under which Zika conspiracy theories emerge, and the particular discursive strategies through which Zika conspiracy theories developed in online forums. Our analysis shows that conspiracy talk emerged as people attempted to make sense of a public health crisis, reflecting their emergent information needs and their pervasive distrust in formal sources of Zika information. Practical implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "prediction": "Conspiracy theories have gotten a lot of attention lately from both academics and the media. They play a big role in public events. But we still do not know much about how these theories start and grow on social media. This study looks at conspiracy theories on Reddit during the Zika virus outbreak. We used a mixed-methods approach, which means we combined different methods like content analysis and discourse analysis. We found different types of conspiracy theories that people shared on Reddit related to Zika. We also looked at what conditions caused these conspiracy theories to appear and how they were discussed online. Our analysis shows that conspiracy talk grew as people tried to understand the health crisis. This talk shows their need for information and their distrust of official Zika information sources. We discuss the real-world implications for researchers in social computing, health workers, and policy makers.",
        "reference": "Conspiracy theories have gained much attention recently, due to their large impact on public events. Little is known about how conspiracy theories are produced and developed on social media. We present a study of conspiracy theory creation on Reddit during a public health crisis. Using content and discourse analysis, we identified types of conspiracy theories that appeared on Reddit in response to the crisis, the conditions of their creation, and the strategies of their development in online forums. Our analysis shows that conspiracy talk come from people trying to make sense of a public health crisis, which reflect their information needs and their lack of confidence in formal sources of information. Implications for social computing researchers, health practitioners, and policymakers are discussed.",
        "metrics": {
          "SARI": 29.331179391903646,
          "BLEU": 0.14475356065998193,
          "FKGL": 9.1
        }
      },
      {
        "source": "Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories evolve, multiply, and interconnect, further complicating efforts to understand them and to limit their propagation. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif such as \\\"governmental agency-controls-communications\\\" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.",
        "prediction": "Conspiracy theories are everywhere in online discussions. They try to explain recent events that have no official report or express political dissent. These theories change and connect with each other, making it hard to understand them and stop their spread. It is important to find ways to study how people talk about conspiracy theories online. What do users discuss when they mention these theories? What are the common themes? What do these themes show about how users think? This study answers these questions by looking at over ten years of talks in r/conspiracy, a Reddit group for conspiracy discussions. We focus on key parts of a conspiracy theory: the agents involved, their actions, and their targets. For example, the idea that \"government agencies control communications\" shows how different statements describe how these agencies manage information. These themes help us find similarities between different conspiracy theories, even if they discuss different events. By doing this, we can understand how users talk about conspiracy theories and what their discussions mean. Our method allows us to study conspiracy theories in alternative news and social media, helping us understand why they spread and how to fight against them.",
        "reference": "Conspiracy theories are omnipresent in online discussions. Conspiracy theories evolve, multiply, and interconnect, complicating how to understand them and to limit their propagation. It is crucial to develop methods to examine the nature of these conspiratorial discussions. What do users talk about? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in an online community on the social media Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif (an idea) such as \"governmental agency-controls-communications\" represents the various ways in which multiple conspiratorial statements suggest how governmental agencies control information. Narrative-motifs expose similarities between multiple conspiracy theories even when they refer to different events or circumstances. These representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media and ways to understand them and limit their propagation.",
        "metrics": {
          "SARI": 30.652696107720335,
          "BLEU": 0.27051972525355134,
          "FKGL": 9.1
        }
      },
      {
        "source": "Blockchain technology is the underlying enabling technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force. Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make that transaction simpler and easier between entities. We investigate factors that influence an individualu0027s intention to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption grounded in the theory of planned behavior (TPB) to: identify the determinants for the acceptance of cryptocurrency and explore the relative importance of each construct. We offer empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context.",
        "prediction": "Blockchain technology supports Bitcoin, a popular cryptocurrency. This technology is gaining popularity because it can change how we use digital money. People and organizations can benefit from blockchain as it offers secure data sharing and makes transactions easier. We study what affects a person's choice to use cryptocurrency. We create a model based on the theory of planned behavior to find out what influences cryptocurrency acceptance and how important each factor is. We provide proof to help understand cryptocurrency use better, especially in e-government settings.",
        "reference": "Blockchain technology is a technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become popular with the potential to become a powerful disruptive force. People and organizations may use it to increase secure data exchange and make transactions simpler and easier. We investigate what influence people to use a blockchain cryptocurrency. We develop a model of cryptocurrency adoption to identify how cryptocurrency is accepted. Our evidence allow a better understanding of cryptocurrency adoption.",
        "metrics": {
          "SARI": 32.35179493597656,
          "BLEU": 0.07743930841570729,
          "FKGL": 11.2
        }
      },
      {
        "source": "With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we propose a 3-phase analysis approach, which enables us to identify mining scripts and conduct a large-scale study on the prevalence of cryptojacking in the Alexa 1 million websites. We find that cryptojacking is common, with currently 1 out of 500 sites hosting a mining script. Moreover, we perform several secondary analyses to gain insight into the cryptojacking landscape, including a measurement of code characteristics, an estimate of expected mining revenue, and an evaluation of current blacklist-based countermeasures.",
        "prediction": "Memory-bound cryptocurrencies like Monero allow mining to happen using JavaScript in web browsers. This has led to a new practice called cryptojacking or drive-by mining. In cryptojacking, a website uses the computer power of its visitors to mine cryptocurrencies without their knowledge. In this paper, we study this issue closely. We suggest a three-step analysis to find mining scripts and check how common cryptojacking is on the top one million websites. Our findings show that cryptojacking is common, with about 1 out of 500 sites running a mining script. We also do further analysis on code details, estimate mining earnings, and review current methods to block cryptojacking.",
        "reference": "Memory-bound cryptocurrencies like Monero can be mined  from a browser with JavaScript, better than with a dedicated rig. However, this technology enables cryptojacking, whereby a site covertly mines for cryptocurrencies on its visitors’ computers. We propose a new approach to identify mining scripts. The approach was evaluated on Alexa 1 websites. Cryptojacking is common, as 1 in 500 sites host a mining script. We also measured code characteristics, estimated mining revenue, and evaluated blacklist-based countermeasures.",
        "metrics": {
          "SARI": 43.8822550081131,
          "BLEU": 0.10353161559969634,
          "FKGL": 9.3
        }
      },
      {
        "source": "The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.  contracts :[39],\"are commonly written in a high-level language such as Ethereumu0027s Solidity, and translated to compact low-level bytecode for deployment on the blockchain. Once deployed, the bytecode is autonomously executed, usually by a %Turing-complete virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts :[39],\"are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal consists of an analysis pipeline that converts low-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations. Users of the framework can express security analyses in a declarative fashion: a security analysis is expressed in a logic specification written in the \\souffle language.",
        "prediction": "Modern blockchains have led to smart contracts. These are automated programs that operate on the blockchain. Smart contracts are becoming very popular and can be used in law, business, trade, and government. They are often written in a simple language like Solidity and changed into small low-level code for the blockchain. Once they are on the blockchain, the code runs by itself using a virtual machine. \n\nLike all programs, smart contracts can have weaknesses that make them easy targets for attacks. This can be due to poor programming, languages, and tools, including faulty compilers. Smart contracts can also hold a lot of cryptocurrency, making them valuable targets. Therefore, developers need security tools to check the low-level code for any security risks.\n\nIn this paper, we introduce Vandal, a security analysis tool for Ethereum smart contracts. Vandal has a system that converts low-level Ethereum code into logical relations. Users can describe security checks clearly using a special language called Souffle.",
        "reference": "Smart contracts have appeared thanks to blockchain technology. These autonomous programs are used in law, business, commerce, and governance. Smart contracts are written in a high-level language such as Ethereum’s Solidity and translated to bytecode. that live and run on the blockchain are predicted in Once deployed on the blockchain, the bytecode is autonomous. Smart contracts are vulnerable to malicious attacks due to poor programming, languages and toolchains Smart contracts can command large amounts of cryptocurrency. Developers and auditors need frameworks to detect security vulnerabilities. Vandal is a security analysis framework for Ethereum smart contracts. Vandal converts Ethereum Virtual Machine bytecode to semantic logic relations. Users can express security analyses in the Soufflé language.",
        "metrics": {
          "SARI": 41.24409434159572,
          "BLEU": 0.1719446450682181,
          "FKGL": 8.5
        }
      },
      {
        "source": "Vandal is both fast and robust, successfully analysing over 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.",
        "prediction": "Vandal is fast and strong. It can analyze over 95% of 141,000 unique contracts in about 4.15 seconds. This is better than the best current tools like Oyente, EthIR, Mythril, and Rattle under the same conditions.",
        "reference": "Vandal is both efficient (95% accuracy), fast (141k in 4.15 s) and robust. It outperforms the state of the art tools like Oyente, EthIR, Mythril, and Rattle",
        "metrics": {
          "SARI": 59.163891638604284,
          "BLEU": 0.26877191072465745,
          "FKGL": 4.4
        }
      },
      {
        "source": "As Bitcoinu0027s popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds.",
        "prediction": "As Bitcoin became popular in the last ten years, it has attracted many kinds of threats.",
        "reference": "One of the most powerful potential adversaries is the country of China, which has expressed adversarial positions regarding the cryptocurrency and demonstrated powerful capabilities to influence it.",
        "metrics": {
          "SARI": 47.35294117647058,
          "BLEU": 0,
          "FKGL": 7.2
        }
      },
      {
        "source": "For decentralised P2P networks, it is very important to have a mechanism in place that allows the nodes to control resource usage and prevent flooding and denial-of-service attacks with spam. In this paper, we discuss and compare the different approaches to fully decentralised resource control that are used by projects in the cryptocurrency space. The introduced methods are then applied to design a decentralised exchange for Namecoin names (or more generally, crypto assets) as an example.",
        "prediction": "For decentralized P2P networks, it's crucial to have a system that helps nodes manage resource use. This prevents flooding and denial-of-service attacks from spam. In this paper, we look at and compare various ways of managing resources in fully decentralized systems used by cryptocurrency projects. We then apply these methods to create a decentralized exchange for Namecoin names or other crypto assets as an example.",
        "reference": "In decentralised P2P networks nodes need a way  to control resource usage and prevent flooding and denial-of-service attacks. We compared different approaches to decentralised resource control in cryptocurrency projects. We designed a decentralised exchange for crypto assets, for  example Namecoin names, with these methods.",
        "metrics": {
          "SARI": 39.230528418594,
          "BLEU": 0.10482854660723816,
          "FKGL": 9.6
        }
      },
      {
        "source": "Cryptocurrency is a rapid developing financial technology innovation which has attracted a large number of people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have triggered panic and chain reactions towards the application and adoption of cryptocurrency and have caused public security related events. In this paper, we analyze the dynamics and systemic risk of the cryptocurrency market based on the public available price history. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.",
        "prediction": "Cryptocurrency is a fast-growing financial technology that attracts many people worldwide. Its quick changes in value and different reactions from governments lead to fear and reactions in how people use it. This can also cause security problems. In this paper, we study the risks and changes in the cryptocurrency market using available price data. Our analysis shows that the cryptocurrency market is fragile and unstable, matching what the public thinks.",
        "reference": "Cryptocurrency is a financial technology innovation which has attracted many people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have caused panic. In this paper, we analyze the dynamics and risks of the cryptocurrency market. Consistent with public perception, our analysis reveals that the cryptocurrency market is fragile and unstable.",
        "metrics": {
          "SARI": 33.71580970407733,
          "BLEU": 0.2120471835947795,
          "FKGL": 9.9
        }
      },
      {
        "source": "Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by treating it as a financial asset. We position cryptocurrency by comparing its dynamic characteristics with two traditional and massively adopted financial assets: foreign exchange and stock. Our investigation suggests that the dynamics of cryptocurrency are more similar to stock. As to the robustness and clustering structure, our analysis shows cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "prediction": "Cryptocurrency started as a type of digital money. Later, it was used in many areas like financial transactions and capital management. This paper looks at cryptocurrency as a financial asset. We compare cryptocurrency to two well-known assets: foreign exchange and stocks. Our study shows that cryptocurrency behaves more like stocks. However, the cryptocurrency market is weaker and more risky than the stock market.",
        "reference": "Cryptocurrency was initially an implementation of digital currency, then derivatives were created in various fields. This paper aims to help understand cryptocurrency. We compared cryptocurrency with foreign exchange and stock. Our investigation suggests that cryptocurrency is similar to stock. Our analysis shows that cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.",
        "metrics": {
          "SARI": 30.88118445456222,
          "BLEU": 0,
          "FKGL": 8.6
        }
      },
      {
        "source": "The smart device owning rate such as smart phone and smart watch is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time, blockchain-based cryptocurrency is becoming a nonnegligible type of currency and the total value of all types of cryptocurrency has reached USD 200 billion. Therefore, it is a natural demand to support cryptocurrency payment on mobile devices. Considering the poor infrastructure and low penetration of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are the two main obstacles for mobile payment using cryptocurrency. We propose two different schemes for cryptocurrency mobile payment, one involves a centralized bank and the other one does not require any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.",
        "prediction": "More people own smart devices like smartphones and smartwatches than ever. Mobile payment is now a common way to pay in many areas. At the same time, blockchain-based cryptocurrency is growing in importance and its total value has hit 200 billion dollars. Because of this, there is a clear need to support cryptocurrency payments on mobile devices. This is especially useful in developing countries where financial services are limited. However, there are two main problems with using cryptocurrency for mobile payments: high storage costs and slow payment processing. We suggest two ways to solve this for mobile payments. One way uses a centralized bank and the other does not need a central authority. We also give a solution for banks to follow KYC (know your customer) and AML (anti-money laundering) rules when they process cryptocurrency mobile payments.",
        "reference": "The owning rate of smart devices is higher than ever before and mobile payment has become a major payment method. Cryptocurrency is becoming an important type of currency and the total value of cryptocurrencies has reached USD 200 billion. Therefore, support of cryptocurrency payment on mobile devices is a natural demand. Considering the poor infrastructure and the low usage of financial service in developing countries, this combination is especially attractive. The high storage cost and payment processing latency are two obstacles for cryptocurrency mobile payment. We propose two solutions, one with a centralized bank and the other one without any centralized party. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) expectations about cryptocurrency mobile payment.",
        "metrics": {
          "SARI": 28.216343016951463,
          "BLEU": 0.13479937313844084,
          "FKGL": 9.2
        }
      },
      {
        "source": "Motivated by recent financial crises significant research efforts have been put into studying contagion effects and herding behaviour in financial markets. Much less has been said about influence of financial news on financial markets. We propose a novel measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is more general and robust measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.",
        "prediction": "Recent financial crises have led to more research on contagion effects and herding behavior in financial markets. However, less focus has been on how financial news affects these markets. We introduce a new measure called the News Cohesiveness Index (NCI). This measure can show systemic risk in financial news. We studied the NCI using financial documents from major news websites from October 2011 to July 2013. We looked at how financial news and markets influence each other. We believe that strong connections in financial news reflect changes in the financial markets. Cohesiveness is a stronger and more reliable measure of systemic risk shown in news compared to simply counting specific terms. Our findings show that cohesiveness in financial news is closely linked to volatility in financial markets.",
        "reference": "Motivated by recent financial crises, research efforts have been put into studying contagion effects (spread of market disturbances) and herding behaviour (individuals acting collectively without centralized direction) in financial markets. Much less has been said about influence of financial news on financial markets. We propose a measure of collective behaviour in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator, which signals probability of financial system crises. We evaluate the NCI on financial documents from Web news sources and analyse the relation between financial markets and financial news. We hypothesized that strong cohesion in financial news reflects variations in the financial markets. Cohesiveness is a better measure of systemic risk expressed in news, than measures based on simple occurrences of specific terms. Our results show that cohesiveness in the financial news is highly correlated with variations in the financial markets.",
        "metrics": {
          "SARI": 26.455351253048047,
          "BLEU": 0.20275908069079956,
          "FKGL": 8.8
        }
      },
      {
        "source": "In this paper, I propose a methodology to study the comovement between the entropy of different financial markets. The entropy is derived using singular value decomposition of the components of stock market indices in financial markets from selected developed economies, i.e., France, Germany, the United Kingdom, and the United States. I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using a dynamic factor model and derive a common factor behind the entropy movements in these four markets.",
        "prediction": "In this paper, I suggest a way to study how different financial markets move together. I use entropy, which measures uncertainty, by breaking down stock market indices from four developed countries: France, Germany, the United Kingdom, and the United States. I look at how a change in the United States' entropy affects the other markets. I also create a model using a dynamic factor model to find a common factor that explains the entropy changes in these four markets.",
        "reference": "In this paper, I propose a methodology to study the comovement (correlation on profits on investment) between the entropy (randomness and uncertainty) of different financial markets. The entropy is based on the components measuring the stock market in financial markets from selected developed economies (France, Germany, the United Kingdom, and the United States). I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using different time variables and obtain a common factor behind the entropy movements in these four markets.",
        "metrics": {
          "SARI": 30.61756488637276,
          "BLEU": 0.25499882007211966,
          "FKGL": 9.8
        }
      },
      {
        "source": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties - none of which passes through company-owned systems, and is to all intents and purposes 'off the grid'. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics. Detecting 'tech-savvy' corporate fraudsters is a constant game of catch-up. It's not only about playing catch-up with the intellect, motives and awareness of the e-fraudster, but also the technologies that can be misused. What's more, the methods through which the technology can be manipulated to secrete, disguise and protect fraudulent activities - all while staying away from corporate networks where they can readily be monitored and detected - are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.",
        "prediction": "Mobile phone forensics helps find fraud, but fraud has changed with smartphones. Smartphones have many features that can be used for stealing data or inappropriate contact. These actions often do not go through company systems, making them hard to track. Employers need to know these risks when giving out devices. They should have processes in place if there are any suspicions, says Philip Ridley from CCL-Forensics. Catching tech-savvy fraudsters is an ongoing challenge. It is not just about keeping up with their skills and motives, but also with the technologies they misuse. New ways to hide and protect fraud are always appearing. This puts a company's important information and data at risk of theft or sabotage.",
        "reference": "The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones have functionalities that can be used for data theft or inappropriate contact with other parties. They are out of the systems of the company and are not monitored or controlled. Employers need to be aware of these risks when devices are delivered and have processes when suspicions are raised. Detecting fraudsters with computer skills is a constant game of catch-up. It's not only about playing catch-up with the cyberfraudster, but also the technologies that can be misused. Moreover, the methods assisting fraudulent activities (still out of reach of company systems) are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or theft.",
        "metrics": {
          "SARI": 29.93365748385876,
          "BLEU": 0.10224584056924586,
          "FKGL": 7.1
        }
      },
      {
        "source": "Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes to phishing campaigns, a cadre of file matching algorithms is implemented to detect phishing websites based on their content, employing a custom data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using a variety of different content-based approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "prediction": "Phishers change the code of websites to make fake sites look real. They do this to trick people and to avoid being caught. Changes can be small, like code edits, or big, like changing important content. To fight these phishing attacks, we use file matching algorithms to find fake websites based on their content. We tested these algorithms with a set of data that includes 17,992 phishing cases from 159 brands. The experiments show that some methods can detect over 90% of phishing sites while keeping false positives low.",
        "reference": "Phishers, people who attempt to trick people over the internet to steal their money, continue to modify the web pages used in their attacks to imitate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as changing the source code, which is the programming instructions, or as apparent as adding or removing significant content. To respond to these changes to phishing campaigns, algorithms are used to detect phishing websites based on their content, employing a data set consisting of 17,992 phishing attacks targeting 159 different brands. The results of the experiments using different approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.",
        "metrics": {
          "SARI": 21.39178787413964,
          "BLEU": 0.08187806924154552,
          "FKGL": 6.7
        }
      },
      {
        "source": "Mobile devices are very common in everyone’s day-to- day life. Nowadays such devices come with many features of desktop or laptop. Hence people can use these devices for diverse applications. As the acceptability and usability of such devices are very high, there are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is in hike. So it becomes necessary to digitally analyze such devices requiring cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and an agent based approach for logically acquiring such devices. Also it describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.",
        "prediction": "Mobile devices are very common in everyday life. Today, these devices have many features like desktop or laptop computers. People use them for various tasks. Because they are so popular and easy to use, they can be used for illegal activities. The number of mobile phones or smartphones involved in cyber crimes is increasing. Therefore, it is important to analyze these devices using cyber forensics tools. This paper talks about different types of digital evidence in Microsoft Windows Mobile smartphones. It also presents an agent-based approach to acquire these devices. Additionally, it describes a tool made for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.",
        "reference": "Mobile devices are very common in everyone’s day-to-day life. Nowadays such devices come with many features of computers. People can use these devices for diverse applications. There are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is increasing. So it becomes necessary to digitally analyze such devices with cyber forensics tools. This paper discusses different types of digital evidence present in Microsoft’s Windows Mobile smart phones and how to acquire such devices. Also it describes a forensic tool for acquiring and analyzing Windows Mobile devices and personal digital assistants.",
        "metrics": {
          "SARI": 36.08388198320322,
          "BLEU": 0.3323049839075986,
          "FKGL": 9.1
        }
      },
      {
        "source": "Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It gives the information about the processing history of an image, and also can expose forgeries present in an image. There have been a few methods proposed to detect different types of image editing operations in a single framework. However, all the operations have to be known a priori in the training phase. But, in real-forensics scenarios it may not be possible to know about the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.",
        "prediction": "Detecting different types of image editing is important in image forensics. It helps us understand the history of an image and find fakes. Some methods have been made to detect these edits in one system. But to train these methods, we need to know all editing types beforehand. In real situations, we might not know how an image was edited. To solve this, we suggest a new deep learning method that can tell apart different image editing types. Our method uses a special neural network called a deep siamese network. It looks at image parts and classifies them as either edited the same way or differently. Once the network learns to tell the features of different edits, it can also find edits not seen during training. Tests show that our method works well in detecting different image editing types.",
        "reference": "Detection of image editing is an important task in image forensics. The edit history of an image can expose forgeries in the image. Various methods have been proposed to detect image editing operations. However all of the possible types of image manipulation must be known in advance. But, in the real world, manipulations on images are sometimes unknowable. A novel deep learning-based method can distinguish between different types of image editing operations. This method uses a deep siamese neural network to compare images two by two. Using deep learning, the computer can learn to see features of edited images as compared to unedited and learn to differentiate between different image editing operations. This method works to find out what editing an image has undergone.",
        "metrics": {
          "SARI": 44.59806984172641,
          "BLEU": 0.12240767875517458,
          "FKGL": 7.5
        }
      },
      {
        "source": "The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices – potentially progressing a wide variety of currently hindered digital investigations.",
        "prediction": "The number of Internet of Things (IoT) devices is growing fast. This means they will play a bigger role in digital forensic investigations in the future. IoT devices are made by different companies and often have limited ways to connect, like USB ports or WiFi/Bluetooth. With more focus on security and privacy of user data, built-in encryption is now common in these devices. This creates a challenge for digital forensics because it can be hard to analyze data from IoT devices.\n\nThis work looks at electromagnetic (EM) side-channel analysis to help with IoT digital forensics. EM side-channel analysis uses unintentional electromagnetic signals to eavesdrop on how devices work and manage data. This method is non-intrusive, meaning it does not change the target device. \n\nThe paper discusses different EM side-channel attack techniques that are useful for investigating IoT devices. The knowledge gained from this research points out future uses of this technique for digital forensic analysis on IoT devices. This could help many digital investigations that are currently stuck.",
        "reference": "Internet of Things (IoT) devices (devices connected to the internet, collecting and sharing data) will be used in digital forensic investigations in the future. These devices have limited interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing focus on the security and privacy of user data, built-in encryption (which encode information) is becoming commonplace in devices. This presents a significant challenge to digital forensic investigations, where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where electromagnetic emissions are used for eavesdropping on the operations and data handling of devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed – selected according to their applicability in IoT device investigations. The data obtained from our survey are used to identify promising future applications of the technique for digital forensic analysis on IoT devices.",
        "metrics": {
          "SARI": 32.66706128607396,
          "BLEU": 0.2586147935257527,
          "FKGL": 10.4
        }
      },
      {
        "source": "The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is of paramount importance. However, this can be difficult since the attacker has many opportunities and means to hide such changes. In particular, cloud computing, host and guest machine time can be manipulated in various ways by an attacker. Guest virtual machines are especially vulnerable to attacks coming from their (more privileged) host. As such, it is important to guarantee the timeline integrity of both hosts and guests in a cloud, or at least to ensure that the alteration of such timeline does not go undetected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. Further, we describe a novel architecture for host and guest time alteration detection and correction/resilience with respect to compromised hosts and guests. The proposed framework has been implemented on an especially built simulator.",
        "prediction": "Changing machine time in a harmful way is a major problem in computer forensics. Our data shows that our idea can work. It is very important to find these changes and recreate the real order of events. However, this is hard because attackers have many ways to hide their actions. In cloud computing, both host and guest machine time can be adjusted by an attacker. Guest virtual machines are at high risk from their more powerful host. Therefore, we need to protect the timeline of both hosts and guests in the cloud. We must ensure that any changes to this timeline are noticed. In this paper, we look at the problems related to the time integrity of hosts and guests in the cloud. We also present a new system to detect and correct or improve time changes for both hosts and guests that may be compromised. We have tested this framework on a special simulator.",
        "reference": "Machine timestamp falsification can be used in cyber attacks. The algorithm detects manipulations effectively. That is why it is important to detect such changes and to reconstruct the actual timeline of events. However, the hacker can try to hide their tracks. In cloud computing, hackers can manipulate host and guest machine time. Guest virtual machines use computing resources provided by a physical machine called a host. Guest virtual machines are especially vulnerable to attacks coming from their host. That is why the timeline integrity of both hosts and guests in a cloud is important. Any manipulation of such a timeline should be detected. In this paper we survey the issues related to host and guest machine time integrity in the cloud. A new algorithm is proposed to detect host and guest time manipulations and to correct or reject them. A simulator has been built for this purpose.",
        "metrics": {
          "SARI": 45.82718058768394,
          "BLEU": 0.19079974020087842,
          "FKGL": 6.4
        }
      },
      {
        "source": "Image tampering, being readily facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. As the most popular multimedia data, JPEG images can be easily tampered without leaving any clues; therefore, JPEG-based forensics , including the detection of double compression, interpolation, rotation, etc., has become an active research topic in multimedia forensics. Nevertheless, the interesting issue of detecting image tampering and its related operations by using the same quantization matrix has not been fully investigated. Aiming to detect such forgery manipulations under the same quantization matrix, we propose a detection method by using shift-recompression -based reshuffle characteristic features. The learning classifiers are applied for classification. Our experimental results indicate that the approach is indeed highly effective in detecting image tampering and relevant manipulations with the same quantization matrix.",
        "prediction": "Image tampering is easy with today’s digital tools. This creates problems for trusting images. JPEG images are the most common type. They can be changed without leaving signs. This has made JPEG forensics a hot topic. This includes finding double compression, interpolation, rotation, and more. However, detecting tampering with the same quantization matrix has not been fully studied. To address this, we suggest a detection method using shift-recompression and reshuffle features. We use learning classifiers for classification. Our tests show this method works well to find image tampering and related changes with the same quantization matrix.",
        "reference": "Image tampering has become a serious problem with the advancement of digital techniques. JPEG images can be easily manipulated without leaving any clues. This is why researchers are trying to develop methods for JPEG image manipulation detection. However, methods based on image compression techniques, like quantization, are rarely applied. To detect such manipulations, we propose a detection method to identify inconsistencies in the quantization matrix caused by the manipulation. Then machine learning algorithms are applied to detect image manipulation. Experimental results show that the approach is highly effective in detecting image tampering.",
        "metrics": {
          "SARI": 36.58716762287709,
          "BLEU": 0,
          "FKGL": 8.2
        }
      },
      {
        "source": "In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations of its monocular camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets of the whole body including the legs, as well as the camera extrinsic and intrinsic parameters. Furthermore, we developed an approach to automatically select a subset of configurations for the calibration process that yields a good trade-off between the number of observations and accuracy. Further, our approach to configuration selection yields substantially better optimization results compared to randomly chosen viable configurations. Hence, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "prediction": "In this paper, we show a new way to calibrate the movement model of a humanoid robot using data from its single camera. Our method estimates the angles of all joints in the body, including the legs, along with camera settings. We created a way to pick the best setups for the calibration. This method balances the number of observations with accuracy. Our selection method gives much better results than choosing setups randomly. As a result, our system needs fewer setups to get accurate results. Our optimization works generally, and the online tool can be easily used for different humanoids.",
        "reference": "In this paper, we present a novel approach to accurately calibrate a humanoid and its motions (robot with human form or characteristics) based on observations of its camera. Our technique estimates the parameters of the complete model, consisting of the joint angle offsets and the camera parameters. We developed an approach to automatically select configurations for the calibration process that produces a good compromise between the number of observations and accuracy. Our approach to configuration selection generates better optimization results compared to randomly chosen viable configurations. Thus, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.",
        "metrics": {
          "SARI": 24.02470980573076,
          "BLEU": 0.1382242755170692,
          "FKGL": 8.9
        }
      },
      {
        "source": "In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. From the data of an onboard consumer-grade depth camera, our system estimates the robot's pose to compensate for drift of odometry and maintains a heightmap representation of the environment. Based on this model, our system iteratively computes sequences of safe actions including footsteps and whole-body motions, leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system leads to robust navigation in cluttered environments and the robot is able to traverse highly challenging passages.",
        "prediction": "This paper presents a navigation system for humanoid robots. It helps robots move by themselves in messy and unknown places. The system uses a simple depth camera to find the robot's position. This helps fix errors in movement tracking and keeps a map of the area. The system then plans safe actions, like where to step and how to move, to guide the robot to its goals. To avoid crashes while planning, we created a new method that considers the robot's shape and obstacles. Tests with a Nao robot show that our system allows for strong navigation in tricky spots, enabling the robot to get through difficult places.",
        "reference": "In this paper, we present a navigation system that allows humanoid robots (robots with human form or characteristics) to autonomously navigate in unknown, cluttered environments. From the data of a camera which can judge depth and distance, our system estimates the robot’s position and maintains a height representation of the environment. Our system determines safe actions (including footsteps and body motions) leading the robot to target locations. To efficiently check for collisions during planning, we developed a new approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments, our system allows the robot to navigate in clustered environments and traverse challenging passages.",
        "metrics": {
          "SARI": 25.40143482942624,
          "BLEU": 0,
          "FKGL": 6.9
        }
      },
      {
        "source": "In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict traversal costs corresponding to potential obstacles in regions not yet observable by the robot's sensors. By taking into account the predicted costs for path planning, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.",
        "prediction": "In this paper, we show a new way for robots to move in messy indoor areas. We suggest measuring how many obstacles are nearby based on objects the robot already sees. This helps predict how hard it will be to move through areas that the robot's sensors cannot see yet. By using these predictions for planning its path, the robot can navigate better and avoid getting stuck in clutter. Our tests show that this method allows the robot to move through messy spaces more efficiently and finish tasks much faster than the usual way that does not use predictions.",
        "reference": "In this paper, we introduce an approach for robot to navigate through cluttered indoor environments. We propose to estimate obstacles based on already detected objects and use them to predict potential obstacles in regions not yet observable by the robot. The robot is then able to navigate in a more anticipated manner and reduces the risk of getting stuck. Our method enables the robot to efficiently navigate through cluttered environments and faster than a standard approach not using any prediction.",
        "metrics": {
          "SARI": 28.121871277104976,
          "BLEU": 0.07843336371066756,
          "FKGL": 8.7
        }
      },
      {
        "source": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, step-by-step following of the human might not be the optimal solution since better paths for the robot exist. We propose to perform a prediction about the human's future movements and use this information in a reinforcement learning framework to generate foresighted navigation actions for the robot. Since frequent occlusions of the human will occur due to obstacles and the robot's constrained field of view, the estimate about the humans's position and the prediction of the next destination are affected by uncertainty. Our approach deals with such situations by explicitly considering occlusions in the reward function such that the robot automatically considers to execute actions to get the human in its field of view. We show in simulated and real-world experiments that our technique leads to significantly shorter paths compared to an approach in which the robot always tries to closely follow the user and, additionally, can handle occlusions.",
        "prediction": "In many cases, people walk along set paths to reach certain places where a mobile robot is needed. Just following the person may not be the best way for the robot because there might be better paths available. We suggest predicting where the person will go next and using this info in a learning system to plan smart movements for the robot. Since the view of the person can be blocked by obstacles, the robot may not always know where the person is headed. Our method handles this by adding the issue of blockages to the reward system, so the robot learns to move in a way that helps it see the person. We show in tests, both in simulations and in real life, that our method results in much shorter paths than when the robot only tries to follow the user closely and can also deal with blockages.",
        "reference": "In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, following the human might not be the optimal solution because better paths for the robot exist. We propose to predict the human’s future movements and use this information to generate navigation actions for the robot. The estimation of the human’s position and the prediction of the next destination are uncertain because of the occlusions due to obstacles and the robot’s limited field of view. Our approach deals with considering occlusions so the robot automatically considers to execute actions to get the human in its field of view. We show that our technique leads to significantly shorter paths compared to an approach in which the robot follows the user and, additionally, can handle occlusions.",
        "metrics": {
          "SARI": 24.52804704309629,
          "BLEU": 0.10578967217121547,
          "FKGL": 9.4
        }
      },
      {
        "source": "An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem, especially when this person moves around since the robot’s field of view is constrained and the environment structure typically poses further visibility constraints that influence the perception of the user. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. As the robot needs time to reach the possible search locations, we take this time as well as the visibility constraints into account when computing effective search locations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.",
        "prediction": "An autonomous service robot has to find a user to do a task. This is hard when the user moves around. The robot cannot see everything clearly. The robot's view is limited and the environment can block visibility. In this paper, we suggest a new method. It checks how likely the robot can see the user at different spots using Monte Carlo simulations. The robot also needs time to reach these spots, so we factor in this time and visibility when choosing where to search. This helps the robot pick the best place to find the user. Our tests in different simulated settings show that our method helps reduce search time compared to a greedy approach with extra information.",
        "reference": "An autonomous service robot often first has to search for a user to execute a task. This is a challenging problem, especially when this person moves around because the robot’s field of view is limited. The environment also poses visibility limitations that influence the perception of the user. In this paper, we propose a method that estimate the user’s observability in the environment. When estimating effective search locations, we take into account the time for the robot to reach the search location and the visibility limitations. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments demonstrate that our approach leads to a significantly shorter search time compared to other approach.",
        "metrics": {
          "SARI": 30.503262626007142,
          "BLEU": 0.17794352139608802,
          "FKGL": 7.3
        }
      },
      {
        "source": "Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Considering dynamic obstacles introduces another level of complexity, since they can lead to necessary replanning or collisions at later stages. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.",
        "prediction": "Robots need to understand their surroundings to move safely. Most robots, like humanoid robots and small wheeled ones, work best on flat surfaces. Humanoid robots cannot just use a simple 2D map because they need to know height to step over and onto objects. This gets harder when there are moving obstacles, which can cause crashes or need changes to their path later. In this paper, we share a system that finds flat areas in height maps and spots moving obstacles. Our system makes prediction maps that help plan paths quickly and efficiently. We tested our framework in simulations and real life, showing it takes less than 10ms for each calculation and supports advanced 3D footstep planning in real-time.",
        "reference": "Perception of the environment is necessary for mobile robots to navigate safely in dynamic environments. Most robots (humanoids and wheeled robots) rely on planar regions. For humanoids (robots with human form or characteristics), a 2D map is not sufficient because they can step over and onto objects, so they need height information. Dynamic obstacles bring another level of complexity, because it can lead to necessary replanning or collisions. In this paper, we present a framework that extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be planned in real time at low processor cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms and allows for real-time 3D footstep planning.",
        "metrics": {
          "SARI": 27.00666977374448,
          "BLEU": 0.13289292315176746,
          "FKGL": 7.6
        }
      },
      {
        "source": "In this paper, a lab automation drone notional concept is introduced. Here, a robotic limb is attached to a robotic rotorcraft. The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high throughput systems (HTS). The resulting drone could augment existing HTS operations. The 6 degree-of-freedom (DOF) arm and gripper design are presented. Test-and-evaluation approach and results are also given.",
        "prediction": "This paper presents a new idea for a lab automation drone. It combines a robotic arm with a flying drone. The arm has a gripper that helps the drone handle small items like micro-arrays and test tubes used in high throughput systems. This drone could improve current HTS tasks. The design of the arm and gripper with six degrees of freedom is explained. Results from testing and evaluation are included.",
        "reference": "In this paper, a concept of lab automation drone is introduced. Here, a robotic arm is attached to a drone. The arm’s gripper allows the drone to manipulate objects such as chips and test tubes often used in scientific experimentations like high throughput systems. The drone could augment existing high throughput systems operations. The 6 degree-of-freedom arm and gripper design are presented. Results of evaluations and tests are also given.",
        "metrics": {
          "SARI": 33.06268332502084,
          "BLEU": 0.1115241181484379,
          "FKGL": 5.5
        }
      },
      {
        "source": "Robot companionship has become more popular in past years. However, humanoid gait might be somewhat unstable for these applications. Even with miniature humanoids, falls occur frequently. Thus, wheel attachments have been added onto a miniature humanoid, so it can move faster and with more stability than walking. In addition, with such attachments a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as an experimentation and performance evaluation platform. This paper discusses preliminary work regarding robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "prediction": "Robot companionship is becoming more popular. But humanoid robots can have unstable movements. Miniature humanoids often fall down. To help with this, wheels have been added to them. This makes the robot move faster and more steadily. The robot can switch between walking and rolling when needed. DARwIn-OP is a humanoid robot used for testing and evaluation. This paper looks at early work on robot companionship using a miniature humanoid that can fetch toys by following voice commands.",
        "reference": "Robot companionship has become more popular in past years. However, humanoid (robots with human form or characteristics) walking might be unstable. Even with miniature humanoids, falls occur frequently. Wheels have been added onto a miniature humanoid, so it can move faster and with more stability than walking. With wheels, a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used for experimentation and evaluation. This paper discusses preliminary work for robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.",
        "metrics": {
          "SARI": 26.32037585340904,
          "BLEU": 0.16034425658358512,
          "FKGL": 5.9
        }
      },
      {
        "source": "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds of up to 6m/sec. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning based navigation. With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest. In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot. Derived from the classic image classification algorithms, our classifier has been constructed in the form of a fast 39-layered Inception model, that evaluates the presence of roads using the tomographic reconstructions of the input frames. Based on the Inception-v3 architecture, our system performs better in terms of processing complexity and accuracy than many existing models for imitation learning. The data used for training the system has been captured from the drone, by flying it in and around urban and semi-urban streets, by experts having at least 6-8 years of flying experience. Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process. With the extensive amount of drone data that we collected, we have been able to navigate successfully through roads without crashing or overshooting, with an accuracy of 98.44%.",
        "prediction": "In today's world, automation and robotics are important. Autonomous vehicles, like drones, are a big focus for research in schools and companies. MAVNet helps drones fly fast, up to 6m/sec. This research shows results that compare with other top methods for vision and navigation. More unmanned aerial vehicles are used in civilian and commercial activities. This means we need better autonomy for these systems. Government rules limit how high civil drones can fly. So, road-tracking navigation is of growing interest. \n\nTo tackle these goals, we suggest using imitation learning. This data-driven approach helps drones navigate city streets by mimicking expert pilots. Our classifier, based on classic image classification methods, uses a 39-layer Inception model. It checks for roads using images captured by the drone. This system, based on Inception-v3, is better in processing and accuracy than many other imitation learning models. \n\nWe collected training data from expert pilots with 6-8 years of experience flying drones in urban areas. We made sure to get permission from the right authorities to keep pedestrians safe during data collection. With the large amount of drone data we collected, we navigated through roads without crashing or going off track, achieving an accuracy of 98.44%",
        "reference": "Current academic and industrial research is interested in autonomous vehicles. MAVNet computational efficiency enables the drone to fly up to 6m/sec. The solution is compared with other recent methods. Drones are increasingly used in the civilian and commercial domain and need to be autonomous. Governments set guidelines on the operation ceiling of civil drones. So, road-tracking based navigation is attracting interest. Researchers propose data-driven solutions allowing drones to autonomously navigate city streets, learning to fly by imitating an expert pilot. The algorithm, based on the Inception model, detects roads using tomographic reconstructions. The Inception model is an extension of classic image classification algorithms. The Inception-v3 architecture has better accuracy than many existing models of imitation learning. The data used for training the system was captured from the drone over urban streets, navigated by an expert pilot. Data collection requires special permission to ensure pedestrian security. The drone can navigate successfully through roads without crashing or overshooting; its accuracy is 98.44%.",
        "metrics": {
          "SARI": 41.38626798573905,
          "BLEU": 0.1043248206729118,
          "FKGL": 9.3
        }
      },
      {
        "source": "Advances in electronics and sensor technology have widened the scopes of networked drones to include applications as diverse as surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording for their operations. Large drones are used singly in missions while small ones are used in formations or swarms. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "prediction": "New electronics and sensor technology have expanded the use of networked drones. Drones are now used for many purposes like watching over areas, recording videos, advertising, sending signals, transporting items, and delivering goods. These tasks often need video recording. Large drones usually fly alone while small drones may fly in groups or swarms. Small drones are useful for civilian tasks. Using small drones for group flying, entertainment, and signal tasks has led to the use of networked drones. To create group displays, a system for real-time drone formation control is suggested. Simulations show that drone groups can share messages effectively.",
        "reference": "Advances in technology allowed drones to be used for surveillance, video recording, operations, entertainment/advertising, signal emission, transportation, and delivery. These applications and services require video recording. Large drones are used individually while small ones are used in groups. The small drones are proving to be useful in civilian applications. Consideration of small drones for the applications such as group flight, entertainment, and signal emission lead to deployment of networked drones. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.",
        "metrics": {
          "SARI": 22.013223245346477,
          "BLEU": 0.1785668650550379,
          "FKGL": 7
        }
      },
      {
        "source": "This study focuses on the digital marketing capabilities of tourism SMEs. The study addresses the question of how the use of ICT-based tools benefit the organisational capabilities of a company. By adopting marketing as a set-of-skills approach, the study provides new insights into the existing tourism literature on e-marketing. Initial findings indicate that the digital marketing capabilities of companies are transformed through ICT-based uptake. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that the use of ICT-based tools transforms digital marketing capabilities from a set of abilities that enables tourism SMEs not only to float in a web-marketing stream, but also to lead such a stream.",
        "prediction": "This study looks at how tourism small and medium enterprises (SMEs) use digital marketing. It explores how ICT tools help improve the skills of these companies. By treating marketing as a set of skills, the study gives new insights into e-marketing in tourism. Early findings show that the use of ICT tools changes digital marketing skills in companies. Four main skills were found, and each one grows with the use of these tools. A key finding is that ICT tools change digital marketing skills. This allows tourism SMEs to not just survive in web marketing but also to lead in it.",
        "reference": "This study focuses on the digital marketing capabilities of tourism small and medium-sized enterprises (SMEs). The study questions on how information and communication technology (ICT) tools can help the organisational capabilities of a company. The study provides new insights from tourism studies on digital marketing. Findings show that digital marketing capabilities of companies are transformed by ICT tools. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that ICT tools transform these capabilities in a set of abilities for SMEs in web-marketing.",
        "metrics": {
          "SARI": 35.1843206190341,
          "BLEU": 0.14280975570468957,
          "FKGL": 6.5
        }
      },
      {
        "source": "The prevalence and rapid development of the Internet and mobile technology in recent decades has revamped our living styles and daily habits. To ride on the digital trend, more business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. The rise of Key Opinion Leaders (KOLs), social media platforms, and Omni-channel retailing have attracted countless business entities to consider the adoption of digital marketing tools for promoting and advertising their brands and products. However, with the increasing diversity of the types of digital marketing tools, they must be carefully selected based on a multiple number of criterion. In this paper, a fuzzy-AHP method is proposed and developed for assisting industry practitioners in systematically and effectively evaluate and select proper digital marketing tool(s) for adoption. The developed method not only streamlines the internal business process of digital marketing tool selection, but it also increases the practitioner's effectiveness of achieving the pre-defined strategic marketing objectives.",
        "prediction": "The Internet and mobile technology have changed how we live and work over the last few decades. Many businesses now use digital tools for their activities. Marketing and advertising have also gone digital. Key Opinion Leaders (KOLs), social media, and Omni-channel retailing have made many businesses use digital marketing tools to promote their brands and products. There are many types of digital marketing tools, so businesses must choose carefully. This paper presents a fuzzy-AHP method to help industry people select the right digital marketing tools. The method makes it easier to choose tools and helps businesses meet their marketing goals.",
        "reference": "The development of the Internet and mobile technology in recent decades has modified our living styles and daily habits. More business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. Many businesses consider the adoption of digital marketing tools for their promotion and advertisement, due to the rise of Key Opinion Leaders (person with expert status and influence), social media platforms, and Omni-channel retailing (multiple channels but a uniform experience for the customer). However, with the increasing diversity of digital marketing tools, they must be carefully selected. In this paper, a method to organize and analyze complex decisions is proposed and developed for assisting businesses to evaluate and select appropriate digital marketing tools. The developed method rationalizes and simplifies the process of digital marketing tool selection. It also increases effectiveness of achieving strategic marketing objectives.",
        "metrics": {
          "SARI": 29.65530548622482,
          "BLEU": 0.1360068950242198,
          "FKGL": 8.9
        }
      },
      {
        "source": "In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be primarily contingent on changes in muscle size. Given this assumption, there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength. However, the conclusion that a change in muscle size affects a change in strength is surprisingly based on little evidence. We suggest that these changes may be completely separate phenomena based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with detraining, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load resistance training, yet divergent results in strength.",
        "prediction": "This paper looks at a topic from 1955 about muscle hypertrophy and strength. Many believe that muscle size growth from exercise leads to more strength. Because of this belief, many training programs focus on growing muscle size and strength together. However, there is not much evidence to support that muscle size and strength changes are linked. We think these changes might be different based on: (1) the weak link between muscle size change and muscle strength change after training; (2) the loss of muscle mass when stopping training but still keeping muscle strength; and (3) similar muscle growth from both low-weight and high-weight training but different strength results.",
        "reference": "In this paper we revisit a topic originally discussed in 1955, namely the lack of evidence that muscle growth plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be related to changes in muscle size. Given this assumption, training programs made for increasing both muscle size and strength were popular. However, the conclusion that a change in muscle size has an influence on strength is surprisingly based on little evidence. We suggest that these changes may be based on: (1) the weak correlation between the change in muscle size and the change in muscle strength after training; (2) the loss of muscle mass with the interruption of training, yet a maintenance of muscle strength; and (3) the similar muscle growth between low-load and high-load strength training, yet different results in strength.",
        "metrics": {
          "SARI": 27.307093163227442,
          "BLEU": 0.21980569706891742,
          "FKGL": 9.4
        }
      },
      {
        "source": "Low-intensity occlusion (50-100 mm hg) training provides a unique beneficial training mode for promoting muscle hypertrophy. Training at intensities as low as 20% 1 repetition maximum with moderate vascular occlusion results in muscle hypertrophy in as little as 3 weeks. A typical exercise prescription calls for 3 to 5 sets to volitional fatigue with short rest periods. The metabolic buildup causes positive physiologic reactions, specifically a rise in growth hormone that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "prediction": "Low-intensity occlusion training uses 50-100 mm hg pressure. It helps muscles grow bigger. You can train with just 20% of your one-rep max and see results in 3 weeks. A typical plan includes 3 to 5 sets until you get tired, with short rest breaks. This training causes a build-up of metabolites. This triggers a rise in growth hormone, which is more than what happens with heavier weights. Occlusion training is good for people who can’t use high weights. This includes those with joint pain, patients recovering from surgery, people in cardiac rehab, athletes needing rest, and astronauts.",
        "reference": "Low-intensity occlusion training, which is a training method with blood flow restriction, provides a unique beneficial training mode for muscle growth. Training at intensities as low as 20% of maximum strength with moderate occlusion results in muscle growth in 3 weeks. A typical exercise consists of 3 to 5 sets to muscle failure with short rest periods. The metabolic accumulation causes positive physiologic reactions, specifically a rise in growth hormone, which is one of the main hormone responsible of muscle growth, that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.",
        "metrics": {
          "SARI": 27.92325833561991,
          "BLEU": 0.1922246429771825,
          "FKGL": 5.7
        }
      },
      {
        "source": "Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. We and others have demonstrated that aerobic exercise acutely and chronically alters protein metabolism and induces skeletal muscle hypertrophy. These findings promote an antithesis to the status quo by providing novel perspective on skeletal muscle mass regulation and insight into exercise-countermeasures for populations prone to muscle loss.",
        "prediction": "Current belief says aerobic exercise training does not change skeletal muscle size much. We and others have shown that aerobic exercise changes protein metabolism in the short and long term. It also increases skeletal muscle size. These results challenge the old view and give new ideas on how to control skeletal muscle mass. They also offer exercise solutions for people at risk of losing muscle.",
        "reference": "Current belief suggests cardio exercise training has minimal effect on muscle size. We and others have demonstrated that cardio exercise alters protein metabolism (responsible for protein synthesis and muscle growth) and induces muscle growth. These findings against the current belief provide a novel perspective on muscle mass regulation and insight into exercise to prevent muscle loss.",
        "metrics": {
          "SARI": 37.378586826322326,
          "BLEU": 0.10377373689530833,
          "FKGL": 8.4
        }
      },
      {
        "source": "Stretch training is widely used in a variety of fitness-related capacities such as increasing joint range of motion, preventing contractures and alleviating injuries. Moreover, some researches indicate that stretch training may induce muscle hypertrophy; however, studies on the topic have been primarily relegated to animal and in vitro models. The purpose of this brief review was to evaluate whether stretch training is a viable strategy to induce muscle hypertrophy in humans. Of the 10 studies identified, 3 observed some significantly positive effects of stretch training on muscle structure. Intriguingly, in these studies, the stretching was carried out with an apparatus that aided in its performance, or with an external overload. Of the 5 available studies that integrated stretching into a resistance training programme, 2 applied the stretching in the interset rest period and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to confer beneficial changes in muscle size and architecture; alternatively, albeit limited evidence suggests that when stretching is done with a certain degree of tensile strain (particularly when loaded, or added between active muscle contractions) may elicit muscle hypertrophy.",
        "prediction": "Stretch training is commonly used in fitness to increase joint movement, prevent tightness, and help with injuries. Some studies suggest that stretch training might help muscles grow bigger, but most research has been on animals or cells in a lab. This review looks at whether stretch training can help muscles grow in humans. Out of 10 studies, 3 showed positive effects of stretch training on muscle structure. These studies used machines or added weight during stretching. Among 5 studies that included stretching with resistance training, 2 had stretching during rest periods and showed more muscle growth. In summary, gentle stretch doesn't seem to help muscle size, but stretching with some tension, especially when added between muscle contractions, may help muscles grow.",
        "reference": "Stretch training is used in a variety of fitness capacities such as increasing flexibility, preventing contractures (shortening of muscle) and reduce injuries. Moreover, some researches indicate that stretch training may induce muscle growth. The purpose of this review was to evaluate if stretch training is a viable strategy to induce muscle growth in humans. Of the 10 studies identified, 3 observed positive effects of stretch training on muscles. In these studies, the stretching was performed with external help like equipment or load. Of the 5 available studies that integrated stretching into strength training, 2 applied the stretching in the rest period between the sets and were the ones that showed enhanced muscle growth. In conclusion, passive, low-intensity stretch does not appear to produce beneficial changes in muscles but evidence suggests that when stretching is done with a certain intensity (particularly when loaded, or added between active muscle contractions), it may induce muscle growth.",
        "metrics": {
          "SARI": 28.917908480450823,
          "BLEU": 0.09729325815818442,
          "FKGL": 8.9
        }
      },
      {
        "source": "Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. Therefore, the purpose of this review is to discuss the effects of cycle training on muscle size and strength of the lower extremity and the possible mechanisms for increasing muscle size with cycle training. It is plausible that cycle training requires a longer period to significantly increase muscle size compared to typical resistance training due to a much slower hypertrophy rate. Cycle training induces muscle hypertrophy similarly between young and older age groups, while strength gain seems to favor older adults, which suggests that the probability for improving in muscle quality appears to be higher in older adults compared to young adults. For young adults, higher-intensity intermittent cycling may be required to achieve strength gains. It also appears that muscle hypertrophy induced by cycle training results from the positive changes in muscle protein net balance.",
        "prediction": "Cycle training is a common part of exercise programs. It helps improve aerobic capacity and heart health. However, we still need to understand how cycle training affects muscle size and strength. We know that professional cyclists have bigger muscles than non-cyclists. This review looks at how cycle training impacts muscle size and strength in the legs. It also explores how cycle training can help increase muscle size. \n\nCycle training may take longer to increase muscle size compared to regular weight training. This is because it has a slower rate of growth, known as hypertrophy. Cycle training can help muscles grow in both young and older people. However, older adults may gain strength more effectively than young adults. This suggests older adults could improve muscle quality better than young adults. \n\nFor young adults, doing high-intensity cycling might be needed for strength gains. Cycle training also seems to promote muscle growth by improving the balance of muscle proteins in the body.",
        "reference": "Cycling training is performed as a major part of any training program which objective is to improve cardiovascular capacities and health. However, the effect of cycling training on muscle size and strength still requires a better comprehension. Therefore, the purpose of this review is to discuss the effects of cycling training on muscle size and strength of the legs and the possible mechanisms for increasing muscle size with cycling training. It is plausible that cycling training requires a longer period to significantly increase muscle size compared to strength training due to a much slower muscle growth rate. Cycling training induces muscle growth similarly between young and older people, while strength gain seems to favor older adults. For young adults, series of high-intensity cycling may be required to achieve strength gains. It also appears that muscle growth induced by cycling training results from the positive changes in protein balance, which is the balance between protein intake which leads to muscle growth and protein loss which leads to muscle loss).",
        "metrics": {
          "SARI": 31.719362437725923,
          "BLEU": 0.14821235315286369,
          "FKGL": 6.9
        }
      },
      {
        "source": "Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship between training dose/volume and physiological response has been hypothesized to exist. However, the majority of available evidence supports a clear dose-response relationship between resistance training volume and physiological responses, such as muscle hypertrophy and health outcomes. Additionally, there is a paucity of data to support the inverted U-shaped response. The overarching principle argued herein is that volume is the most easily modifiable variable that has the most evidenced-based response with important repercussions, be these muscle hypertrophy or health-related outcomes.",
        "prediction": "Resistance training is the best way to increase muscle mass. It also helps improve health. Many studies show it is safe and can treat or prevent many diseases. Researchers have looked into how little exercise is needed to be effective. Some suggest that there is an inverted U-shaped pattern where too much or too little training is less effective. However, most evidence shows a clear link between how much you train and the results you get, like muscle growth and health benefits. There is not much data to back the inverted U-shaped idea. The main point is that training volume is easy to change and has strong evidence showing it affects muscle growth and health outcomes.",
        "reference": "Strength training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is helpful for treating and preventing diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. Similarly, an inverted U-shaped relationship (which means that the efficiency increases until a point, and then starts decreasing) between training volume and physiological response has been hypothesized to exist. However, evidence supports a clear correlation between resistance training volume (number of exercises, sets, etc.) and physiological responses, such as muscle growth and health outcomes. Additionally, there is a lack of data to support the inverted U-shaped response. The principle suggested is that volume is the most easily modifiable variable that has the most important response, for muscle growth or health outcomes.",
        "metrics": {
          "SARI": 25.10496440695917,
          "BLEU": 0.12233627342131605,
          "FKGL": 6.6
        }
      },
      {
        "source": "Although the effects of short versus long inter-set rest intervals in resistance training on measures of muscle hypertrophy have been investigated in several studies, the findings are equivocal and the practical implications remain unclear. Current evidence indicates that both short and long inter-set rest intervals may be useful when training for achieving gains in muscle hypertrophy. Novel findings involving trained participants using measures sensitive to detect changes in muscle hypertrophy suggest a possible advantage for the use of long rest intervals to elicit hypertrophic effects. However, due to the paucity of studies with similar designs, further research is needed to provide a clear differentiation between these two approaches.",
        "prediction": "Studies have looked at how short and long rest times between sets affect muscle growth in resistance training. The results are mixed and it's not clear what they mean for training. Evidence shows that both short and long rest times can help with muscle growth. New studies with trained people suggest that longer rest times might be better for muscle growth. But there are not enough studies with the same setup, so more research is needed to clearly compare these two methods.",
        "reference": "The effects of short versus long rest intervals between sets in strength training on muscle growth have been investigated in several studies but the findings and the practical implications are unclear. Current evidence indicates that both short and long rest intervals may be useful for muscle growth. Novel findings suggest for trained people a possible advantage for the use of long rest intervals to induce muscle growth. However, due to the lack of studies with similar methodologies, further research is needed to provide a clear differentiation between these two approaches.",
        "metrics": {
          "SARI": 30.867410227899246,
          "BLEU": 0.14866880296379792,
          "FKGL": 6.1
        }
      },
      {
        "source": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review describes a cellular memory in skeletal muscle in which hypertrophy is 'remembered' such that a fibre that has previously been large, but subsequently lost its mass, can regain mass faster than naive fibres. A new cell biological model based on the literature, with the most reliable methods for identifying myonuclei, can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells before hypertrophic growth. Even if subsequently subjected to grave atrophy, the higher number of myonuclei is retained, and the myonuclei seem to be protected against the elevated apoptotic activity observed in atrophying muscle tissue. Fibres that have acquired a higher number of myonuclei grow faster when subjected to overload exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit in the elderly, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "prediction": "Memory is when we take in, keep, and recall information. In vertebrates, it was thought that memory happens only in the brain. This review talks about memory in skeletal muscle. When muscle grows larger, it can remember this size. If a muscle fiber loses size but has been large before, it can grow back faster than a new fiber. A new biological model explains this. It shows that untrained fibers take myonuclei from active satellite cells before they grow larger. Even if the muscle shrinks a lot later, the extra myonuclei stay. These myonuclei are protected from the high cell death that can happen in shrinking muscles. Fibers with more myonuclei grow faster with heavy exercise. So, these nuclei act as a type of memory for strength. This memory may last a long time in humans. Myonuclei can stay stable for at least 15 years and maybe even longer. However, it can be harder to get myonuclei in older people. If this long-lasting muscle memory is true for humans, early strength training can be important for public health.",
        "reference": "Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, it has been said that it occurs only in the brain. This review describes a cellular memory in muscles in which muscle growth is 'remembered' such that a fibre that has lost its mass, can regain it faster than untrained fibres. A new model, with the most reliable methods for identifying myonuclei (nuclei in muscle fiber), can explain this phenomenon. According to this model, previously untrained fibres recruit myonuclei from activated satellite cells (quiescent cells acting as a reserve population of cells for regeneration of muscle) before muscle growth. Even after grave muscle loss, the higher number of myonuclei is retained, and the myonuclei seem to be protected against programmed cell death observed during muscle loss. Fibres that have acquired a higher number of myonuclei grow faster during strength exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might be very long lasting in humans, as myonuclei are stable for at least 15 years and might even be permanent. However, myonuclei are harder to recruit for old people, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.",
        "metrics": {
          "SARI": 23.22067044175814,
          "BLEU": 0.13008137427641236,
          "FKGL": 5.6
        }
      },
      {
        "source": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several observational studies lend support to this hypothesis, with an inverse relationship noted between the frequency of eating and adiposity. The purpose of this narrative review is to present and discuss a meta-analysis with regression that evaluated experimental research on meal frequency with respect to changes in fat mass and lean mass. Feeding frequency was positively associated with reductions in fat mass and body fat percentage as well as an increase in fat-free mass. However, sensitivity analysis of the data showed that the positive findings were the product of a single study, casting doubt as to whether more frequent meals confer beneficial effects on body composition. In conclusion, although the initial results of this meta-analysis suggest a potential benefit of increased feeding frequencies for enhancing body composition, these findings need to be interpreted with circumspection.",
        "prediction": "Some people think that eating small meals often can help lose fat and keep weight steady. Some studies show that eating more often is linked to lower body fat. This review looks at research on meal frequency and changes in fat mass and lean mass. Eating more often was linked to lower fat mass and body fat percentage and to more fat-free mass. But a closer look at the data revealed that these positive results were based on only one study. This raises questions about whether eating more meals really helps body composition. In summary, while the early results of this review suggest that eating more often might help with body composition, we need to be careful when interpreting these findings.",
        "reference": "It has been hypothesized that eating small, frequent meals enhances fat loss and helps for better weight maintenance. Several studies support this hypothesis, with a link between the frequency of eating and being overweight. This review aims to present and discuss a research on meal frequency with respect to changes in body mass. Eating frequency was positively associated with reductions in fat mass and increasing in fat-free mass. However, the positive findings were the product of a single study, leaving a doubt about the beneficial effect of frequent meals on body composition. In conclusion, although the initial results suggest a potential benefit of increased eating frequencies for enhancing body composition, these findings need to be carefully interpreted.",
        "metrics": {
          "SARI": 28.316229871773572,
          "BLEU": 0.11807715740786374,
          "FKGL": 7.7
        }
      },
      {
        "source": "Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by resting metabolic rate reduction and fat accumulation. Ten weeks of resistance training may increase lean weight by 1.4 kg, increase resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of resistance training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Resistance training may assist prevention and management of type 2 diabetes by decreasing visceral fat, reducing HbA1c, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Resistance training may enhance cardiovascular health, by reducing resting blood pressure, decreasing low-density lipoprotein cholesterol and triglycerides, and increasing high-density lipoprotein cholesterol. Resistance training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Resistance training may be effective for reducing low back pain and easing discomfort associated with arthritis and fibromyalgia and has been shown to reverse specific aging factors in skeletal muscle.",
        "prediction": "Inactive adults lose muscle mass at a rate of 3% to 8% every ten years. They also have a lower resting metabolic rate and gain fat. Doing resistance training for ten weeks can help. It may increase lean body weight by 1.4 kg, boost resting metabolic rate by 7%, and lower fat weight by 1.8 kg. Benefits of resistance training include better physical performance, improved movement control, faster walking speed, more independence, better thinking skills, and higher self-esteem. It can help prevent and manage type 2 diabetes by reducing belly fat, lowering HbA1c levels, increasing glucose transporters, and improving insulin sensitivity. Resistance training may also improve heart health by lowering resting blood pressure and decreasing bad cholesterol while increasing good cholesterol. It can help build stronger bones, with studies showing a 1% to 3% increase in bone mineral density. Resistance training can reduce low back pain and help with arthritis and fibromyalgia. It may also reverse some aging effects in muscles.",
        "reference": "Per decade, inactive adults experience a muscle loss of 3% to 8%, a reduction of resting metabolic rate (energy consumed by the body at rest) and fat accumulation. Ten weeks of strength training may increase fat-free mass by 1.4 kg, resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of strength training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Strength training may assist prevention and management of type 2 diabetes by decreasing abdominal fat, reducing HbA1c which is a form of hemoglobin linked to a sugar, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Strength training may enhance cardiovascular health, by reducing resting blood pressure, decreasing 'bad' cholesterol and triglycerides (main constituents of body fat), and increasing 'good' cholesterol. Strength training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Strength training may be effective for reducing low back pain, joint and muscle pain, and reverse specific aging factors in muscles.",
        "metrics": {
          "SARI": 33.611726547107985,
          "BLEU": 0.2687198318867888,
          "FKGL": 9
        }
      },
      {
        "source": "We hypothesized that compared to post-exercise protein intake, co-ingestion of alcohol would down-regulate translation initiation signaling and decrease rates of MPS.",
        "prediction": "We believe that drinking alcohol after exercise will lower the signaling that starts protein making and reduce muscle protein synthesis rates compared to just taking protein.",
        "reference": "We assumed that consumption of protein and alcohol (compared to protein only) would reduce protein synthesis.",
        "metrics": {
          "SARI": 65.94246031746033,
          "BLEU": 0,
          "FKGL": 15.8
        }
      },
      {
        "source": "Eight healthy physically active male subjects (age 21.4±4.8 yr, body mass (BM) 79.3±11.9 kg, peak oxygen uptake (VO2peak) 48.1±4.8 mL·kg−1·min−1, leg extension one repetition maximum (1RM) 104±20 kg; values are mean ± SD) who had been participating in regular exercise (3 times wk−1 for >6 months) volunteered for this study.",
        "prediction": "Eight healthy and active young men took part in this study. They were about 21 years old and weighed around 79 kg. Their peak oxygen uptake was about 48 mL per kg per minute, and they could lift an average of 104 kg in a leg extension exercise. All of them exercised regularly, at least 3 times a week for more than 6 months.",
        "reference": "Eight healthy physically active men (3 trainings per week for at least 6 months) volunteered for this study.",
        "metrics": {
          "SARI": 42.18605429343969,
          "BLEU": 0,
          "FKGL": 4.8
        }
      },
      {
        "source": "The study employed a randomized counter-balanced, cross-over design in which each subject completed bouts of consecutive resistance, continuous and intermittent high-intensity exercise with either post-exercise ingestion of alcohol-carbohydrate (ALC-CHO), alcohol-protein (ALC-PRO) or protein only (PRO) beverages on three separate occasions.",
        "prediction": "The study used a random design. Each participant did exercise sessions that included resistance, continuous, and intermittent high-intensity workouts. After these sessions, they drank one of three types of drinks on different days: alcohol-carbohydrate, alcohol-protein, or protein only.",
        "reference": "Subjects did high intensity exercise and just after consumed an alcohol-carbohydrate, an alcohol-protein or a protein drink in three separate occasions.",
        "metrics": {
          "SARI": 37.261339507303795,
          "BLEU": 0,
          "FKGL": 13
        }
      },
      {
        "source": "Resistance exercise consisted of eight sets of five repetitions at ∼80% of 1RM.",
        "prediction": "Resistance exercise had eight sets of five reps at about 80% of one-rep max",
        "reference": "Training consisted of eight sets of five repetitions of leg extension at 80% of their maximum strength.",
        "metrics": {
          "SARI": 42.68306631991594,
          "BLEU": 0.18842888419335746,
          "FKGL": 5.2
        }
      },
      {
        "source": "After completion of the final set, subjects rested for 5 min before commencing 30 min of continuous cycling at ∼63% PPO (∼70% VO2peak).",
        "prediction": "After finishing the last set, subjects rested for 5 minutes. Then they started 30 minutes of continuous cycling at about 63% of their peak power output. This is around 70% of their peak oxygen uptake.",
        "reference": "After 5 minutes rest, subjects cycled for 30 minutes at medium intensity,",
        "metrics": {
          "SARI": 42.02586129945621,
          "BLEU": 0,
          "FKGL": 6.7
        }
      },
      {
        "source": "Upon completion, subjects rested on the bike for 2 min before undertaking 10×30 s high intensity intervals at ∼110% of PPO, with 30 s active recovery (∼50% PPO) between each work bout.",
        "prediction": "After finishing, the participants rested on the bike for 2 minutes. Then they did 10 times 30 seconds of high intensity exercise at about 110% of their peak power output. After each 30 seconds of work, they had 30 seconds of active recovery at about 50% of their peak power output.",
        "reference": "then 5 minutes at high intensity.",
        "metrics": {
          "SARI": 39.73063973063973,
          "BLEU": 0,
          "FKGL": 8.7
        }
      },
      {
        "source": "Immediately following exercise and after 4 h recovery, subjects ingested a 500 mL solution of either protein (PRO, 25 g whey protein powder; ISO8, Musashi, Melbourne, VIC Australia) or an energy-match in the form of CHO (25 g maltodextrin, International Health Investments, Helensvale, QLD Australia).",
        "prediction": "After exercise and 4 hours of rest, participants drank 500 mL of either a protein solution or a carb solution. The protein solution had 25 grams of whey protein powder. The carb solution had 25 grams of maltodextrin.",
        "reference": "Immediately following exercise and after 4 h recovery, subjects consumed a 500mL drink of protein or carbohydrates.",
        "metrics": {
          "SARI": 34.01256015226603,
          "BLEU": 0,
          "FKGL": 8.2
        }
      },
      {
        "source": "Furthermore, a CHO-based meal (1.5 g·kg−1 BM) was consumed ∼2 h post-exercise, immediately after the muscle biopsy, according to recommendations for post-exercise glycogen recovery [24].",
        "prediction": "A CHO-based meal was eaten about 2 hours after exercise. This happened right after taking a muscle biopsy. It follows the guidelines for recovering glycogen after exercise.",
        "reference": "A carbohydrate meal was consumed 2 h after exercise, according to recommendation for glycogen recovery.",
        "metrics": {
          "SARI": 45.64810366574539,
          "BLEU": 0,
          "FKGL": 8
        }
      },
      {
        "source": "The 8 h time frame represents an important phase of post-exercise recovery [1] as well as the period during which blood alcohol concentrations are likely to be elevated by a post-event drinking binge [14].",
        "prediction": "The 8-hour time frame is key for recovery after exercise. It is also when blood alcohol levels may be high due to drinking after an event.",
        "reference": "The 8 h period after exercise is an important phase for recovery but also the period during which blood alcohol level may be the highest after excessive consumption of alcohol.",
        "metrics": {
          "SARI": 39.29275926616352,
          "BLEU": 0,
          "FKGL": 6
        }
      },
      {
        "source": "The alcohol ingestion protocol (1.5 g·kg−1 BM; 12±2 standard drinks) began 1 h post-exercise and was consumed in 6 equal volumes of 1 part vodka (∼60 mL) to four parts orange juice (∼240 mL, 1.8 g CHO·kg−1 BM) during a 3 h period.",
        "prediction": "The alcohol drinking plan started 1 hour after exercise. Participants drank 1.5 grams of alcohol per kilogram of body mass, which is about 12 standard drinks. The drinks were made of 1 part vodka, about 60 mL, and 4 parts orange juice, about 240 mL. This was done in 6 equal servings over 3 hours.",
        "reference": "The alcohol consumption started 1 h after exercise with 6 drinks (60mL of vodka and 240mL of orange juice) and during 3 h.",
        "metrics": {
          "SARI": 41.414814077092196,
          "BLEU": 0,
          "FKGL": 4
        }
      },
      {
        "source": "The focus of the early post-exercise period (i.e., 1–8 h) is to enhance physiological processes that are critical for reversing the exercise-induced disturbances to homeostasis and physiological function and for promoting adaptations to training [1].",
        "prediction": "The early post-exercise period is 1 to 8 hours after working out. This time is important for fixing the changes in the body caused by exercise. It helps restore balance and function in the body. It also supports the body’s ability to adapt to training.",
        "reference": "The period after exercise (1-8h) enhances the body's processes that reverse the effects of the exercise and promotes adaptation to the training.",
        "metrics": {
          "SARI": 36.68015186729861,
          "BLEU": 0,
          "FKGL": 6.5
        }
      },
      {
        "source": "For the PRO condition, orange juice was consumed with a matched volume of water in place of the alcohol.",
        "prediction": "In the PRO condition, orange juice was drank with the same amount of water instead of alcohol.",
        "reference": "For the protein drink, orange juice was consumed with water instead of alcohol.",
        "metrics": {
          "SARI": 52.213274198568314,
          "BLEU": 0.33380800216772966,
          "FKGL": 7.6
        }
      },
      {
        "source": "Subjects ingested the beverages within 5 min every 30 min.",
        "prediction": "Subjects drank the drinks in 5 minutes every 30 minutes",
        "reference": "Subjects consumed the drinks in 5 minutes every 30 minutes.",
        "metrics": {
          "SARI": 87.81268568033273,
          "BLEU": 0.7080735452207036,
          "FKGL": 4.8
        }
      },
      {
        "source": "Blood, cell signaling and mRNA data were analyzed by two-way ANOVA (two factor: time × treatment) with repeated measures and myofibrillar protein synthesis was analyzed by one-way ANOVA with repeated measures.",
        "prediction": "Blood, cell signaling, and mRNA data were studied using two-way ANOVA. This method considered two factors: time and treatment, with repeated measures. Myofibrillar protein synthesis was studied using one-way ANOVA also with repeated measures.",
        "reference": "Data (blood, protein synthesis, responses, etc.) were analyzed several times for the 3 different consumptions of drinks.",
        "metrics": {
          "SARI": 34.758120419885124,
          "BLEU": 0,
          "FKGL": 10.1
        }
      },
      {
        "source": "The first novel finding of this study was that mTOR signaling and rates of myofibrillar protein synthesis (MPS) following concurrent resistance, continuous and intermittent high-intensity exercise, designed to mimic the metabolic profile of many team sports, were impaired during the early (8 h) recovery phase by the ingestion of large amounts (1.5 g•kg−1 BM) of alcohol.",
        "prediction": "This study found that mTOR signaling and myofibrillar protein synthesis were affected after intense exercise. This exercise was like what many team sports do. Drinking a lot of alcohol, about 1.5 grams per kilogram of body mass, made recovery harder in the first 8 hours.",
        "reference": "The first finding of this study was that the mTOR enzyme (responsible for muscle growth) and protein synthesis (after exercise) were reduced by an excessive alcohol consumption during the 8 h period of recovery.",
        "metrics": {
          "SARI": 37.74609818520575,
          "BLEU": 0,
          "FKGL": 6.5
        }
      },
      {
        "source": "These outcomes were most evident (37% reduction in rates of MPS) when alcohol was consumed in the absence of post-exercise protein intake, as is likely to occur when intoxication reduces the athlete's compliance to sound recovery practices.",
        "prediction": "These results showed a 37% decrease in MPS rates. This happened when alcohol was consumed without protein intake after exercise. This is common because being drunk can make athletes follow recovery practices less.",
        "reference": "Consequences were most evident when alcohol was consumed without protein (37 % reduction of protein synthesis).",
        "metrics": {
          "SARI": 51.26512068957911,
          "BLEU": 0.14219389639501667,
          "FKGL": 7.6
        }
      },
      {
        "source": "However, a second finding was that even when protein was consumed in amounts shown to be optimally effective to stimulate MPS [8] during post-exercise recovery, the intake of alcohol reduced MPS by ∼24%, representing only a partial ‘rescue’ of the anabolic response compared with protein alone.",
        "prediction": "A second finding was that when people ate enough protein after exercise, it helped muscle protein synthesis (MPS). However, drinking alcohol lowered MPS by about 24%. This means that alcohol can reduce the benefits of protein ingestion on muscle growth.",
        "reference": "A second finding was that even with protein consumption, alcohol reduced protein synthesis by around 24%.",
        "metrics": {
          "SARI": 58.367519374198174,
          "BLEU": 0.11382866842595922,
          "FKGL": 9.7
        }
      },
      {
        "source": "The mechanistic target of rapamycin complex 1 (mTORC1) is a central node for integrating nutrient (i.e. amino acid) and exercise/contraction signal transduction [31], [32].",
        "prediction": "mTORC1 is an important part of how the body uses nutrients like amino acids and responds to exercise signals",
        "reference": "mTOR is important in absorption of nutrients (like amino acids) and in sending signals to the body to produce an effort.",
        "metrics": {
          "SARI": 61.71446243814665,
          "BLEU": 0,
          "FKGL": 9.5
        }
      },
      {
        "source": "In conclusion, the current data provide the novel observation that alcohol impairs the response of MPS in exercise recovery in human skeletal muscle despite optimal nutrient provision.",
        "prediction": "In conclusion, new data show that alcohol affects how muscle protein synthesis works during exercise recovery in human skeletal muscle even when nutrients are provided.",
        "reference": "The data from this study show that alcohol reduce the protein synthesis after exercise even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 42.065125420388576,
          "BLEU": 0,
          "FKGL": 16.6
        }
      },
      {
        "source": "The quantity of alcohol consumed in the current study was based on amounts reported during binge drinking by athletes.",
        "prediction": "The amount of alcohol in this study was based on what athletes said they drank during binge drinking",
        "reference": "The quantity of alcohol consumed in this study was based on consumption by athletes during binge drinking.",
        "metrics": {
          "SARI": 49.3862954855602,
          "BLEU": 0.3817666460451127,
          "FKGL": 6.8
        }
      },
      {
        "source": "However, published reports suggest intakes of some individuals can be significantly greater [9], [50], which is of concern for many reasons related to health and safety [13].",
        "prediction": "Some studies show that some people eat much more than others. This is worrying for many health and safety reasons.",
        "reference": "However, reports show that some individuals can consume higher quantities, which is a concern for health and safety issues.",
        "metrics": {
          "SARI": 52.416632382832574,
          "BLEU": 0,
          "FKGL": 4.8
        }
      },
      {
        "source": "Recommended nutritional strategies to maximize recovery in skeletal muscle include protein for enhancing rates of protein synthesis and carbohydrate for replenishing glycogen stores [2],[3].",
        "prediction": "To recover muscles better, eat protein to help build muscle and carbohydrates to refill energy stores",
        "reference": "Nutritional strategies to optimize muscle recovery include proteins which are recommended to allow protein synthesis (production of new proteins which improve, for example, muscle mass) and carbohydrates to recharge glycogen stores (energy storage).",
        "metrics": {
          "SARI": 43.15689387241111,
          "BLEU": 0,
          "FKGL": 11.9
        }
      },
      {
        "source": "Regrettably, there has been difficulty in finding an educational message with alcohol consumption related to sports performance that has resonance with athletes.",
        "prediction": "It is hard to find an educational message about alcohol use and sports performance that connects with athletes",
        "reference": "Regrettably, it is difficult to find an educational message about alcohol consumption and sport performances that speaks to athletes.",
        "metrics": {
          "SARI": 64.25003554905516,
          "BLEU": 0.3099401951159055,
          "FKGL": 10.3
        }
      },
      {
        "source": "Given the need to promote protein synthesis that underpins adaptation, repair and regeneration of skeletal muscle the results of the current study provide clear evidence of impaired recovery when alcohol is consumed after concurrent (resistance, continuous and intermittent high-intensity) exercise even in the presence of optimal nutritional conditions.",
        "prediction": "The study shows that drinking alcohol after exercise can harm muscle recovery. This is true even when nutrition is good. Protein synthesis is important for muscle adaptation, repair, and regeneration.",
        "reference": "Considering the importance of protein synthesis (adaptation, repair and regeneration of muscles), the results of this study provide evidence of reduced recovery when alcohol is consumed after exercise, even with an optimal consumption of nutrients.",
        "metrics": {
          "SARI": 38.55487102572907,
          "BLEU": 0,
          "FKGL": 9.6
        }
      },
      {
        "source": "We propose our data is of paramount interest to athletes and coaches.",
        "prediction": "Our data is very important for athletes and coaches",
        "reference": "We consider our data crucial for athletes and coaches.",
        "metrics": {
          "SARI": 52.51204813704814,
          "BLEU": 0.2824099048856541,
          "FKGL": 6.8
        }
      },
      {
        "source": "Our findings provide an evidence-base for a message of moderation in alcohol intake to promote recovery after exercise with the potential to alter current sports culture and athlete practices.",
        "prediction": "Our findings show that drinking alcohol in moderation can help recovery after exercise. This idea could change sports culture and athlete habits.",
        "reference": "Our findings provide evidence for a message of moderation in alcohol consumption to promote recovery after exercise, with the potential to change sport cultures and athlete practices.",
        "metrics": {
          "SARI": 25.921304038151867,
          "BLEU": 0,
          "FKGL": 7.6
        }
      },
      {
        "source": "Muscle contraction and the intake of leucine-rich protein sources activate independent but complimentary signaling responses that converge at the mechanistic target of rapamycin (mTOR) to stimulate protein translation enhancing rates of muscle protein synthesis [4]–[6].",
        "prediction": "Muscle contraction and eating leucine-rich proteins activate separate but related signaling responses. These responses meet at mTOR, which helps start protein translation and increase muscle protein synthesis.",
        "reference": "Muscle contraction and consumption of leucine-rich protein (essential amino acid composing proteins) induce reactions which increase the protein synthesis.",
        "metrics": {
          "SARI": 56.07960940197781,
          "BLEU": 0,
          "FKGL": 13.3
        }
      },
      {
        "source": "The ingestion of ∼20–25 g of high quality protein soon after exercise [7], repeated every 4 h [8] has been shown to maximise the anabolic response in skeletal muscle.",
        "prediction": "Eating about 20 to 25 grams of high-quality protein soon after exercise helps build muscle. Doing this every 4 hours boosts muscle growth.",
        "reference": "Consumption of 20-25 g of high quality protein after exercise and repeated every 4 h showed an optimal anabolic response (increasing muscle mass) of the muscles.",
        "metrics": {
          "SARI": 34.552879744296135,
          "BLEU": 0,
          "FKGL": 6.6
        }
      },
      {
        "source": "The cultural environment surrounding some sports often involves the intake of large amounts of alcohol after training and competition, with athletes in several team sports being particularly at risk of “binge drinking” practices [9]–[11].",
        "prediction": "Some sports have a culture that encourages drinking a lot of alcohol after training and games. Athletes in team sports are especially at risk of binge drinking.",
        "reference": "Several studies showed that athletes are more likely to drink alcohol to excess (50-65% drink above dangerous level).",
        "metrics": {
          "SARI": 37.07671286905567,
          "BLEU": 0,
          "FKGL": 7.4
        }
      },
      {
        "source": "The outcomes of binge drinking after exercise are likely to include the direct effect of alcohol on physiological processes as well as the indirect effect on the athlete's recovery due to not eating or resting adequately as a result of intoxication.",
        "prediction": "Binge drinking after exercise can harm the body. Alcohol directly affects how our body works. It can also slow down recovery for athletes. This happens because they might not eat or rest enough when they are drunk.",
        "reference": "Consequences of an excessive consumption after exercise are direct effect on the body but also indirect effect on the recovery due to an inappropriate eating and rest.",
        "metrics": {
          "SARI": 34.72665559223867,
          "BLEU": 0,
          "FKGL": 4.6
        }
      },
      {
        "source": "Although the concurrent consumption of carbohydrate can partially offset the deleterious effects of alcohol intake on post-exercise glycogen resynthesis [14], the effect of alcohol consumption on muscle protein synthesis is unknown.",
        "prediction": "Drinking carbohydrates at the same time as alcohol can help reduce some harmful effects of alcohol on glycogen recovery after exercise. However, we do not know how alcohol affects muscle protein building.",
        "reference": "Consumption of carbohydrates can partially counter balance the effects of alcohol on the recharge of glycogen stores but the effect on muscle protein synthesis are unknown.",
        "metrics": {
          "SARI": 42.71060661013619,
          "BLEU": 0.09147827112247602,
          "FKGL": 11.9
        }
      },
      {
        "source": "The aim of the current study was to determine the effect of alcohol intake on anabolic cell signaling and rates of myofibrillar protein synthesis (MPS) in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.",
        "prediction": "This study aimed to find out how alcohol intake affects anabolic cell signaling and myofibrillar protein synthesis in humans. This was measured during recovery after hard exercise similar to what athletes feel during training for team sports like football, rugby, and court sports.",
        "reference": "The aim of this study was to determine the effect of alcohol on anabolism and protein synthesis in humans during recovery following an effort an athlete could do (in rugby or football for example).",
        "metrics": {
          "SARI": 34.57102723584897,
          "BLEU": 0.07276375309803214,
          "FKGL": 12.9
        }
      },
      {
        "source": "Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the age-associated loss of muscle mass and is one of the main contributors to musculoskeletal impairments in the elderly. Previous research has demonstrated that resistance training can attenuate skeletal muscle function deficits in older adults, however few articles have focused on the effects of resistance training on functional mobility. The purpose of this systematic review was to 1) present the current state of literature regarding the effects of resistance training on functional mobility outcomes for older adults with skeletal muscle function deficits and 2) provide clinicians with practical guidelines that can be used with seniors during resistance training, or to encourage exercise. We set forth evidence that resistance training can attenuate age-related changes in functional mobility, including improvements in gait speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in progressive resistance training activities, and should be admonished to move along a continuum of exercise from immobility, toward the recommended daily amounts of activity.",
        "prediction": "Human aging causes changes in skeletal muscle. Sarcopenia is the loss of muscle mass with age and leads to problems in movement for older people. Research shows that resistance training helps improve muscle function in older adults. However, not many studies look at how resistance training affects movement ability. This review aims to 1) share what we know about resistance training and movement outcomes for older adults with muscle issues and 2) give useful guidelines for doctors to help seniors with resistance training and exercise. We found that resistance training can help reduce age-related changes in movement ability. This includes better walking speed, balance, and lower risk of falls. Older adults should be encouraged to do progressive resistance training and increase their activity level over time.",
        "reference": "Human aging results in a variety of changes to muscles. Sarcopenia is the age-associated muscle loss and is one of the main contributors to muscle disorders in older adults. Previous research has demonstrated that strength training can attenuate muscle function deficits in older adults, however few articles have focused on the effects of strength training on mobility. The purpose of this review was to present the effects of strength training on mobility for older adults with muscle function deficits and to provide guidelines that can be used with seniors during strength training. We present evidence that strength training can attenuate age-related changes in mobility, including improvements in walking speed, static and dynamic balance, and fall risk reduction. Older adults should be encouraged to participate in strength training activities, and should avoid immobility.",
        "metrics": {
          "SARI": 30.148742059342876,
          "BLEU": 0.15166307362183892,
          "FKGL": 9.5
        }
      },
      {
        "source": "Maximizing the hypertrophic response to resistance training (RT) is thought to be best achieved by proper manipulation of exercise program variables including exercise selection, exercise order, length of rest intervals, intensity of maximal load, and training volume. An often overlooked variable that also may impact muscle growth is repetition duration. Duration amounts to the sum total of the concentric, eccentric, and isometric components of a repetition, and is predicated on the tempo at which the repetition is performed. We conducted a systematic review and meta-analysis to determine whether alterations in repetition duration can amplify the hypertrophic response to RT. Results indicate that hypertrophic outcomes are similar when training with repetition durations ranging from 0.5 to 8 s. From a practical standpoint it would seem that a fairly wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at volitionally very slow durations (>10s per repetition) is inferior from a hypertrophy standpoint, although a lack of controlled studies on the topic makes it difficult to draw definitive conclusions.",
        "prediction": "Maximizing muscle growth from resistance training is best done by adjusting key exercise factors. These factors include choosing exercises, the order they are done, rest time, how much weight to lift, and total training volume. One important factor that is often missed is the duration of each repetition. This refers to how long each part of the movement takes and depends on the speed of the rep. We did a thorough review and analysis to see if changing the duration of reps can improve muscle growth. Our results show that muscle growth is similar with rep durations from 0.5 to 8 seconds. This means you can use a variety of rep durations to grow muscles. However, training with very slow repetitions lasting more than 10 seconds is less effective for muscle growth. There is not enough controlled research on this topic to make strong conclusions.",
        "reference": "Maximizing the muscle growth following strength training can be done by manipulating variables like exercise selection, exercise order, rest intervals, intensity of maximal load, and training volume (number of exercises, sets, etc.). A neglected variable that also may impact muscle growth is repetition duration. Total duration of a repetition is the sum of the contraction, the lengthening and isometric (static contraction) and is based on the tempo. We conducted a review and analysis of multiple studies to determine if modification of repetition duration can amplify the muscle growth. Results indicate that muscle growth is similar when training with repetition durations from 0.5 to 8 s. Using a wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at very slow durations (>10s per repetition) is less efficient for muscle growth, although a lack of studies on the topic makes it difficult to draw definitive conclusions.",
        "metrics": {
          "SARI": 29.196944677831954,
          "BLEU": 0.10058364982431188,
          "FKGL": 7.8
        }
      },
      {
        "source": "Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also refers to specific physiological processes or states occurring after exercise that are distinct from the physiology of either the exercising or the resting states. In this context, recovery of the cardiovascular system after exercise occurs across a period of minutes to hours, during which many characteristics of the system, even how it is controlled, change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. Furthermore, some of these changes may provide insight into when the cardiovascular system has recovered from prior training and is physiologically ready for additional training stress. This review focuses on the most consistently observed hemodynamic adjustments and the underlying causes that drive cardiovascular recovery and will highlight how they differ following resistance and aerobic exercise. Primary emphasis will be placed on the hypotensive effect of aerobic and resistance exercise and associated mechanisms that have clinical relevance, but if left unchecked, can progress to symptomatic hypotension and syncope. Finally, we focus on the practical application of this information to strategies to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities of this state. We will explore appropriate field measures, and discuss to what extent these can guide an athlete's training.",
        "prediction": "Recovery from exercise is the time it takes to go from working out to resting. It involves changes in the body that are different from when you are exercising or fully at rest. After exercise, the cardiovascular system takes minutes to hours to recover. During this time, many aspects of the system change. Some of these changes help the body adapt to regular exercise, while others can cause problems. These changes can show when the heart and blood vessels are ready for more training. This review looks at how the heart and blood vessels adjust after exercise and why these changes happen. It will also explain how recovery differs between resistance training and aerobic exercise. We will focus on how aerobic and resistance exercise can lower blood pressure and the risks involved. If not managed, this can lead to low blood pressure and fainting. Finally, we will discuss how to use this information in practice to improve recovery or reduce risks. We will look at useful measures and how they can aid an athlete's training.",
        "reference": "Recovery from exercise refers to the time period between the end of exercise and the return to a resting or recovered state. It also refers to specific physiological processes occurring after exercise. Recovery of the cardiovascular system occurs across a period of minutes to hours, during which many characteristics of the system change over time. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. These changes may provide insight into when the cardiovascular system has recovered and is physiologically ready for additional training. This review focuses on the most consistently observed blood flow adjustments and the causes that drive cardiovascular recovery and how they differ following strength and cardio exercise. First we focus on the effect leading to low blood pressure of cardio and strength exercise and associated mechanisms, which can progress to symptomatic hypotension (low blood pressure) and fainting. Finally, we focus on the practical application to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities. We will explore measures, and discuss how these can guide an athlete's training.",
        "metrics": {
          "SARI": 29.016594835721758,
          "BLEU": 0.15701356258083052,
          "FKGL": 7.8
        }
      },
      {
        "source": "Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed \"exercise as medicine\". Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often overlooked as a target for exercise-based therapy. Despite the vast range of severity in kidney disease (e.g., pre-dialysis, dialysis, transplant), exercise has a potential role in all patients suffering from the condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'.",
        "prediction": "Exercise and physical activity are important for treating and preventing medical problems like arthritis and diabetes. This idea is known as \"exercise as medicine\". Exercise helps lower the risk of heart disease, reduce inflammation, and improve strength and breathing. Chronic kidney disease affects about 10% of people but is often ignored for exercise treatment. Even with different stages of kidney disease, such as pre-dialysis and dialysis, exercise can benefit all patients. In this review, we discuss how exercise can help manage kidney disease and how it should be prescribed as treatment.",
        "reference": "Exercise and physical activity are key tools in the treatment and prevention of several medical conditions including joint disorders and diabetes. Exercise can reduce cardiovascular risk, inflammation, illness causing muscle loss, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often neglected as a target for exercise-based therapy. Despite the vast range of severity in kidney disease, exercise has a potential role in all patients suffering from the condition. In this review, we summarise the potential important role of exercise in the management of kidney disease and how this form of 'medicine' should be used.",
        "metrics": {
          "SARI": 26.888545130638313,
          "BLEU": 0.16269739461814417,
          "FKGL": 10.4
        }
      },
      {
        "source": "Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The pooled standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% CI=-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.",
        "prediction": "Blood flow restriction (BFR) is a method used to help people get stronger. Its impact on knee pain and function is not clear. The goal of this study was to see if adding BFR to resistance exercise helps reduce pain and improve function in people with knee pain. We reviewed many studies that compared resistance exercise with and without BFR in people over 18 years old who had knee pain. The results showed that resistance exercises with BFR did not work better than resistance exercises alone for reducing pain or improving knee function in these patients.",
        "reference": "Exercise with blood flow restriction (BFR) is used to increase strength in healthy individuals. However, its effects on individuals with knee pain are unknown. The objective is to determine the effectiveness of adding BFR to strength exercise for pain relief and improvement of function in patients with knee pain. We conducted a review and analysis of multiple studies. Studies that compared strength exercise with or without BFR to treat knee pain and function. The results showed that resistance exercises with BFR was not more effective than strength exercises for reducing pain and improving knee function in patients with knee pain.",
        "metrics": {
          "SARI": 41.01620532695133,
          "BLEU": 0.26478695624864773,
          "FKGL": 8.4
        }
      },
      {
        "source": "In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports). In this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review recommends safe and effective whole-body aerobic exercise at moderate intensity (i.e., 50-65% of maximum oxygen intake, 30-60 min per session, 3-4 times a week) that primarily focuses on the major muscle groups for the prevention and treatment of hypertension. Resistance exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but resistance exercise is contraindicated in patients with hypertension who have chest symptoms such as chest pain.",
        "prediction": "In Japan, about 43 million people had high blood pressure in 2010. Managing this condition is very important for health control, and lifestyle changes are key for prevention and treatment. There is a focus on increasing daily activities and exercise. This review looks at exercise recommendations for preventing and treating high blood pressure from both Japanese and foreign studies. It suggests safe aerobic exercise at moderate levels, which means 50-65% of the maximum oxygen intake. This should be done for 30-60 minutes per session, 3-4 times a week, targeting major muscle groups. Resistance training should be low-intensity and not involve holding your breath. However, resistance training is not safe for patients with high blood pressure who have chest pains.",
        "reference": "In Japan, 43 million patients had high blood pressure in 2010. The management of this condition and the importance of lifestyle changes for the prevention and treatment of high blood pressure has been recognized in Japan. In particular, increasing the levels of activities of daily living and physical exercise. In this review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of high blood pressure. This review recommends full body cardio exercise at moderate intensity (i.e., 50-65% of maximum intensity, 30-60 min per session, 3-4 times a week) that focuses on the major muscle groups. Strength exercise should be performed at low-intensity without breath-holding and should be used as supplementary exercise, but it is contraindicated in patients with high blood pressure who have chest symptoms such as chest pain.",
        "metrics": {
          "SARI": 29.23427096736928,
          "BLEU": 0.2208445363529957,
          "FKGL": 9.1
        }
      },
      {
        "source": "Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common sense advices call for late inception of intense, strength training-related activities, like weight lifting and plyometrics, which are usually postponed at the end of the growth age, even among sport practitioners. However, such advices seem to have a mainly anecdotal nature. Current literature does not seem to have any particular aversion against the practice of strength training by children and adolescents, provided that some safety rules are followed, like medical clearance, proper instruction from a qualified professional and progressive overload. At the same time, several studies provide consistent findings supporting the benefits of repeated, intense physical efforts in young subjects. Improved motor skills and body composition, in terms of increased fat free mass, reduced fat mass and enhanced bone health, have been extensively documented, especially if sport practice began early, when the subjects were pubescent. It can be therefore concluded that strength training is a relatively safe and healthy practice for children and adolescents.",
        "prediction": "Physical activity helps prevent diseases and improve health. Common advice says to start intense strength training, like weight lifting, later in life. This is often suggested even for athletes. However, this advice is mostly based on personal experience. Current research does not show strong opposition to strength training for children and teens if safety rules are followed. These include getting a doctor's approval, having proper training from a qualified coach, and gradually increasing the intensity. Many studies show that repeated intense physical activity can benefit young people. It can improve motor skills and body composition, leading to more muscle and less fat, and better bone health. This is especially true if kids start training early during puberty. Thus, strength training is generally safe and healthy for children and teens.",
        "reference": "Physical activity has many benefits like preventing diseases and improving general health. In popular opinion, it is recommended to start intense physical activities like weight lifting and plyometrics (exercises like sprint and jumps) after adolescence. But this advice does not seem to be based on facts. We did not find any recommendation against strength training at a young age if it is well supervised with safety rules, medical clearance, proper instruction from a professional and progressive overload (gradual increase of stress on muscle and nervous system). Several studies showed many benefits of repeated, intense physical efforts in young people. Improved motor skills and body composition (muscle mass increased, fat mass decreased, healthier bone) were found in studies, especially if sport practice began during puberty. We can conclude that strength training is a safe and healthy practice for children and adolescents.",
        "metrics": {
          "SARI": 40.38416220610936,
          "BLEU": 0.09145817130578161,
          "FKGL": 9.5
        }
      },
      {
        "source": "recently, there has been a renewed public interest in IFast. Given the importance of nutrition in optimizing athletic performance, there is a concern about the effects of IFast on athletics. Looking at high-intensity, endurance, and resistance exercises, studies have been varied but are uniform in showing that there is no benefit to athletic performance while fasting.",
        "prediction": "Recently, more people are interested in IFast. Nutrition is important for athletes. There are concerns about how IFast affects athletic performance. Studies on high-intensity, endurance, and resistance exercises show mixed results. However, they all agree that fasting does not help athletic performance.",
        "reference": "Nowadays, people are interested in intermittent fasting (an eating pattern where you cycle between periods of eating and fasting) Nutrition is important for the optimization of sport performance so there is a concern about the effects of intermittent fasting on performance. Studies showed that there are no benefit in high-intensity, endurance or strength training while fasting.",
        "metrics": {
          "SARI": 39.15545337618546,
          "BLEU": 0.11804878926485186,
          "FKGL": 10.1
        }
      },
      {
        "source": "Nearly every physically active person encounters periods in which the time available for exercise is limited (e.g., personal, family, or business conflicts). During such periods, the goal of physical training may be to simply maintain (rather than improve) physical performance. Similarly, certain special populations may desire to maintain performance for prolonged periods, namely athletes (during the competitive season and off-season) and military personnel (during deployment). In general populations, endurance performance can be maintained for up to 15 weeks when training frequency is reduced to as little as 2 sessions per week or when exercise volume is reduced by 33–66% (as low as 13–26 minutes per session), as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size (at least in younger populations) can be maintained for up to 32 weeks with as little as 1 session of strength training per week and 1 set per exercise, as long as exercise intensity (relative load) is maintained; whereas, in older populations, maintaining muscle size may require up to 2 sessions per week and 2–3 sets per exercise, while maintaining exercise intensity. Our primary conclusion is that exercise intensity seems to be the key variable for maintaining physical performance over time, despite relatively large reductions in exercise frequency and volume.",
        "prediction": "Almost everyone who exercises will face times when they cannot work out much due to personal, family, or work issues. During these times, the goal of training is to keep physical ability steady instead of getting better. Some groups, like athletes in sports seasons and military members during deployments, also want to maintain their performance for longer amounts of time. Studies show that regular people can keep their endurance for up to 15 weeks if they cut down training to 2 times a week or reduce workout time by 33-66%, which is about 13-26 minutes each time, as long as they keep the exercise intensity high. Younger people can keep strength and muscle size for up to 32 weeks with just 1 strength training session per week and 1 set of each exercise, as long as they maintain the intensity. Older adults may need 2 sessions per week and 2-3 sets per exercise to keep their muscle size, while also keeping the intensity. The main takeaway is that exercise intensity is essential for maintaining physical performance over time, even with less frequent workouts.",
        "reference": "Physically active people encounter periods with limited time available for exercise. During such periods, the goal of training may be to simply maintain physical performance. Some people may also desire to maintain performance for prolonged periods, like athletes during competitive season or militaries during deployment. Endurance performance can be maintained for up to 15 weeks with 2 trainings per week, or with a reduction of 13–26 minutes per training, as long as exercise intensity (exercising heart rate) is maintained. Strength and muscle size can be maintained for up to 32 weeks with 1 strength training per week and 1 set per exercise, as long as exercise intensity (load) is maintained. In older populations, maintaining muscle size may require up to 2 trainings per week and 2–3 sets per exercise, while maintaining exercise intensity. Exercise intensity seems to be the key variable for maintaining physical performance, even with reduction of frequency and volume (number of exercises, sets, etc.)",
        "metrics": {
          "SARI": 38.14607697849255,
          "BLEU": 0.17886370249640363,
          "FKGL": 12.3
        }
      },
      {
        "source": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients--primarily protein and carbohydrate--in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been postulated that the timing of nutritional consumption may be more important than the absolute daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. Theoretically, consuming the proper ratio of nutrients during this time not only initiates the rebuilding of damaged muscle tissue and restoration of energy reserves, but it does so in a supercompensated fashion that enhances both body composition and exercise performance. Several researchers have made reference to an anabolic “window of opportunity” whereby a limited time exists after training to optimize training-related muscular adaptations. However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Not only is nutrient timing research open to question in terms of applicability, but recent evidence has directly challenged the classical view of the relevance of post-exercise nutritional intake with respect to anabolism.",
        "prediction": "Nutrient timing is a way to eat that focuses on when you take in nutrients, mainly protein and carbs, around exercise. Some people believe this can greatly improve body composition. They even think that when you eat may matter more than how much you eat each day. The time after exercise is seen as the most important for nutrient timing. Eating the right mix of nutrients then can help repair muscle and restore energy in a better way. This can improve both body composition and exercise performance. Many researchers talk about an anabolic \"window of opportunity\" which is a short time after training to make the most of muscle changes. However, how important this post-exercise window is can depend on different factors. There is still debate about nutrient timing in research, and new evidence challenges the idea that eating after exercise is crucial for muscle growth.",
        "reference": "Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients around an exercise session. Some people say that this approach can produce dramatic improvements in body composition. It has even been hypothesized that the timing may be more important than the total daily intake of nutrients. The post-exercise period is considered the most important part of nutrient timing. In theory, consuming the proper ratio of nutrients during this time allows the rebuilding of damaged muscle tissue and restoration of energy reserves, but also enhances both body composition and exercise performance. Researchers have made reference to an anabolic “window of opportunity” (a limited time after training to optimize muscular adaptations). However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Research is uncertain about the applicability of nutrient timing. Besides, the relevance of post-exercise nutritional intake concerning anabolism (increasing muscle mass) has been challenged by recent evidence.",
        "metrics": {
          "SARI": 26.409523101109617,
          "BLEU": 0.11571204772316376,
          "FKGL": 9.6
        }
      },
      {
        "source": "Lack of time is among the more commonly reported barriers for abstention from exercise programs. The aim of this review was to determine how strength training can be most effectively carried out in a time-efficient manner by critically evaluating research on acute training variables, advanced training techniques, and the need for warm-up and stretching. When programming strength training for optimum time-efficiency we recommend prioritizing bilateral, multi-joint exercises that include full dynamic movements (i.e. both eccentric and concentric muscle actions), and to perform a minimum of one leg pressing exercise (e.g. squats), one upper-body pulling exercise (e.g. pull-up) and one upper-body pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights based on training goals, availability, and personal preferences. Weekly training volume is more important than training frequency and we recommend performing a minimum of 4 weekly sets per muscle group using a 6–15 RM loading range (15–40 repetitions can be used if training is performed to volitional failure). Advanced training techniques, such as supersets, drop sets and rest-pause training roughly halves training time compared to traditional training, while maintaining training volume. However, these methods are probably better at inducing hypertrophy than muscular strength, and more research is needed on longitudinal training effects. Finally, we advise restricting the warm-up to exercise-specific warm-ups, and only prioritize stretching if the goal of training is to increase flexibility.",
        "prediction": "Many people say they don't have enough time to exercise. This review looks at how to do strength training efficiently. We check research on quick training methods, advanced techniques, and warm-up needs. \n\nTo make strength training quick, focus on exercises that work multiple joints and muscles at once. You should include at least one leg exercise like squats, one upper-body pulling exercise like pull-ups, and one upper-body pushing exercise like bench presses. You can use machines or free weights based on your goals and what you like.\n\nTraining volume matters more than how often you train. Aim for at least 4 sets per muscle group each week, using a weight that allows for 6-15 reps. You can do 15-40 reps if you go to muscle failure. \n\nUsing advanced methods like supersets, drop sets, and rest-pause training can cut training time by half while keeping the same volume. These techniques may help increase muscle size better than strength, and we need more studies to understand the long-term effects.\n\nFor warm-ups, keep them specific to the exercises you’ll do. Stretching is only important if your goal is to gain flexibility.",
        "reference": "Lack of time is one of the most common reason for not exercising. The aim of this review was to determine how strength training can be time-efficient by evaluating research on training variables, advanced training techniques, and the need for warm-up and stretching. We recommend choosing bilateral (with the two arms or legs), multi-joint (multiple joints involved) exercises that include full dynamic movements and to perform a minimum of one leg pressing exercise (e.g. squats), one pulling exercise (e.g. pull-up) and one pushing exercise (e.g. bench press). Exercises can be performed with machines and/or free weights. Weekly training volume (number of exercises, sets, etc.) is more important than training frequency and we recommend a minimum of 4 weekly sets per muscle group using a load that permits to do 6 to 15 repetitions. Advanced training techniques like combining exercises with no rest or shortening the rest time while using lighter weights can cuts the training time in half, while maintaining training volume. However, these methods are probably better to increase muscle mass than strength. Finally, we advise exercise-specific warm-ups, and stretching only if the goal of training is to increase flexibility.",
        "metrics": {
          "SARI": 29.068665022111666,
          "BLEU": 0.04727705595990309,
          "FKGL": 6.6
        }
      },
      {
        "source": "Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequency on a volume-equated basis. Meta-regression analysis of non-volume-equated studies showed a significant effect favoring higher frequencies, although the overall difference in magnitude of effect between frequencies of 1 and 3+ days per week was modest. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "prediction": "Training frequency is important for muscle growth from resistance exercise. This study looked at many experiments to find out how weekly training frequency affects muscle growth. The results showed no major difference between high and low frequency when the amount of work was the same. A special analysis of some studies suggested that higher frequency might have a slight benefit, but the overall difference between training 1 day a week and 3 or more days was small. In conclusion, there is strong evidence that how often you train does not greatly change muscle size when the total workout amount is equal. So, people can choose how often to train a muscle group each week based on what they prefer.",
        "reference": "Training frequency is an important variable for hypertrophy (increasing muscle mass) in strength training. The purpose of this paper was to conduct a review and analysis of multiple studies investigating the effects of weekly training frequency on hypertrophy. Results showed no significant difference between higher and lower frequency with the same training volume (number of exercises, sets, etc.). Analysis of studies with different training volumes showed better results for higher frequencies, although the difference between frequencies of 1 and 3+ days per week was small. In conclusion, there is strong evidence that resistance training frequency does not significantly impact muscle hypertrophy when training volume is the same. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.",
        "metrics": {
          "SARI": 29.910684468675825,
          "BLEU": 0.14124300475199053,
          "FKGL": 9.8
        }
      },
      {
        "source": "A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Forced repetitions/drop sets, supersets, and heavy negatives, in particular, have been purported to enhance the hypertrophic response to resistance exercise. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to resistance training programs.",
        "prediction": "Many training techniques can help increase muscle growth. Some of these are forced repetitions, drop sets, supersets, and heavy negatives. These methods may improve muscle size from resistance exercise. This article will look at how these techniques can help muscle growth and how to use them in resistance training programs.",
        "reference": "A variety of training techniques have been suggested to support muscle growth. Forced repetitions/drop sets (use lighter weight or receiving help to continue the set), supersets (combining exercises with no rest), and heavy negatives (lowering phase of the lift), are pretended to enhance hypertrophy (increasing muscle mass). This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to strength training.",
        "metrics": {
          "SARI": 29.264224376711496,
          "BLEU": 0.08550518609889807,
          "FKGL": 7
        }
      },
      {
        "source": "The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. Although both groups are known to display impressive muscularity, it is not clear which method is superior for hypertrophic gains. It has been shown that many factors mediate the hypertrophic process and that mechanical tension, muscle damage, and metabolic stress all can play a role in exercise-induced muscle growth. Therefore, the purpose of this paper is twofold: (a) to extensively review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and (b) to draw conclusions from the research as to the optimal protocol for maximizing muscle growth.",
        "prediction": "Many people who lift weights want to increase lean body mass. But there isn’t much research on the best way to build muscle. Bodybuilders usually train with moderate weights and short breaks. This creates a lot of metabolic stress. Powerlifters train with heavy weights and take long breaks. Both types of lifters have strong muscles, but it is unclear which training method builds more muscle. Many factors affect muscle growth. Mechanical tension, muscle damage, and metabolic stress all play a role. This paper has two main goals: to review the research on how muscles grow and to find the best training method for muscle growth.",
        "reference": "Increasing muscle mass is the goal of many people who lift weights. Research is lacking, however, as to the optimal way to induce muscle growth. Bodybuilders train with moderate loads and short rest intervals. Powerlifters, on the other hand, train with high-intensity loads and lengthy rest periods. Although both groups are known to display impressive muscularity, it is not clear which method is superior for increasing muscle mass. It has been shown that many factors mediate the process of muscle growth and that mechanical tension, muscle damage, and metabolic stress all can play a role in muscle growth. Therefore, the purpose of this paper is to review the literature as to the mechanisms of muscle hypertrophy and their application to exercise training and to draw conclusions from the research as to the optimal way to induce muscle growth.",
        "metrics": {
          "SARI": 27.14060566822221,
          "BLEU": 0.1339892176528543,
          "FKGL": 4.3
        }
      }
    ]
  ]
}