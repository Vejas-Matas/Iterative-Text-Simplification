This innovative mobile app helps manage wildfires and works alongside a web-based platform to prevent them. Novel technological advances in mobile devices and applications can be exploited in fighting wildfires, enabling end-users to easily conduct several everyday tasks, such as access to data and information, sharing of intelligence and coordination of personnel and vehicles. Several tasks can be accomplished from the AEGIS App, such as routing, spatial search for closest facilities and firefighting support infrastructures, access to weather data and visualization of fire management data (water sources, gas refill stations, evacuation sites etc.). An innovative feature of AEGIS App is the support of these tasks by a digital assistant for artificial intelligence named Cortana (developed by Microsoft for Windows Phone devices), that allows information utilization through voice commands. The application is to be used by firefighting personnel in Greece and is potentially expected to contribute towards a more sophisticated transferring of information and knowledge between wildfire control centers and firefighting units in the field.
In recent years, mobile technologies have developed and applied in education fields, and some mobile emerging carriers with mobile technologies include a personal digital assistant (PDA), smart phone, and e-book. Some of the mobile carriers use technology to connect to the internet and provide new learning experiences that are different from the past learning. In view of the application of mobile technology in education, some previous studies have addressed that mobile learning is a meaningful learning that can improve the interaction between students and the situations and reach the purposes of learning. In the main stream of mobile learning, using mobile carriers with suitable learning methods or strategies in mobile learning activities for different students to enhance learning have gradually become a important and concern issue. The purpose of this study is to investigate the learning
This paper proposes an electrophysiological wireless patient monitoring system which integrates a Wireless ECG signal transmitter, GPS device and a mobile phone to acquire physiological signals and transmit them to a local server via Bluetooth wireless technology. Four types of monitor units were designed for wireless communication: a control center, a local monitor, mobile devices, and a Web page for both patients and doctors. The use of various monitor units is created to fulfill different medical personnel requirements and wishes. This application was developed to promote the mobility and flexibility for the patients and also for the medical personnel, which further will improve both the quality of health care and lifestyle of the patient.
As various kinds of output devices emerged, such as high-resolution devices, the importance of high-quality resolution conversion has been increasing. This paper proposes a new method for enlarging image with high quality. Images often look pixelated when enlarged. We propose a new interpolation method, using a special computer algorithm to determine the best values for the new pixels. The results are presented and discussed. The effectiveness of our methods is discussed by comparing with the conventional methods.
A significative percentage of the human population suffer from impairments in their capacity to distinguish or even see colours. Navigating through public transportation can be challenging for people with colour vision impairments. We present a novel technique for extracting colour information from everyday natural stimuli and presenting it to visually impaired users as pleasant, non-invasive sound. Colour information is extracted from images and grouped into categories based on how people perceive colour. This information is subsequently converted into sound and sent to the user via speakers or headphones. In the original implementation, it is possible for the user to send its feedback to reconfigure the system, however several features such as these were not implemented because the current technology is limited.We are confident that the full implementation will be possible in the near future as PDA technology improves.
Inappropriate use of medicines increases the risk of hospital admissions for the elderly. Not only does this lead to unnecessary suffering for the patients but also incurs a great financial cost to the society. A medicine decision support system in a PDA can help track patients' medicines and warn against bad combinations. Our goal was to see if a mobile medicine system helps nurses in home care. The participants received a comprehensive overview from the patients' medicine use and noted drug-drug interactions, therapeutic duplications and warnings for drugs unsuitable for elderly people. The nurses found the decision support system helpful and safe. Our findings suggest that most of the content and functions were regarded as important. Therefore, this decision support system might be a useful tool for district nurses.
:[0],"access to patient appointment schedules can help clinicians manage time and problems better. Many large healthcare organizations use systems to manage appointment schedules, but these systems are often hard for clinicians to access. Many a clinician today uses a personal digital assistant (PDA) containing basic calendaring functionality. Our “PalmOversite” project demonstrates the feasibility of integrating enterprise appointment schedule information into a PDA calendar, making schedule information much more readily available to the clinician.
Partners In Health (PIH) and its sister organization in Lima, Peru, Socios En Salud (SES), treat a majority of multidrug-resistant tuberculosis (MDR-TB) patients in Peru, in conjunction with the Peruvian National TB Program (NTP). Monthly bacteriology tests, which must be collected from health establishments located across this major city, are an integral part of this treatment. We developed a new electronic system to help collect and process data from health establishments. After an initial implementation period, we performed a pilot study to test the use of this system. We conducted a baseline assessment in two districts and then implemented the electronic system in one district, keeping the other district on the old system. The PDA-based system processed data in 6.2 days, which is faster than the baseline (54.8 days) and control sites (64.4 days). It was also able to reduce the frequency of discrepancy from 10.1% to 2.8% and receive positive feedback from the users. Finally, the system’s cost would be recuperated in three months from time savings due to increased work efficiency. This system will be the subject of a larger study to determine its impact on delays, errors and costs.
In order to facilitate knowledge transfer between specialists and generalists and between experts and novices, and to promote interdisciplinary communication, there is a need to provide methods and tools for doing so. This interdisciplinary research team developed and evaluated a decision support tool (DST) on a personal digital assistant (PDA) for cardiac tele-triage/tele-consultation when the presenting problem was chest pain. They used a combination of two methods to design the decision support tool: one to understand how users interact with the system and another to create an intuitive interface. A pilot clinical trial was conducted at a quaternary cardiac care hospital over a 3-month period. During this time, the DST was used by the nine nursing coordinators who provide tele-triage/tele-consultation 24/7. This clinical trial validated the design and demonstrated its usefulness to advanced cardiac care nurses, its potential for use by nurses less experienced in cardiac care, and for its potential use in an interdisciplinary team environment.
This paper presents M-CALL, a mobile computer-assisted language learning courseware for Korean language learners. Since conventional computer-assisted language learning is often boring, it exploits a cyber pet game to increase the learner’s interest. M-CALL runs on a personal digital assistant with public wireless LAN for mobile learning. It grows a virtual pet by learning Korean. Korean Proficiency Test (KPT), a nationally certified Korean language test, was used as problem sets. It consists of cyber pet game, mobile learning courseware, mobile learning system, and mobile tutoring. It provides various functions for Korean language learning. Currently, the prototype M-CALL was designed and partly implemented between mobile PDA and personal computer.
This study compared text entry performance of two stylus-driven soft keyboards for use in hand-held computing devices: the QWERTY and the T9. Participants typed text on a computer using a hand-held device and a stylus and one of these two keyboards. We found a way to measure typing speed. Using this technique, we calculated the maximum transcription rate for each keyboard. The results show that transcription rates were higher for the QWERTY keyboard than for the T9, despite the T9 keyboard’s apparent superior physical characteristics. An ancillary experiment demonstrated that the poorer performance of the T9 layout may have resulted from an increase in visual scanning time due to perceptual grouping of the letters on the keys. Together these findings imply that the QWERTY keyboard layout remains the most effective of the currently available designs for stylus tapping on soft keyboards.
The proliferation of mobile devices such as smart phones and Personal Digital Assistant (PDA) opens new ways for developing mobile E-commerce systems (so called Mobile-commerce or M-commerce system). M-commerce systems use many of the same components as E-commerce applications, such as web servers and databases. However, these new applications raise some unique challenges. For example, the limitations of mobile devices, such as small screens and slow processors, require software development to be different from desktop applications. When designing M-commerce systems, several questions arise, such as how users access information on mobile devices and how to create useful applications. In this paper, we discuss the software development issues for mobile commerce systems based on our experience with a tourist M-commerce system. Our prototype, easyHotel, is an useful software that allows booking hotel rooms via mobile phones.
It is widely acknowledged that information such as web content should be adapted for mobile platforms to account for restrictions in mobile environments. As various mobile platforms, such as smartphones and tablets tend to vary largely in their capabilities, we suggest that adaptation should be platform-specific. Common approaches for content adaptation are automated conversion and explicit specification of adapted content, with a trade-off between quality and development/maintenance effort. As alternative avoiding this trade-off, we propose a simple object-oriented framework for content adaptation. To facilitate the use of this framework in the Web, we base our approach on the object-oriented WebComposition model and its XML-based implementation WCML. We apply our object-oriented approach to an example application to demonstrate how object-oriented specification of platform-adapted content reduces development/maintenance effort.
In monitoring a patient's real-time vital signs through Body Area Networks (BAN), rich data sources are communicated to medical practitioners. It is essential that data is delivered in timely-context aware manner. In this paper a system is designed for patients with cardiac disorders, with an emphasis on the design of the sensing device and communication scheme chosen. Several existing wearable physiological devices (Patient Sensing Device — PSD) used in the Healthcare systems are bulky in design and are not flexible and comfortable for the elderly patients. Presented is a unique flexible, as well as detachable PSD for the comfort of patients. Also discussed is a model for transmission, storage and processing of patient related data, which ensures periodic logging of patient data without saturating communication networks. A proof of concept prototype has been developed and implemented to enable transmission of Electrocardiogram (ECG) signal and body temperature of a patient, which can be expanded to include other vital signs. Communication between a mobile smart-phone and the ECG and temperature acquisition apparatus is implemented Bluetooth. The Data Management System is designed to manage data from the patient's sensors and store it in the medical provider's database using a smartphone or personal digital assistant connected to the GSM network.
Advancement of wireless and mobile technology has enabled additional platforms to support learning in a single learning space. Current trend integrates usage of mobile terminals such as smart PDA (personal digital assistant) in a learning system. Such devices can communicate with wireless mobile terminal and realize the intelligence exchange with its surroundings [1]. A system called One Day Trip helps foreign students learn Japanese in real life situations using a personal digital assistant. This paper proposes using Knowledge Management concepts to support teachers and learners in the learning environment.
In recent years, we have developed applications for teaching both wired and wireless networking. These applications have been written for the Cybiko PDA, an inexpensive alternative to other hardware. Unfortunately, the Cybiko PDA was recently discontinued. As a result, these applications had to be ported to other platforms to remain useful. Instead of porting each application to individual platforms, which would prove to be extremely time consuming because of other technical details, we created the messaging layer. This layer abstracts the networking structure and interface, allowing these apps needed to be moved to other devices with as few changes as possible.
Unlike many other classes of hardware, smartcards do not have the ability to communicate securely with the user. Deprived of means to keep the owner informed, the positive properties of smartcards are difficult to utilize. We explore the area at the border between smartcards and other, more powerful (and thus more useful), machines. On the other side of this border we find the Personal Digital Assistant :[66],"our view, to be useful as an extension of the user's private sphere, a machine must at least have enough functionality and resources to create trustworthy digital signatures. A less resourceful machine can only serve as a memory aid, helping the owner remember addresses and phone numbers, which are designed to be tamper-resistant, and as such they seem ideal as a minimal machine. However, trustworthy digital signatures can not be created by smartcards alone, simply because the user does not know what is given to the card for  :[163],"order to be trusted--that is, being able to make trustworthy digital signatures--a smartcard must be supported by some infrastructure outside the card proper. We need to figure out what infrastructure is needed to support smartcards and make digital signatures possible. that smartcards are :[257],"here to stay, finding ways to apply them in constructive manners is prudent.
In this paper we present the main features of a system that we have developed and that provides a new kind of tele-assistance service to elderly people. We rate the assistance as high quality because it can monitor vital signs and alert caregivers to any problems in real time. Therefore, it sends alarms autonomously when  this paper we :[84],"The three main functionalities of AINGERU are: assistance available anywhere and at any time, monitoring of vital signs, and remote monitoring by authorized caregivers. Moreover we show how those functionalities are achieved by making use of PDAs (Personal Digital Assistant), wireless communication, Semantic Web, Web services and agent technologies.
Objective: To evaluate the effectiveness of a personal digital assistant (PDA)–based clinical decision support system (CDSS) on nonsteroidal anti-inflammatory drug (NSAID) prescribing safety in the outpatient :[26],"The design was a randomized, controlled trial conducted in a university-based resident clinic. Internal medicine residents received a PDA-based CDSS suite. For intervention residents, the CDSS included a prediction rule for NSAID-related gastrointestinal risk assessment and treatment recommendations. Patients with musculoskeletal symptoms presented to the doctors without being announced in advance. Safety outcomes were assessed from the prescriptions given to the SPs. Clinicians who did not know which participants were in the intervention or control group reviewed each prescription. :[108],"Prescriptions were judged as safe or unsafe. :[26],"The main outcome measure was the differential change in unsafe prescribing of NSAIDs for the intervention versus the control  :[135],"At baseline, the mean proportion of cases per physician with unsafe prescriptions for the two groups was similar (0.27 vs. 0.29, p u003e 0.05). Intervention participants prescribed more safely than controls after receiving the CDSS. Participants who used the CDSS made fewer mistakes when prescribing NSAIDs than those who didn't.
Recently pedestrian navigation has been studied by many researchers, which provides pedestrians with a similar function of car navigation. On the other hand, a 3D map, a key concept in computer vision, has been utilized as a simulation tool in city and landscape planning, and other engineering fields. The 3D maps will give more intuitive information compared to conventional 2-dimensional (2D) ones. In this paper we first propose pedestrian navigation based on 3D maps, and describe technologies required and its use situations. Secondly we compare effectiveness of 2D and 3D maps for navigation by object search experiments under conditions: 3D maps with and without texture; display sizes corresponding to a mobile phone and a PDA. The experimental results show that 3D maps with texture on mobile phone display size, and 3D maps without texture on PDA display size are superior to 2D maps in search time and error rate. From these results, we can say the effectiveness of 3D maps in pedestrian navigation.
Mobile agents are a competitive concept of client-server computing and are especially suitable in mobile environments, that are characterized by slow internet connection and frequent disconnections to stationary systems. They can be used for information retrieval and information filtering, in which case they evaluate replies and return only the relevant data. Mobile agents as a metaphor of active objects are created on a mobile device such as a Personal Digital Assistant (PDA), will be launched into the information galaxy and are fulfilling the tasks that a mobile user wants to perform on the internet. One transmission channel for these itinerant agents is email. This paper introduces Active M 3 as an example of an active mail framework, which can be regarded as a first approach in designing mobile agents in an interactive way. Active M 3 integrates two known concepts: active mail and multimedia mail.
DVB-H is a technology that allows digital video broadcasting to handheld devices like smartphones and PDAs. This paper deals with the conditions of receiving digital video broadcast type services in mobile handheld devices. The DVB-H standard is introduced and the key technology elements are discussed in some details. The DVB-H terminal and network design are explained in detail.
Summary form only given, as follows. We present a design for a low cost but powerful and high speed communications device powered by IDT's RV4640 as its processing engine. The device can achieve very high speed, thanks to the PCI bus compatible IDT ATM NIC. The device could be used as a two way pager, a set-top box, an Internet terminal, a personal digital assistant or as a video phone. The RV4640 is a low-cost 64-bit processor that runs at a speed of 133 MHz. The RV4640 can be connected to a System Interface Chip which will provide I/O and memory control. The IDT77903 ATM card is a high-speed network interface with a PCI bus connection and costs under $100. Integrating this card into our device as a network interface will make highspeed videoconferencing and multimedia applications easily implementable.
Dr. Who is a Microsoft research project aiming at creating a speech-centric multimodal interaction framework, which serves as the foundation for the NET natural user interface. The paper discusses MiPad's design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface. MiPad is the application prototype that demonstrates compelling user advantages for wireless personal digital assistant (PDA) devices, MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in today's PDAs. Unlike a cellular phone, MiPad avoids speech-only interaction. It incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built-in roller to navigate, the tapping action narrows the number of possible instructions for spoken word understanding. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 machine where speech recognition is performed. The Dr Who speech recognition engine uses a single language model that combines grammar and word patterns. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager.
Little is known about how viable new mobile applications emerge in complex commercial environments. New mobile applications emerge through social interaction between different groups, who work together to understand and solve mobile technology problems, a process that only becomes clear in hindsight. Research in this process suggests that the evolution of new applications is shaped by how different groups interact and define problems and solutions. This theory is illustrated by the early evolution of the personal digital assistant (PDA). According to this view, established technological communities attempt to impose their own definitions of key problems and solutions on new mobile technology, thereby forcing newer players to provide a "total system" that completely redefines a new application area.
In the ubiquitous environment, mobile terminals like personal digital assistant or cellular phone are used to access the Web. We implemented the procedures in a ubiquitous/universal passive viewer (u-PaV) system that transforms Web content into passive viewing content that is especially useful for mobile terminals. u-PaV extracts images and text from Web content and converts it into a format that can be easily displayed on mobile devices. Although the presentation of Web content depends on the terminal type, the Web interface of mobile terminals is designed almost the same as that of personal computers. However, comparing mobile and desktop terminals, some difference in device characteristics, such as screen resolution or sound faithfulness, affect the presentation on the terminal. Moreover, when changing the access style between interactive and passive, the presentation should be adjusted for the terminal type because the lacking information is usually not perceived after transformation. To adjust the play style of Web content to the terminal type, we developed a method for visual presentation. First, using keyword analysis, we represent the emotional aspect of a Web page by adjusting the background color of the screen. Second, we emphasize the subject of the content by the logotype of the keyword. Third, we segment the picture of the content into pieces and present one after another using visual effects. These procedures can be applied to any type of terminal.
We developed a system called "The Beijing Explorer" that lets users share their location and see each other's locations in real time using a handheld device with GPS and Wi-Fi. Users can see their position and their chats on the screen of a PDA using the system real-timely. The system was used for the guidance of the Palace Museum (Forbidden City) in Beijing, China. We carried on experiments two times using the system. The results of experiments show that the service using positioning data and sharing contents were valuable and interesting. The correct positioning information is important for the guidance system.
A truly personal machine, called a private machine and implemented as a Personal Digital Assistant (PDA), is fundamentally different from traditional machines. It is personal and private in an unprecedented manner, and its modus operandi is such that network and power failures will not be rare. Designing distributed systems where PDAs are treated as "first class citizens" is a  private :[59],"assets (electronic money, keys for authentication and opening doors) will be stored in PDAs. Ownership and control of these :[59],"assets and the media that store and communicate them should remain with the user. This must be reflected in the design of systems for private  :[105],"introduce the "open-ended argument" to describe the design strategy we used for designing a system that is designed to reveal information to the user (as opposed to hide it). We argue and show that when systems are designed this way, the user can make better decisions about their data than the system can, as they can evaluate the information provided. The system we have designed and implemented under this design guidelines is presented and discussed.
Depression is under recognized in a variety of pediatric settings. The purpose of this paper is to describe the development and initial evaluation of a personal digital assistant (PDA)-based decision support system (DSS) for pediatric depression screening in ages 8 to 18 years of age by pediatric advanced practice nurse (APN) students. Three aspects are described: selection of depression screening instrument; integration of the instrument into the PDA; and quantitative (usage) and qualitative (focus group) evaluation. Only one third of eligible patients were screened. Twenty percent of those screened were identified as at risk for mood disorder. The main barriers to screening were a lack of time, knowledge, and resources, as well as cultural differences. Suggestions for educational, research, and interventions to integrate clinical based PDA-based screening are discussed.
This study explored the potential of the application of wireless and mobile computing technologies to be used in improving the efficiency of patient care and education and future developments in information and communication technologies to support healthcare professionals and medical students in healthcare research, medical education and training. The study used a thorough review of existing research from online databases, including EMBASE, MEDLINE, and the Cochrane Library. Today, more than 50% of healthcare professionals and medical students are using Personal Digital Assistant with expected growth of more than 75% by year-end 2007. In addition, wireless and mobile computing technologies allows Personal Digital Assistant to connect directly to networks or the Internet. Studies relating to processes of patient care and should evaluate mobile computing technologies as a potential timesaving tool. Wireless and mobile computing technologies is only beginning to take its first step in improving patient care and education. They have shown a positive impact on patient safety, health care efficiency, and ultimately patient satisfaction.
Nursing students can learn many things through practical training by experiencing actual medical practice and by coming in contact with patients. Therefore practical training is an effective learning opportunity for developing the practical nursing skills of nursing students. Moreover, at hospitals, which are important training facilities, with regard to medical safety, the use of learning tools that produce electrical waves is not possible. So, we created a learning support environment that facilitates the imagination of nursing techniques, and enables effective preparation, review, and learning at any time and any location using a portable digital assistant (PDA) device for practical training. As described in this paper, we report on the outline of the educational materials named "digital nursing dictionary" that we developed and the evaluation of the practices using it.
Limitation in display size and resolution on mobile devices is one of the main obstacles for wide-spread use of web applications in a wireless environment. Web pages are often too large for a PDA (Personal Digital Assistant) screen to present. The same problem exists for devices with low resolution such as WebTV. Rebuilding web pages for these devices would make things easier, but it would also be very time-consuming because designers would have to customize each page for each device. In this paper, we propose a document segmentation and presentation system. The system breaks down a web document into smaller sections based on the screen size. Additional information such as overviews and summaries is also extracted to facilitate navigation. The system presents the segments and structural information of a web document to make full use of the screen for information finding.
Take 8-bit microcontroller (C8051F005) as the nucleus and study a kind of voltage harmonic monitor device that satisfies the long-range monitor. The harmonic analysis algorithm adopts Fast Fourier Transform (FFT). This device has two work modes: "native" and "remote". It can also communicate with the monitor center through phone, computer, or other devices.
With network and small screen device improvements, such as wireless abilities, increased memory and CPU speeds, users are no longer limited by location when accessing on-line information. We are interested in studying the effect of users switching from a large screen device, such as a desktop or laptop to use the same web page on a small device, in this case a PDA (Personal Digital Assistant). We discuss two common transformation approaches for display of web pages on the small screen: Direct Migration and Gateway. We introduce a new method called Gateway, which helps users navigate web pages on small screens. The users in an initial study prefer using the Gateway and Direct Migration approach for web pages previously used on the large screen, despite the common Linear approach used by many web sites.
The limitations and constraints of mobile systems need to be adequately addressed in software development. We have developed a system to categorize risks, using a well-established risk assessment tool, and applied it to a project that helps with negotiations for a handheld device. In our research, we will look at how to improve risk management by combining existing strategies with Agile techniques.
Mobile services and private data have required increased level of protection. Speaker recognition, a biometric technology, has many researchers interested in it due to its simplicity and convenience. In this paper, a robust speaker recognition system is presented to ensure reliable voice authentication. A large dataset of various devices is used to test the system's performance.
This paper presents a method to generate unique and random pseudonyms in a network. More precisely, each user can now generate his pseudonym locally in his own personal device. There is no need for any information interchange between issuing parties or global data (especially keys), except unique identifiers for each user and each device of the system. Additionally the holder can prove, that he generated a specific pseudonym without revealing his identity and he can reveal his identity by disclosing the pseudonym. Whereas the verifier of a disclosed pseudonym can be sure, that the presenter of the pseudonym is the person who originally generated it. The identifier of the user and the identifier of the user's device will be used to generate unique pseudonyms, but to ensure pseudonymity, both components will be stored in the pseudonym in encrypted form.
In todayu0027s mobile information society, location-based services play an increasingly important role. These services are to be accessed by users with a mobile end device for the use of city maps, route planning, navigation, or traffic information. Mobile devices don't have the same capabilities as computers. These limitations can be overcome by using special techniques in the app development. This paper outlines the experience gained in the development of a prototype for route computation in public transport networks to be used on a personal digital assistant. Subdivision of the route computation process into various steps for minimizing the computation expenditure on the mobile end device plays a significant role.
DIANE is a digital assistant system that aims to fasten the doctor access to various informations at the hospital such as health care facility, medical records, and also human resource data. The fasten access could be achieved by implementing face recognition and live streaming.
Digital assistants are emerging to become more prevalent in our daily lives. Performing multiple tasks quickly in interacting with these assistants.
As extensive experimental research has shown individuals suffer from diverse biases in decision-making. In our paper we analyze the effects of decision-making biases of managers in collaborative decision processes on organizational performance. In the simulations, managerial decisions which are based on different levels of organizational complexity and different incentive systems suffer from biases known from descriptive decision theory. Some combinations of biases can actually improve performance, even though each bias would be bad on its own. The results illustrate how biases in combination with each other and in different organizational contexts affect organizational performance. We find that, this might evoke considerations whether decision-making should be as rational as possible.
Big data, the enhanced ability to collect, store and analyze previously unimaginable quantities of data in tremendous speed and with negligible costs, delivers immense benefits in marketing efficiency, healthcare, environmental protection, national security and more. The central tenets of the current privacy framework, the principles of data minimization and purpose limitation, are severely strained by the big data technological and business reality. As we increasingly interact with these artificial agents in unsupervised settings, with no human oversight, their advanced capabilities raise important legal and philosophical questions. The focus on the machine is a distraction from the debate surrounding data driven ethical dilemmas, such as privacy, fairness and discrimination. The machine may exacerbate, enable, or simply draw attention to the ethical challenges, but it is humans who must be held accountable. Policymakers should seek to devise agreed-upon guidelines for ethical data analysis and profiling. Such guidelines would address the use of legal and technical mechanisms to obfuscate data; criteria for calling out unethical, if not illegal, behavior; categories of privacy and non-privacy harms; and strategies for empowering individuals through access to data in intelligible form.
Amazon's Echo, and Apple's Siri have drawn attention from different user groups; however, these existing commercial VUIs support limited language options for users including native English speakers and non-native English speakers. We conducted a study of the Google Home Smart Speaker with 20 participants, including those who are native and non-native English speakers to understand their differences in using the device. The findings show that compared with their counterparts, the native English speakers had better and more positive user experiences in interacting with the device. It also shows that users' language skills are crucial for interacting with VUIs. The findings from this study can create insights for VUI designers and developers for implementing multiple language options and better voice recognition algorithms in VUIs for different user groups across the world.
the smart speakers cannot distinguish human voice from machine voice. a way to tell if a human or computer is speaking to the smart speaker is desired. to prevent such attacks in the absence of residents, we propose a system consisting of a speaker and a microphone array to detect the existence of a human nearby, supposing it can be incorporated in a smart speaker in the future.
As smart speakers with voice interaction capability permeate continuously in the world, more and more people will gradually get used to the new interaction medium–voice. Although speech recognition, natural language processing (NLP) have been greatly improved over the past few years, users still may encounter errors from time to time like “cannot understand”, “no requested audio resource (such as music)”, which can frustrate users. Therefore, when an error message is reported, it is vital that the smart speaker gives an effective and proper response. However, currently the response strategies adopted by leading smart speaker brands in China differed mainly on two dimensions: “apology or not” and “humor or neutral”. We explored user’s preference of response strategies under two error scenarios——“cannot understand” and “no requested audio resource”. Two key factors were measured: the user's satisfaction and how sincere the speaker's response seemed. The results showed that participants were more satisfied and perceived higher sincerity when smart speaker apologized in both error scenarios. In the “no requested audio resource” scenario, humor had no significant impact on the perception of satisfaction and sincerity. But in the “cannot understand” scenario, humorous expression decreased perceived sincerity.
A pharmacophore analysis approach was used to investigate and compare different classes of compounds relevant to the drug discovery process (specifically, drug molecules, compounds in high throughput screening libraries, combinatorial chemistry building blocks and nondrug molecules). Significant differences were observed between the pharmacophore profiles for drug molecules and screening compounds, which are related to the distribution of pharmacophores in non-drug molecules. It is suggested that the analysis of pharmacophore profiles could be used as an additional tool for the optimization of compound selection and library design, thus improving the odds of success in lead discovery projects.
Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With annotation, assays can be searched, clustered, tagged, and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been designed to provide a structured way to organize and link terms related to bioassays.
Next generation sequencing (NGS) produces massive datasets consisting of billions of reads and up to thousands of samples. Bioinformatic analysis is typically done with the help of open source tools, one application at a time. This leaves bioinformaticians with the tasks of combining tools, managing data, and documenting the analysis.
Special software was used to analyze the relationships between symptoms, syndromes, and traditional Chinese medicine. The classic Apriori algorithm is useful to mine cases of influenza treated by contemporary famous old Chinese medicine.
Identifying antimicrobial resistant (AMR) bacteria in metagenomics samples is essential for public health and food safety. Next-generation sequencing (NGS) technology has provided a powerful tool in identifying the genetic variation and constructing the correlations between genotype and phenotype in humans and other species. Genetic Polymorphisms Assignments (GPA) is a method for estimating bacterial genotypes in mixed samples.
The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (changing the genetic code) and gene regulation. These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria.
The hidden connection between these reports is that gene editing could be used to solve issues related to health care allocation. It may improve public health by making future generations healthier. However, enhancing future generations will require In Vitro Fertilisation and Pre-implantation Genetic Diagnosis. The current discussion on future generations suggests that women might be morally required to use IVF. The necessary involvement of women in an enhancing scenario has not been discussed by its proponents. There is no clear feminist view on the ethics of human genome modification. Enhancing future generations will be gendered, unless the artificial womb is developed. These are challenging issues that require a wider perspective, of both women and men. Despite the lack of a unified feminist conclusion in the discussions about the merits and risks of human genome modification, there is an urgent need to clarify the role of women in this scenario.
CRISPR-Cas is a tool that is widely used for editing genes. However, unexpected off-target effects may occur as a result of long-term nuclease activity. Anti-CRISPR proteins, which are powerful molecules that inhibit the CRISPR-Cas system, may have the potential to promote better utilization of the CRISPR-Cas system in gene therapy. Additionally, more in-depth research on these proteins would help researchers to better understand the evolution of bacteria and viruses over time. Therefore, it is necessary to collect and integrate data on various types of anti-CRISPRs.
CRISPR has become a hot research area ever since its advent for its efficiency and specificity in editing almost every section of DNA sequences. Based on its ability to edit genes, various techniques have been developed to achieve different goals. Genes in a DNA strain can be precisely broken and repaired, allowing them to be modified in various ways. The high efficiency of CRISPR/Cas9 reagents preparation and the easiness of experiment conduction make it now a powerful tool of high-content screen.
The CRISPR-Cpf1 system has been successfully applied in genome editing. However, target efficiency of the CRISPR-Cpf1 system varies among different gRNA sequences. A computer program predicts how genetic instructions will work. A new tool, CRISPR-DT, helps users design the best instructions for the CRISPR system, making it easier to edit genes.
Emerging self-driving vehicles are vulnerable to different attacks due to the principle and the type of communication systems that are used in these vehicles. These vehicles are increasingly relying on external communication via VANETs. VANETs add new threats to self-driving vehicles, causing significant challenges in autonomous systems. These communication systems render self-driving vehicles vulnerable to many types of malicious attacks, such as Sybil attacks, Denial of Service (DoS), black hole, grey hole and wormhole attacks. In this paper, we propose an intelligent security system designed to secure external communications for self-driving and semi self-driving cars. The hybrid detection system relies on the Back Propagation neural networks (BP), to detect a common type of attack in VANETs: Denial-of-Service (DoS). The experimental results show that the proposed BP-IDS is capable of identifying malicious vehicles in self-driving and semi self-driving vehicles.
Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation. One of the main challenges for self-driving vehicles on public roads is the safe cooperation among multiple vehicles. When self-driving vehicles are in the same area, they may collide or stop suddenly, making it uncomfortable for passengers. In this paper, we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both their own perception systems for cooperating with other self-driving and/or human-driven vehicles. Our simulation results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols, while ensuring safety.
This paper provides a holistic study of how stock prices vary in their response to financial disclosures across different topics. Thereby, we specifically shed light into the extensive amount of filings for which no a priori categorization of their content exists. For this purpose, we utilize an approach from data mining - namely, latent Dirichlet allocation - as a means of topic modeling. This technique helps us categorize a large number of company filings from the U.S. We then evaluate the subsequent stock market reaction. Our research shows that different types of news stories have varying effects on the financial markets. For instance, we find a statistically significant abnormal return in response to earnings results and credit rating, but also for disclosures regarding business strategy, the health sector, as well as mergers and acquisitions. Our results yield findings that benefit managers, investors and policy-makers by indicating how regulatory filings should be structured and the topics most likely to precede changes in stock valuations.
A significant problem of using deep learning techniques is the limited amount of data available for training. There are some datasets available for the popular problems like item recognition and classification for self-driving cars, however, it is very limited for the industrial robotics field. In previous work, we have trained a special type of AI called a Convolutional Neural Network (CNN) to identify the robot in the image and determine its 3D position by using just a 2D image, but it was limited to a range of robots produced by Universal Robots (UR). In this work, we extend our method to work with a new robot arm - a new robot arm, which has a different appearance. However, instead of collecting large datasets once again, we collect a number of smaller datasets containing a few hundred frames each and use transfer learning techniques on the CNN trained on UR robots to adapt it to a new robot having different shapes and visual features. We have proven that transfer learning is not only applicable in this field, but it requires smaller well-prepared training datasets, trains significantly faster and reaches similar accuracy compared to the original method, even improving it on some aspects.
We introduce Ignition: a system that trains self-driving cars in simulated environments. The model is a neural network that processes images from the front of a simulated F1 car and provides instructions for steering, acceleration, and braking. Importantly, we never explicitly train the model to detect road features like the outline of a track or distance to other cars; instead, we illustrate that these latent features can be automatically encapsulated by the network.
As for the humanoid robots, the internal noise, which is generated by motors, fans and mechanical components when the robot is moving or shaking its body, severely degrades the performance of the speech recognition accuracy. In this paper, a novel speech recognition system robust to ego-noise for humanoid robots is proposed, in which on/off state of the motor is employed as auxiliary information for finding the relevant input features. For this, we consider the bottleneck features, which have been successfully applied to a deep neural network for speech recognition. When learning the bottleneck features to catch, we use the motor's on/off state and acoustic features to help the first model learn the bottleneck features. The second model combines features from the first model and acoustic features to improve speech recognition. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.
Recognizing Traffic Signs using intelligent systems can drastically reduce the number of accidents happening world-wide. With the arrival of Self-driving cars it has become a staple challenge to solve the automatic recognition of Traffic and Hand-held signs in the major streets. Various machine learning techniques like Random Forest, SVM as well as deep learning models has been proposed for classifying traffic signs. Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks. In this paper, we propose a novel and one-for-all architecture that aces multiple benchmarks with better overall score than the state-of-the-art architectures. Our model uses a combination of residual blocks and skip connections to improve performance. With this we score 99.33% Accuracy in German sign recognition benchmark and 99.17% Accuracy in Belgian traffic sign classification benchmark. Moreover, we propose a newly devised dilated residual learning representation technique which is very low in both memory and computational complexity.
In this paper, we present a transfer learning method for the end-to-end control of self-driving cars, which enables a convolutional neural network (CNN) trained on a source domain to be utilized for the same task in a different target domain. A conventional CNN is designed to convert a single camera image into a steering command. To enable the transfer learning, we let the CNN produce not only a steering command but also a lane departure level (LDL) by adding a new task module, which takes the output of the last convolutional layer as input. The CNN trained on the source domain, called source network, is then utilized to train another task module called target network, which also takes the output of the last convolutional layer of the source network and is trained to produce a steering command for the target domain. The steering commands from the source and target network are finally merged according to the LDL and the merged command is utilized for controlling a car in the target domain. To demonstrate the effectiveness of the proposed method, we utilized two simulators, TORCS and GTAV, for the source and the target domains, respectively. Experimental results show that the proposed method outperforms other baseline methods in terms of stable and safe control of cars.
This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE), a novel minimal simulation of a centralised financial market, based on a Limit Order Book (LOB) such as is common in major stock exchanges. Construction of BSE was motivated by the fact that most of the world’s major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Current research into modern financial markets is limited because real-world exchanges are not open to experimental testing, so researchers rely on past data. Similarly, university-level education of the engineers who can create next-generation automated trading systems requires that they have hands-on learning experience in a sufficiently realistic teaching environment. BSE as described here addresses both those needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHuB.
Once self-driving car becomes a reality and passengers are no longer worry about it, they will need to find new ways of entertainment. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the driving car is achieved at 98.04% and our approach can minimize delay. However, retrieving entertainment contents at the Data Center (DC) can hinder content delivery service due to high delay of car-to-DC communication. To address these challenges, we propose a deep learning based caching for self-driving car, by using Deep Learning approaches deployed on the Multi-access Edge Computing (MEC) structure. In the proposed system, a special server called MEC is used to predict what content passengers will want in different areas. The results are then sent to roadside units to reduce delays. A special computer program uses images to predict the age and gender of passengers. Through this, the self-driving car can identify the contents need to be downloaded from the MEC server and cached. Finally, we formulate deep learning based caching in the self-driving car that enhances entertainment services as an optimization problem whose goal is to minimize content downloading delay. To solve the formulated problem, a Block Successive Majorization-Minimization (BS-MM) technique is applied.
Moral responsibility is a major concern in automated decision-making, with applications in self-driving cars.
A key problem for autonomous car navigation is the understanding, at an object level, of the current driving situation. Addressing this issue requires the extraction of meaningful information from on-board images by classifying the fundamental elements of urban scenes into categories that include streets, buildings, pedestrians, vehicles, and signs. A novel fusion architecture based on the Stixel framework is presented for combining images from a map and a camera to identify objects in a scene, while integrating information about the layout of the environment. The proposed approach was tested on a manually labeled data set in urban environments. The results show that the classification accuracy of the fundamental elements composing the urban scene was significantly enhanced by this method compared to segmentation of objects using a computer algorithm alone.
This paper proposes a MATLAB/Simulink benchmark suite for an open-source self-driving system based on Robot Operating System (ROS). One approach to developing self-driving systems is using ROS, an open-source tool for robot applications. On the other hand, the popular approach in the automotive industry is the utilization of MATLAB/Simulink which is software for modeling, simulating, and analyzing. MATLAB/Simulink provides an interface between ROS and MATLAB/Simulink that enables to create functionalities of ROS-based robots in MATLAB/Simulink. However, it is not been fully utilized in the development of self-driving systems yet because there are not enough samples for self-driving, and it is difficult for developers to adopt co-development. Therefore, we provide a MATLAB/Simulink benchmark suite for a ROS-based self-driving system called Autoware. Autoware is popular open-source software that provides a complete set of self-driving modules. The provided benchmark contains MATLAB/Simulink samples available in Autoware. They help to design ROS-based self-driving systems using MATLAB/Simulink.
The management of self-driving systems is becoming more complex as the development of self-driving technology progresses. The automotive industry uses MATLAB/Simulink for designing and testing self-driving systems, but ROS is also used in the development. These models are incompatible with ROS-based systems. To allow the two to be used in tandem, it is necessary to rewrite the code and incorporate them into the ROS-based system, which makes development inefficient. Therefore, the proposed framework allows models created using MATLAB/Simulink to be used in a ROS-based self-driving system, thereby improving development efficiency. Furthermore, our evaluations of the proposed framework demonstrated its practical potential.
Ensuring the safety of self-driving cars is important, but neither industry nor authorities have settled on a standard way to test them. It's not possible to test all possible driving scenarios by manually creating tests. Therefore, we developed AsFault, a tool for automatically generating virtual tests for self-driving car software. We demonstrate AsFault by testing the lane keeping feature of an artificial intelligence-based self-driving car, for which AsFault generates scenarios that cause it to drive off the road. A video illustrating AsFault in action is available at: https://youtu.be/lJ1sa42VLDw
With the developing of the technology on self-driving, more and more L3 driverless vehicles are launched in market, people get opportunities to experience the self-driving cars in their daily life. As a result, there is a growing demand for the autopilot experience. This paper examines three ways to interact with self-driving cars: eye movements, voice commands, and hand gestures. It looks at the strengths and weaknesses of current methods and considers the future direction of human-computer interaction in self-driving vehicles.
In recent years self-driving vehicles have become more commonplace on public roads, with the promise of bringing safety and efficiency to modern transportation systems. Increasing the reliability of these vehicles on the road requires an extensive suite of software tests, ideally performed on highfidelity simulators, where multiple vehicles and pedestrians interact with the self-driving vehicle. It is therefore of critical importance to ensure that self-driving software is assessed against a wide range of challenging simulated driving scenarios. Current methods for creating driving scenarios still require human input. We propose to automate the process using advanced computer algorithms to create scenarios that test self-driving vehicles' abilities and help prevent accidents. We show that by incorporating the generated scenarios into the training set of the self-driving policy, and by fine-tuning the policy we obtain safer self-driving behavior.
At the brink of the introduction of self-driving vehicles, only little is known about how potential users perceive them. This is especially true for self-driving vehicles deployed in public transport services. In this study, a discrete choice model is used to analyze people's attitudes towards self-driving vehicles and their interest in technology. Based on the responses of 282 respondents from the Netherlands and Germany, a model is estimated to understand these attitudes. Based on the responses, a discrete choice model is used to analyze people's attitudes towards self-driving vehicles and their interest in technology. The results show that currently public transport passengers prefer the self-driving bus over the regular bus only for short trips. This is due to the finding that the value of travel time is about twice as high for the self-driving bus as for the regular bus for a short commuting trip. Findings from this study further suggest that the popularity of self-driving busses decreases with the presence of a human steward on-board, or if they are operated as a demand-responsive service with fixed routes. People who currently show a strong interest in technology or trust in automated vehicle technology perceive the self-driving busses better than others. Preferences towards automated public transport services are expected to evolve along with the transition from demonstration pilots to their deployment in regular operations.
In the wake of the on-going digital revolution, we will see a dramatic transformation of our economy and most of our societal institutions. While the benefits of this transformation can be massive, there are also tremendous risks to our society. After the automation of many production processes and the creation of self-driving vehicles, the automation of society is next. We are at a crossroads: we must choose between a society where decisions are made from the top down, or one where people have a say in the decisions that affect them. I believe human dignity, autonomy, and democracy are under threat, but they're essential for a better society and should be protected. Modern information and communication systems (ICT) enable both, but the latter has economic and strategic benefits.
This paper addresses the financial markets with particular emphasis on trading systems seen in banking today. With the advent of modern computing, trading across different regions can now happen very quickly, a reality that is impossible without the advancements of software systems technology. We then move on and discuss the anatomy of a trading system and how it fits in with the banks' internal systems.
In this paper we present theoretical methods for modeling and analyzing traffic flow for the control of autonomous mobility-on-demand MOD systems wherein robotic, self-driving vehicles transport customers within an urban environment and rebalance themselves to ensure acceptable quality of service throughout the network. We first model the MOD system using a complex network model that accounts for lost passengers. The theoretical insights are used to design a real-time rebalancing system, which is applied to a case study of New York City and implemented on an eight-vehicle mobile robot testbed. The case study of New York shows that the current taxi demand in Manhattan can be met with about 8,000 robotic vehicles roughly 70% of the size of the current taxi fleet operating in Manhattan. Finally, we extend our setup to include traffic congestion, and study the impact of autonomously rebalancing vehicles on overall congestion. Collectively, this paper provides a rigorous approach to coordinating self-driving vehicles, and provides one of the first characterizations of the benefits of robotic transportation.
Many model based techniques have been proposed in the literature for applying domestic service tasks on humanoid robots, such as teleoperation, learning from demonstration and imitation. However sensor based robot control helps robots work in uncertain environments, where previous methods struggled. Furthermore, for service and manipulation tasks, it is more suitable to study the interaction between the robot and its environment at the contact point using the sensor based control, rather than specifying the joint positions and velocities required to achieve them.
Inaccurate information, in the field of library and information science, is often regarded as a problem that needs to be corrected or simply understood as either misinformation or disinformation without further consideration. Misinformation and disinformation, however, may cause significant problems for users in online environments, where they are constantly exposed to an abundance of inaccurate and/or misleading information. This paper aims to establish conceptual groundwork for future empirical research by examining the relationships among information, misinformation, and disinformation. Our analysis covers how to identify and detect misinformation and disinformation. Misinformation is a complex idea that goes beyond just being incorrect or incomplete. Disinformation doesn't always mean the same thing as misinformation. We argue that misinformation and disinformation are related yet distinct sub-categories of information.
The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We create a system that combines human and artificial intelligence to identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert finding for misinformation identification. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert finding with respect to misinformation identification in microblogs.
The importance of research on misinformation has received wide recognition. Two major challenges faced by this research community are the lack of theoretical models and the scarcity of misinformation in support of such research. This paper aims to address the challenges of misinformation by defining and making it easier to share and use. In particular, a representation and a model of misinformation are proposed through surveying, synthesizing, and explicating existing work in the field. The ontology-supported misinformation model can not only guide future misinformation research but also lay the foundation for building a digital misinformation library by advancing our knowledge on misinformation and by improving its sharing, management, and reuse. In addition, we present a formal methodology for managing misinformation in a digital library, and suggest future research directions related to the misinformation model.
Online Social Networks OSNs have become increasingly popular means of information sharing among users. The spread of news regarding emergency events is common in OSNs and so is the spread of misinformation related to the event. We define misinformation as false or inaccurate information spread by mistake or on purpose. In this paper we study the problem of misinformation identification in OSNs, and we focus in particular on the Twitter social network. We build a model that identifies suspicious behavior and detects misinformation based on how users interact with tweets. Our experiments show that our approach works well at identifying misinformation during emergencies. Furthermore, our model manages to timely identify misinformation, a feature that can be used to limit the spread of the misinformation.
The advent of WWW changed the way we can produce and access information. Recent studies showed that users tend to select information that is consistent with their system of beliefs, forming polarized groups of like-minded people around shared narratives where dissenting information is ignored. In this environment, users work together to reinforce their shared views, making it hard to challenge their opinions. Such a configuration occurs even in the consumption of news online, and considering that 63% of users access news directly from social media, one hypothesis is that more polarization allows for further spreading of misinformation. We focus on how users in several European countries (Italy, France, Spain, and Germany) interact with news on Facebook. First, we compare the page posting behavior and the users interacting patterns across countries and observe different posting, liking and commenting rates. Second, we explore the tendency of users to interact with different pages (i.e., selective exposure) and the emergence of polarized communities generated around specific pages.
We find that Italy is the most polarized country, followed by France, Germany and lastly Spain. We use a specific model to simulate how communities form online, taking into account how users engage with and trust the news. Our findings suggest that trust in information broadcaster plays a pivotal role against polarization of users online.
Online social media provide users with unprecedented opportunities to engage with diverse opinions. We discuss the policy implications of our model, highlighting the downside of debunking strategies and suggesting alternative strategies to contrast misinformation. Simultaneously, they allow the spread of misinformation by empowering individuals to self-select the narratives they want to be exposed to, both through active (confirmation bias) and passive (personalized news algorithms) self-reinforcing mechanisms. A precise theoretical understanding of such trade-offs is still largely missing. We introduce a stylized social learning model where most participants in a network update their beliefs unbiasedly based on the arrival of new information, while a fraction of participants display confirmation bias, enabling them to reject news that are incongruent with their pre-existing beliefs. We show that this simple confirmation bias mechanism can generate permanent opinion polarization. Furthermore, the model results in states where unbiased agents behave "as if" they were biased, due to their biased neighbours effectively functioning as gatekeepers, restricting their access to free and diverse information. We analyzed how confirmation bias affects people's beliefs and tested this using data from US counties. Our findings indicate that confirmation bias in small doses may actually result in improved accuracy across individuals by preserving information diversity in a social network. However, results also indicate that when confirmation bias grows past an optimal value, accuracy declines as biased agents restrict information flow to subgroups.
Superintelligence is a hypothetical AI that is much smarter than humans in many areas. If built, superintelligence could be a transformative event, with potential consequences that are massively beneficial or catastrophic. The prospect of superintelligence is a topic of ongoing debate, which includes a lot of misinformation. Superintelligence misinformation is potentially dangerous, ultimately leading bad decisions by the would-be developers of superintelligence and those who influence them. This paper surveys strategies to counter superintelligence misinformation. Two types of strategies are examined: preventing superintelligence misinformation and correcting it when it happens. Most misinformation is hard to correct, so preventing it is crucial. The strategies proposed can be applied to lay public attention to superintelligence, AI education programs, and efforts to build expert consensus.
The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. We analyze the problem of stopping the spread of misinformation on online social networks in realistic scenarios, allowing for multiple sources of misinformation. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. Second, we show that the misinformation containment problem cannot be approximated within a significant factor in the time it takes to solve a complex problem unless a very efficient solution exists. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.
Yet contemporary work in Philosophy argues provisional belief in conspiracy theories is—at the very—least understandable (because conspiracies occur) and if we take an evidential approach—judging individual conspiracy theories on their particular merits—belief in such theories turns out to be warranted in a range of cases. I look at the types of evidence used in conspiracy theories and find that the problems with this evidence are not exclusive to these theories. As such, if there is a problem with the conspiracy theorist’s use of evidence, it is one of principle: is the principle which guides their use of evidence somehow in error? I argue that whatever we might think about conspiracy theories generally, there is no prima facie case for a scepticism of conspiracy theories based purely on their use of evidence.
Conspiracy theories have gained much academic and media attention recently, due to their large impact on public events. However, little is known about how conspiracy theories are produced and developed on social media. We present a qualitative study of conspiracy theorizing on Reddit during a public health crisis--the Zika virus outbreak. We used a combination of content and discourse analysis to identify the types of conspiracy theories that appeared on Reddit in response to the Zika crisis and how they developed in online forums. Our analysis shows that conspiracy talk emerged as people struggled to understand the Zika crisis and lost trust in official sources. Practical implications for social computing researchers, health practitioners, and policymakers are discussed.
Conspiracy theories are omnipresent in online discussions---whether to explain a late-breaking event that still lacks official report or to give voice to political dissent. Conspiracy theories spread and become more complicated, making it harder to understand and stop them. It is therefore crucial to develop scalable methods to examine the nature of conspiratorial discussions in online communities. What do users talk about when they discuss conspiracy theories online? What are the recurring elements in their discussions? What do these elements tell us about the way users think? This work answers these questions by analyzing over ten years of discussions in r/conspiracy---an online community on Reddit dedicated to conspiratorial discussions. We focus on the key elements of a conspiracy theory: the conspiratorial agents, the actions they perform, and their targets. For example, a narrative-motif such as "government-controls-communications" represents the various ways in which multiple conspiratorial statements denote how governmental agencies control information. Thus, narrative-motifs expose commonalities between multiple conspiracy theories even when they refer to different events or circumstances. In the process, these representations help us understand how users talk about conspiracy theories and offer us a means to interpret what they talk about. Our approach enables a population-scale study of conspiracy theories in alternative news and social media with implications for understanding their adoption and combating their spread.
Blockchain technology is the underlying enabling technology developed for Bitcoin, the most common cryptocurrency. Blockchain technologies have become increasingly popular with the potential to become a powerful disruptive force. Individuals and organizations may benefit from blockchain with its ability to increase secure data exchange and to make that transaction simpler and easier between entities. We created a model to understand why people adopt cryptocurrencies, based on a well-known theory called the Theory of Planned Behavior. We investigate factors that influence an individualu0027s intention to use a blockchain cryptocurrency. We offer empirical evidence for a better theoretical understanding of cryptocurrency adoption with practical implications in an e-government context.
With the introduction of memory-bound cryptocurrencies, such as Monero, the implementation of mining code in browser-based JavaScript has become a worthwhile alternative to dedicated mining rigs. Based on this technology, a new form of parasitic computing, widely called cryptojacking or drive-by mining, has gained momentum in the web. A cryptojacking site abuses the computing resources of its visitors to covertly mine for cryptocurrencies. In this paper, we systematically explore this phenomenon. For this, we use a three-step approach to analyze cryptojacking: identifying mining scripts and figuring out how common it is. We find that cryptojacking is common, with about 1 in 500 websites hosting mining scripts. We also examine other aspects of cryptojacking, such as the types of code used and how secure current measures are.
The rise of modern blockchains has facilitated the emergence of smart contracts: autonomous programs that live and run on the blockchain. Smart contracts have seen a rapid climb to prominence, with applications predicted in law, business, commerce, and governance.  contracts :[39],"are commonly written in a high-level language such as Ethereumu0027s Solidity and then compiled into a format that can be executed on the blockchain. Once deployed, the code is automatically run by a virtual machine. As with all programs, smart contracts can be highly vulnerable to malicious attacks due to deficient programming methodologies, languages, and toolchains, including buggy compilers. At the same time, smart contracts :[39],"are also high-value targets, often commanding large amounts of cryptocurrency. Hence, developers and auditors need security frameworks capable of analysing low-level bytecode to detect potential security vulnerabilities. In this paper, we present Vandal: a security analysis framework for Ethereum smart contracts. Vandal is a tool that converts low-level blockchain code into a format that's easier to analyze for potential security vulnerabilities.
Vandal is both fast and robust, successfully analysing over 95% of all 141,000 contracts with an average runtime of 4.15 seconds; outperforming the current state of the art tools---Oyente, EthIR, Mythril, and Rattle---under equivalent conditions.
As Bitcoinu0027s popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds.
For peer-to-peer networks, it is very important to have a mechanism in place that allows the nodes to control resource usage and prevent flooding and denial-of-service attacks with spam. In this paper, we discuss and compare the methods used by cryptocurrency projects. The introduced methods are then applied to design a decentralised exchange for Namecoin names (or more generally, crypto assets) as an example.
Cryptocurrency is a rapid developing financial technology innovation which has attracted a large number of people around the world. The high-speed evolution, radical price fluctuations of cryptocurrency, and the inconsistent attitudes of monetary authorities in different countries have triggered panic and chain reactions towards the application and adoption of cryptocurrency and have caused public security related events. In this paper, we examine the dynamics and risks of the cryptocurrency market based on historical price data. Furthermore, consistent with public perception, our quantitative analysis reveals that the cryptocurrency market is relatively fragile and unstable.
Cryptocurrency was built initially as a possible implementation of digital currency, then various derivatives were created in a variety of fields such as financial transactions, capital management, and even nonmonetary applications. This paper aims to offer analytical insights to help understand cryptocurrency by comparing cryptocurrency to traditional assets like foreign exchange and stock. Our research shows that cryptocurrency behaves similarly to stock. As to the robustness and clustering structure, our analysis shows cryptocurrency market is more fragile than stock market, thus it is currently a high-risk financial market.
The smart device owning rate is higher than ever before and mobile payment has become one of the major payment methods in many different areas. At the same time, blockchain-based cryptocurrency is becoming a type of currency and the total value of all types of cryptocurrency has reached USD 200 billion. Therefore, it is a natural demand to support cryptocurrency payment on mobile devices. Considering the poor infrastructure and low penetration of financial service in developing countries, this combination is especially attractive. We propose two different approaches for mobile payment using cryptocurrency: one involves a bank and the other does not require a bank. We also provide a solution for the bank to meet KYC (know your customer)/AML (antimoney laundering) compliance requirements when it is involved in cryptocurrency mobile payment processing.
Researchers studied how financial crises affect markets and how people's behavior influences each other's investment decisions. Much less has been said about the influence of financial news on financial markets. We propose a novel measure of collective behavior in financial news on the Web, News Cohesiveness Index (NCI), and show that it can be used as a systemic risk indicator. We evaluate the NCI on financial documents from large Web news sources on a daily basis from October 2011 to July 2013 and analyse the interplay between financial markets and financially related news. We hypothesized that strong cohesion in financial news reflects movements in the financial markets. Cohesiveness is a better measure of systemic risk than looking at individual news events. Our results indicate that cohesiveness in the financial news is highly correlated with and driven by volatility on the financial markets.
In this paper, I propose a methodology to study the comovement between the entropy of different financial markets. The entropy is derived using singular value decomposition of the components of stock market indices in financial markets from major economies, i.e., France, Germany, the United Kingdom, and the United States. I study how a shock in the entropy in the United States affects the entropy in the other financial markets. I also model the entropy using a dynamic factor model and derive a common factor behind the entropy movements in these four markets.
The use of mobile phone forensics to investigate fraudulent activity is nothing new. But mobile phones have evolved into smartphones, and fraudsters have evolved with them. Smartphones are packed with functionality that can easily be used for data theft or inappropriate contact with other parties - none of which passes through company-owned systems, and is to all intents and purposes 'off the grid'. Employers need to be aware of these risks when devices are issued, along with ensuring that processes are in place when suspicions are raised, explains Philip Ridley of CCL-Forensics. Detecting corporate fraudsters is a constant challenge. It's about staying one step ahead of those who misuse technology to commit fraud. What's more, the methods through which technology can be used to commit fraud are constantly evolving. This means a company's intellectual property and sensitive data is at risk of sabotage or just good old-fashioned theft.
Phishers continue to alter the source code of the web pages used in their attacks to mimic changes to legitimate websites of spoofed organizations and to avoid detection by phishing countermeasures. Manipulations can be as subtle as source code changes or as apparent as adding or removing significant content. To appropriately respond to these changes to phishing campaigns, A variety of algorithms are used to detect phishing websites by analyzing their content, based on a large database of known phishing attacks. The results of the experiments using a variety of different content-based approaches demonstrate that some can achieve a detection rate of greater than 90% while maintaining a low false positive rate.
Mobile devices are very common in everyone’s day-to- day life. Nowadays such devices come with many features of desktop or laptop. Hence people can use these devices for diverse applications. As the acceptability and usability of such devices are very high, there are chances that these devices can be used for illegal activities. The percentage of mobile phones or smart phones involved in cyber crimes is in hike. This paper discusses digital evidence found in Windows Mobile phones and a way to safely retrieve data from them using a special approach. Also it describes a tool developed for forensically acquiring and analyzing Windows Mobile devices and WinCE PDAs.
Detection of different types of image editing operations carried out on an image is an important problem in image forensics. It reveals the history of image processing and detects forgeries present in an image. In real-forensics scenarios, it may not be possible to know the editing operations carried out on an image. To solve this problem, we propose a novel deep learning-based method which can differentiate between different types of image editing operations. The proposed method classifies image patches in a pair-wise fashion as either similarly or differently processed using a deep siamese neural network. Once the network learns feature that can discriminate between different image editing operations, it can differentiate between different image editing operations not present in the training stage. The experimental results show the efficacy of the proposed method in detecting/discriminating different image editing operations.
The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices often have limited standard interfaces for communication, such as USB ports or wireless connections. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option for digital forensic investigations because it doesn't require modifying the device. The literature on various EM side-channel analysis attack techniques are discussed – selected on the basis of their applicability in IoT device investigation scenarios. The insights gained from the background study are used to identify potential future uses of the technique for digital forensic analysis on IoT devices.
The malicious alteration of machine time is a big challenge in computer forensics. Performance figures show the feasibility of our proposal. Detecting such changes and reconstructing the actual timeline of events is of paramount importance. However, this can be difficult since the attacker has many opportunities and means to hide such changes. In particular, cloud computing, host and guest machine time can be manipulated by an attacker in various ways. We examine the challenges of keeping host and guest machine times accurate in the cloud. Guest virtual machines are susceptible to attacks from their host. As such, it is important to guarantee the timeline integrity of both hosts and guests in a cloud, or at least to ensure that the alteration of such timeline does not go undetected. We describe a new way to detect changes to host and guest time. The proposed framework has been implemented on an especially built simulator.
Image tampering, being readily facilitated and proliferated by today’s digital techniques, is increasingly causing problems regarding the authenticity of images. As the most popular multimedia data, JPEG images can be easily tampered without leaving any clues; therefore, JPEG-based forensics , including the detection of double compression, interpolation, rotation, etc., has become an active research topic in multimedia forensics. Nevertheless, the interesting issue of detecting image tampering and its related operations by using the same quantization matrix has not been fully investigated. Aiming to detect such forgery manipulations under the same quantization matrix, we propose a
In this paper, we present a novel approach to accurately calibrate the kinematic model of a humanoid based on observations of its monocular camera. Our method estimates the camera's position and the angles of the robot's joints. Our method automatically selects the best configurations for calibration. Further, our approach to configuration selection yields substantially better optimization results compared to randomly chosen viable configurations. Hence, our system only requires a reduced number of configurations to achieve accurate results. Our optimization is general and the implementation, which is available online, can easily be applied to different humanoids.
In this paper, we present an integrated navigation system that allows humanoid robots to autonomously navigate in unknown, cluttered environments. Using the camera on the robot, our system estimates its position and corrects for any errors in its movement. Based on this model, our system iteratively computes safe actions leading the robot to target locations. To efficiently check for collisions during planning, we developed an approach that takes into account the shape of the robot and the obstacles. As we demonstrate in experiments with a Nao humanoid, our system leads to robust navigation in cluttered environments and the robot is able to traverse highly challenging passages.
In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict the difficulty of navigating around obstacles. By considering the costs of the path, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.
In many situations, users walk on typical paths between specific destinations at which the service of a mobile robot is needed. Depending on the environment and the paths, step-by-step following of the human might not be the optimal solution since better paths for the robot exist. We predict the human's future movements to help the robot navigate. The robot's view of the human is often blocked by obstacles, making it hard for the robot to see where the human is going. Our approach helps the robot see the human again by taking actions. Our technique is more efficient than following the user closely, even when the robot can't see the user.
An autonomous service robot often first has to search for a user to carry out a desired task. This is a challenging problem when the person moves around, as the robot's view is limited and the environment blocks its sight. In this paper, we propose a novel method that computes the likelihood of the user’s observability at each possible location in the environment based on Monte Carlo simulations. The robot considers both time and visibility when searching for the user. In this way, the robot can choose the next search location that has the maximum expected observability of the user. Our experiments in various simulated environments demonstrate that our approach leads to a significantly shorter search time compared to a greedy approach with background information.
Perception of the local environment is a precondition for mobile robots to navigate safely in dynamic environments. Most robots, i.e., humanoids and smaller wheeled robots rely on planar regions. For humanoids, a simple 2D occupancy map as environment representation on which a path is planned is hereby not sufficient since they can step over and onto objects and therefore need height information. Dynamic obstacles require the robot to replan its path to avoid collisions. In this paper, we present a framework that first extracts planar regions in height maps and detects dynamic obstacles. Our system then uses this information to create a set of prediction maps, in which paths can be efficiently planned in real time at low CPU cost. We show in simulation and real-world experiments that our framework keeps run times well under 10ms for one computation cycle and allows for foresighted real-time 3D footstep planning.
In this paper, a lab automation drone notional concept is introduced. Here, a robotic limb is attached to a robotic rotorcraft. The limb's gripper allows the unmanned aerial vehicle to dexterously manipulate objects such as micro-arrays and test tubes often used in high-speed systems. The resulting drone could augment existing high-speed systems operations. The 6-axis robotic arm and gripper design are presented. Test-and-evaluation approach and results are also given.
Robot companionship has become more popular in past years. However, humanoid gait might be somewhat unstable for these applications. Even with miniature humanoids, falls occur frequently. Wheel attachments have been added to miniature humanoids to improve their stability and speed. In addition, with such attachments a robot can switch from walking to rolling when necessary. DARwIn-OP is a humanoid robot that has been used as an experimentation and performance evaluation platform. This paper discusses preliminary work regarding robot companionship applications by using a miniature humanoid capable of fetching different toys based on voice command.
In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. The computational efficiency of MAVNet enables the drone to fly at high speeds. We present the same results in this research and compare them with other state-of-the-art methods of vision and learning based navigation. With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too. Due to guidelines set by the governments regarding the operation ceiling of civil drones, road-tracking based navigation is garnering interest. In an attempt to achieve the above mentioned tasks, we propose an imitation learning based, data-driven solution to UAV autonomy for navigating through city streets by learning to fly by imitating an expert pilot. Our classifier uses a advanced algorithm to identify roads in images, based on a complex model that reconstructs the input frames. Based on this, our system performs better than many existing models for imitation learning. We achieved high accuracy by successfully navigating roads with the data we collected, with a success rate of 98.44%. The data used for training the system has been captured from the drone, by flying it in and around urban and semi-urban streets, by experts having at least 6-8 years of flying experience. Permissions were taken from required authorities who made sure that minimal risk (to pedestrians) is involved in the data collection process.
Advances in electronics and sensors have expanded the use of networked drones to include surveillance, video recording, and delivery. These applications and services require video recording for their operations. Large drones are used singly in missions while small ones are used in formations or swarms. The small drones are proving to be useful in civilian applications. Small drones are used in group flights, entertainment, and signal emission. To develop group display applications, a real-time drone formation control for group display is proposed. Simulation shows that drone formation can display messages effectively.
This study focuses on the digital marketing capabilities of tourism SMEs. The study addresses the question of how the use of ICT-based tools benefit the organisational capabilities of a company. By adopting a skills-based approach to marketing, the study provides new insights into the existing tourism literature on e-marketing. Initial findings indicate that the digital marketing capabilities of companies are transformed through ICT-based uptake. Four major capabilities were identified, each of which evolves as a result of using the tools. A key finding of the study is that the use of ICT-based tools enables tourism SMEs to lead the web-marketing stream.
The prevalence and rapid development of the Internet and mobile technology in recent decades has revamped our living styles and daily habits. To ride on the digital trend, more business activities have been engaged in the digital world. Marketing and advertising is one of typical business areas that is transformed digitally. The growth of social media and online platforms has attracted many businesses to consider using digital marketing tools to promote their brands and products. However, with the increasing diversity of the types of digital marketing tools, they must be carefully selected based on a multiple number of criterion. In this paper, a fuzzy-AHP method is proposed and developed for assisting industry practitioners in systematically and effectively evaluate and select proper digital marketing tool(s) for adoption. The developed method not only streamlines the internal business process of digital marketing tool selection, but it also increases the practitioner's effectiveness of achieving the pre-defined strategic marketing objectives.
In this paper we revisit a topic originally discussed in 1955, namely the lack of direct evidence that muscle hypertrophy from exercise plays an important role in increasing strength. To this day, long-term adaptations in strength are thought to be primarily contingent on changes in muscle size. Given this assumption, there has been considerable attention placed on programs designed to allow for maximization of both muscle size and strength. However, the idea that bigger muscles make you stronger is not supported by much evidence. Muscle size and strength are not connected. People can lose muscle mass when they stop training, but they can still keep their strength.
Low-intensity occlusion (50-100 mm hg) training provides a unique beneficial training mode for promoting muscle hypertrophy. Training at intensities as low as 20% of the maximum weight with some blood vessel constriction results in muscle hypertrophy in as little as 3 weeks. A typical exercise prescription calls for 3 to 5 sets to volitional fatigue with short rest periods. The metabolic buildup causes positive physiologic reactions, specifically a rise in growth hormone that is higher than levels found with higher intensities. Occlusion training is applicable for those who are unable to sustain high loads due to joint pain, postoperative patients, cardiac rehabilitation, athletes who are unloading, and astronauts.
Current dogma suggests aerobic exercise training has minimal effect on skeletal muscle size. Aerobic exercise has been shown to change how the body uses protein and build muscle over time. These findings challenge the current understanding of muscle growth and offer new approaches to prevent muscle loss.
Stretch training is widely used in a variety of fitness-related capacities such as increasing joint range of motion, preventing contractures and alleviating injuries. Moreover, some researches indicate that stretch training may induce muscle hypertrophy; however, studies on the topic have been primarily relegated to animal and in vitro models. The purpose of this brief review was to evaluate whether stretch training is a viable strategy to induce muscle hypertrophy in humans. Of the 10 studies, 3 found that stretch training had a positive effect on muscle structure. In these studies, the stretching was done with either an apparatus or an external load. Two studies found that muscle growth was enhanced when stretching was done during rest periods in a resistance program. In conclusion, passive, low-intensity stretch does not appear to confer beneficial changes in muscle size and architecture; alternatively, albeit limited evidence suggests that when stretching is done with a certain degree of tensile strain (particularly when loaded, or added between active muscle contractions) may elicit muscle hypertrophy.
Cycle training is widely performed as a major part of any exercise program seeking to improve aerobic capacity and cardiovascular health. However, the effect of cycle training on muscle size and strength gain still requires further insight, even though it is known that professional cyclists display larger muscle size compared to controls. It is plausible that cycle training may take longer to build muscle compared to weightlifting due to a slower rate of muscle growth. Cycle training induces muscle hypertrophy similarly between young and older age groups, while strength gain seems to favor older adults, which suggests that the probability for improving in muscle quality appears to be higher in older adults compared to young adults. For young adults, higher-intensity intermittent cycling may be required to achieve strength gains. It also appears that muscle hypertrophy induced by cycle training results from the positive changes in muscle protein net balance.
Resistance training is the most effective method to increase muscle mass. It has also been shown to promote many health benefits. Although it is deemed safe and of clinical relevance for treating and preventing a vast number of diseases, a time-efficient and minimal dose of exercise has been the focus of a great number of research studies. A U-shaped relationship between the amount of exercise and its effect on the body has been suggested, but most evidence supports a straightforward relationship between exercise volume and its benefits. Additionally, there is a paucity of data to support the U-shaped response. The overarching principle argued herein is that volume is the most easily modifiable variable that has the most evidenced-based response with important repercussions, be these muscle hypertrophy or health-related outcomes.
Although the effects of short versus long inter-set rest intervals in resistance training on measures of muscle hypertrophy have been investigated in several studies, the findings are equivocal and the practical implications remain unclear. Current evidence indicates that both short and long inter-set rest intervals may be useful when training for achieving gains in muscle hypertrophy. Novel findings involving trained participants using specialized tests that can detect muscle growth suggest a possible advantage for the use of long rest intervals to elicit hypertrophic effects. However, due to the paucity of studies with similar designs, further research is needed to provide a clear differentiation between these two approaches.
Memory is a process in which information is encoded, stored, and retrieved. For vertebrates, the modern view has been that it occurs only in the brain. This review describes a cellular memory in skeletal muscle in which hypertrophy is 'remembered' such that a fibre that has previously been large, but subsequently lost its mass, can regain mass faster than naive fibres. According to this model, previously untrained muscle fibers get ready for growth by recruiting new cells from satellite cells before hypertrophic growth. The myonuclei are protected and remain even when the muscle is atrophying. Fibres that have acquired a higher number of myonuclei grow faster when subjected to overload exercise, thus the nuclei represent a functionally important 'memory' of previous strength. This memory might last a long time in humans, as myonuclei remain stable for many years. However, myonuclei are harder to recruit in the elderly, and if the long-lasting muscle memory also exists in humans, one should consider early strength training as a public health advice.
It has been hypothesized that eating small, frequent meals enhances fat loss and helps to achieve better weight maintenance. Several observational studies lend support to this hypothesis, with an inverse relationship noted between the frequency of eating and adiposity. The purpose of this review is to discuss the results of a study that looked at how meal frequency affects body fat and muscle mass. Feeding frequency was positively associated with reductions in fat mass and body fat percentage as well as an increase in fat-free mass. However, sensitivity analysis revealed that the positive results were largely due to one study, raising questions about the effectiveness of more frequent meals for improving body composition. In conclusion, although the initial results of this meta-analysis suggest a potential benefit of increased feeding frequencies for enhancing body composition, these findings need to be interpreted with circumspection.
Inactive adults experience a 3% to 8% loss of muscle mass per decade, accompanied by a decrease in metabolism and an increase in body fat. Ten weeks of resistance training may increase lean weight by 1.4 kg, increase resting metabolic rate by 7%, and reduce fat weight by 1.8 kg. Benefits of resistance training include improved physical performance, movement control, walking speed, functional independence, cognitive abilities, and self-esteem. Resistance training may assist prevention and management of type 2 diabetes by decreasing visceral fat, reducing HbA1c, increasing the density of glucose transporter type 4, and improving insulin sensitivity. Resistance training may enhance cardiovascular health, by reducing resting blood pressure, decreasing low-density lipoprotein cholesterol and triglycerides, and increasing high-density lipoprotein cholesterol. Resistance training may promote bone development, with studies showing 1% to 3% increase in bone mineral density. Resistance training may be effective for reducing low back pain and easing discomfort associated with arthritis and fibromyalgia and has been shown to reverse specific aging factors in skeletal muscle.
We hypothesized that compared to post-exercise protein intake, alcohol consumption would slow down the process of starting protein synthesis.
Eight healthy physically active male subjects (age 21.4±4.8 yr, body mass (BM) 79.3±11.9 kg, peak oxygen uptake: 48.1 mL/kg/min, leg extension strength: 104 kg; values are mean ± SD) who had been participating in regular exercise (3 times wk−1 for >6 months) volunteered for this study.
The study employed a randomized study design where each participant completed a sequence of exercises with varying intensity and resistance with post-exercise consumption of drinks with alcohol and carbs, alcohol and protein, or protein only on three separate occasions.
Resistance exercise consisted of eight sets of five repetitions at 80% of maximum strength.
After completion of the final set, subjects rested for 5 min before commencing 30 minutes of continuous cycling at a moderate intensity.
Upon completion, subjects rested on the bike for 2 min before undertaking 10 sets of 30 seconds of high-intensity exercise, with 30 seconds of recovery in between, at 110% of their personal power output.
Immediately following exercise and after 4 h recovery, subjects ingested a 500 mL solution of either protein (PRO, 25 g whey protein powder; a specific brand from Musashi) or an energy-match in the form of CHO (25 g maltodextrin).
Furthermore, a CHO-based meal (1.5 grams per kilogram of body mass) was consumed ∼2 h post-exercise, immediately after the muscle biopsy, according to recommendations for post-exercise glycogen recovery [24].
The 8 h time frame represents an important phase of recovery after exercise, during which blood alcohol levels are likely to be high after drinking.
The alcohol ingestion protocol (1.5 g·kg−1 BM; 12±2 standard drinks) began 1 h post-exercise and was consumed in 6 equal volumes of 1 part vodka (∼60 mL) to four parts orange juice (∼240 mL, 1.8 g CHO·kg−1 BM) during a 3 h period.
The focus of the early post-exercise period (i.e., 1–8 h) is to enhance processes that help the body recover from the effects of exercise and for promoting adaptations to training [1].
For the PRO condition, orange juice was consumed with a matched volume of water instead of alcohol.
Subjects ingested the beverages within 5 min every 30 min.
Blood, cell signaling and mRNA data were analyzed by looking at data over time and treatment. myofibrillar protein synthesis was analyzed by data analysis with repeated checks.
The first novel finding of this study was that mTOR signaling and muscle protein production following concurrent resistance, continuous and intermittent high-intensity exercise, designed to mimic the physical demands of team sports, were impaired during the early (8 h) recovery phase by the ingestion of large amounts (1.5 g•kg−1 BM) of alcohol.
These outcomes were most evident (37% reduction in rates of MPS) when alcohol was consumed when drinking, it's harder for athletes to follow a good recovery routine.
even when protein is consumed after exercise in the optimal amount, the intake of alcohol reduced the muscle-building process by ∼24%, representing only a partial ‘rescue’ of the anabolic response compared with protein alone.
The mTORC1 is a central node for integrating nutrient and exercise/contraction signal transduction [31], [32].
In conclusion, the current data provide the novel observation that alcohol impairs the response of Muscle protein synthesis (MPS) in exercise recovery in muscle tissue despite optimal nutrient provision.
The study measured alcohol consumption based on reports from athletes who binge drink.
However, published reports suggest intakes of some individuals can be significantly greater [9], [50], which is of concern for several health and safety concerns [13].
Recommended healthy eating plans to maximize recovery include protein to build muscle for replenishing glycogen stores [2],[3].
Regrettably, there has been difficulty in finding an educational message with alcohol consumption related to sports performance that has resonance with athletes.
Given the need to promote protein synthesis that helps repair and grow muscle the results of the current study provide clear evidence of impaired recovery when alcohol is consumed after resistance and high-intensity exercise even in the presence of optimal nutritional conditions.
We propose our data is of paramount interest to athletes and coaches.
Our findings provide a message of moderation in alcohol intake to aid recovery after exercise with the potential to alter current sports culture and athlete practices.
Muscle contraction and the intake of leucine-rich protein sources activate independent but complimentary signaling responses that converge at the mTOR pathway to stimulate protein translation enhancing rates of muscle protein synthesis [4]–[6].
The ingestion of 20-25 grams of high-quality protein soon after exercise [7], repeated every 4 h [8] has been shown to maximise the anabolic response in skeletal muscle.
The cultural environment surrounding some sports often involves the intake of large amounts of alcohol after training and competition, with athletes in several team sports being particularly at risk of “binge drinking” practices [9]–[11].
The outcomes of binge drinking after exercise are likely to include the direct effect of alcohol on physiological processes as well as the reduced recovery due to not eating or resting after drinking.
Although the consumption of carbohydrate can somewhat reduce the negative effects of alcohol on glycogen levels after exercise, The impact of drinking on muscle protein growth is unclear.
The aim of the current study was to determine the effect of alcohol intake on cell signaling and muscle protein growth in humans during recovery from a bout of strenuous exercise approximating stresses an athlete may experience in training and performance for various team sports such as various football and rugby codes, and court sports.
Human aging results in a variety of changes to skeletal muscle. Sarcopenia is the age-associated loss of muscle mass and is one of the main contributors to musculoskeletal impairments in the elderly. Previous research has demonstrated that resistance training can attenuate skeletal muscle function deficits in older adults, however few articles have
Maximizing the hypertrophic response to resistance training (RT) is thought to be best achieved by proper manipulation of exercise program variables including exercise selection, exercise order, length of rest intervals, intensity of maximal load, and training volume. An often overlooked variable that also may impact muscle growth is repetition duration. Duration includes the time it takes to lift, lower, and hold a weight, and depends on how quickly you do each part. We conducted a systematic review and meta-analysis to determine whether alterations in repetition duration can amplify the hypertrophic response to RT. Results indicate that hypertrophic outcomes are similar when training with repetition durations ranging from 0.5 to 8 s. From a practical standpoint it would seem that a fairly wide range of repetition durations can be employed if the primary goal is to maximize muscle growth. Findings suggest that training at volitionally very slow durations (>10s per repetition) is inferior from a hypertrophy standpoint, although a lack of controlled studies on the topic makes it difficult to draw definitive conclusions.
Recovery from exercise refers to the time period between the end of a bout of exercise and the subsequent return to a resting or recovered state. It also refers to specific physiological processes or states occurring after exercise that are distinct from the physiology of either the exercising or the resting states. After exercise, the cardiovascular system recovers over a period of minutes to hours, with many changes to its functions. Some of these changes may be necessary for long-term adaptation to exercise training, yet some can lead to cardiovascular instability during recovery. Furthermore, some of these changes may provide insight into when the cardiovascular system has recovered from prior training and is physiologically ready for additional training stress. This review focuses on the key changes in the cardiovascular system after exercise and how they differ between different types of exercise. Primary emphasis will be placed on the effect of aerobic and resistance exercise on blood pressure. Finally, we focus on the practical application of this information to strategies to maximize the benefits of cardiovascular recovery, or minimize the vulnerabilities of this state. We will explore appropriate field measures, and discuss to what extent these can guide an athlete's training.
Exercise and physical activity are increasingly becoming key tools in the treatment and prevention of several medical conditions including arthritis and diabetes; this notion has been termed "exercise as medicine". Exercise has favorable effects on reducing cardiovascular risk, inflammation, cachexia, and hypertension, in addition to increasing physical functioning, strength, and cardio-respiratory capacity. Chronic kidney disease, a condition that affects around 10% of the population, is often overlooked as a target for exercise-based therapy. Despite the different stages of kidney disease (pre-dialysis, dialysis, and transplant), exercise can benefit all patients with the condition. In this review, we summarise the important role exercise may have in the clinical management of kidney disease and how this form of 'medicine' should be best administered and 'prescribed'.
Blood flow restriction (BFR) is an effective clinical intervention used to increase strength in healthy individuals. However, its effects on pain and function in individuals with knee pain are unknown. Objective: To determine the effectiveness of adding BFR to resistance exercise for pain relief and improvement of function in patients with knee pain. Methods: Systematic review with meta-analysis of randomized clinical trials. Randomized clinical trials that compared resistance exercise with or without BFR to treat knee pain and function in individuals older than 18 years of age with knee pain were included. The standardized mean difference (SMD) estimate showed that resistance exercises with BFR was not more effective than resistance exercises for reducing pain (SMD: -0.37cm, 95% confidence interval (-0.93, 0.19) and improving knee function (SMD=-0.23 points, 95% CI=-0.71, 0.26) in patients with knee pain.
In Japan, there were an estimated 43 million patients with hypertension in 2010. The management of this condition is given the highest priority in disease control, and the importance of lifestyle changes for the prevention and treatment of hypertension has been recognized in Japan. In particular, emphasis has been placed on increasing the levels of activities of daily living and physical exercise (sports). In this literature review, we examined appropriate exercise prescriptions (e.g., type, intensity, duration per session, and frequency) for the prevention and treatment of hypertension as described in Japanese and foreign articles. This review recommends safe and effective aerobic exercise at moderate intensity (30-60 minutes, 3-4 times a week) to help prevent and treat hypertension.
Physical activity has proved to be an effective means of preventing several diseases and improving general health. Common sense advices call for late inception of intense, strength training-related activities, like weight lifting and plyometrics, which are usually postponed at the end of the growth age, even among sport practitioners. However, such advices seem to have a mainly anecdotal nature. Research suggests that strength training is safe for children and adolescents, as long as they follow basic safety guidelines. several studies provide consistent findings supporting the benefits of repeated, intense physical efforts in young subjects. Improved motor skills and body composition have been shown to improve in young people who start exercising early. It can be therefore concluded that strength training is a relatively safe and healthy practice for children and adolescents.
recently, there has been a renewed public interest in IFast. Nutrition is crucial for athletes, and there's a question about how IFast affects them. studies have found that fasting has no benefit for athletes. Looking at high-intensity, endurance, and resistance exercises.
Nearly every physically active person encounters periods in which the time available for exercise is limited (e.g., personal, family, or business conflicts). During such periods, the goal of physical training may be to simply maintain (rather than improve) physical performance. Special populations, such as athletes and military personnel, may need to maintain performance over a long time. In general populations, endurance performance can be maintained for up to 15 weeks with reduced training, either by cutting the number of sessions from 3 to 2, or by shortening the length of each session by 33-66%, as long as the intensity of exercise is maintained. Strength and muscle size (at least in younger populations) can be maintained for up to 32 weeks with as little as 1 session of strength training per week and 1 set per exercise, as long as exercise intensity is maintained; whereas, in older populations, maintaining muscle size may require up to 2 sessions per week and 2-3 sets per exercise, while maintaining exercise intensity. Our primary conclusion is that exercise intensity seems to be the key variable for maintaining physical performance over time, despite relatively large reductions in exercise frequency and volume.
Nutrient timing is a popular nutritional strategy that involves the consumption of combinations of nutrients--primarily protein and carbohydrate--in and around an exercise session. Some have claimed that this approach can produce dramatic improvements in body composition. It has even been postulated that the timing of nutritional consumption may be more important than the absolute daily intake of nutrients. The post-exercise period is widely considered the most critical part of nutrient timing. A specific time after exercise optimizes muscle growth. Theoretically, consuming the proper ratio of nutrients during this time not only initiates the rebuilding of damaged muscle tissue and restoration of energy reserves, but it does so in a supercompensated fashion that enhances both body composition and exercise performance. Several researchers have made reference to an anabolic “window of opportunity” whereby a limited time exists after training to optimize training-related muscular adaptations. However, the importance - and even the existence - of a post-exercise ‘window’ can vary according to a number of factors. Not only is nutrient timing research open to question in terms of applicability, but recent evidence has directly challenged the classical view of the relevance of post-exercise nutritional intake with respect to anabolism.
Lack of time is among the more commonly reported barriers for abstention from exercise programs. The aim of this review was to determine how strength training can be most effectively carried out in a time-efficient manner by critically evaluating research on acute training variables, advanced training techniques, and the need for warm-up and stretching. To train efficiently, we recommend doing exercises that work multiple joints at once, such as squats, pull-ups, and bench press. Exercises can be performed with machines and/or free weights based on training goals, availability, and personal preferences. Weekly training volume is more important than training frequency and we recommend performing a minimum of 4 weekly sets per muscle group using a 6–15 RM loading range (15–40 repetitions can be used if training is performed to volitional failure). Advanced training techniques, such as supersets, drop sets and rest-pause training roughly halves training time compared to traditional training, while maintaining training volume. However, these methods are probably better at inducing hypertrophy than muscular strength, and more research is needed on longitudinal training effects. Finally, we advise restricting the warm-up to exercise-specific warm-ups, and only prioritize stretching if the goal of training is to increase flexibility.
Training frequency is considered an important variable in the hypertrophic response to regimented resistance exercise. The purpose of this paper was to conduct a systematic review and meta-analysis of experimental studies designed to investigate the effects of weekly training frequency on hypertrophic adaptations. Results showed no significant difference between higher and lower frequency on a volume-equated basis. Higher frequency training showed a positive effect on muscle growth, but the difference was relatively small. In conclusion, there is strong evidence that resistance training frequency does not significantly or meaningfully impact muscle hypertrophy when volume is equated. Thus, for a given training volume, individuals can choose a weekly frequency per muscle groups based on personal preference.
A variety of specialized training techniques have been advocated as a means to heighten muscle growth. Forced repetitions, supersets, and heavy negatives can help with muscle growth. This article will explore the potential role of these techniques in promoting muscle hypertrophy and provide an insight into possible applications to resistance training programs.
The quest to increase lean body mass is widely pursued by those who lift weights. Research is lacking, however, as to the best approach for maximizing exercise-induced muscle growth. Bodybuilders generally train with moderate loads and fairly short rest intervals that induce high amounts of metabolic stress. Powerlifters, on the other hand, routinely train with high-intensity loads and lengthy rest periods between sets. It's unclear which training method is best for building muscle. Multiple factors, such as mechanical tension and muscle damage, contribute to muscle growth. The main goal of this paper is to review the research on muscle growth and determine the best exercise protocol.
