{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7d840b",
   "metadata": {},
   "source": [
    "## Convert **.ipynb** to **.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8817b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Iterative Simplification.ipynb to python\n",
      "[NbConvertApp] Writing 16445 bytes to Iterative Simplification.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to python \"Iterative Simplification.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef7eb2-7762-4fa9-99c9-71a4e67b1fda",
   "metadata": {},
   "source": [
    "## ChatBot setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36f4ef53-d970-4028-92bf-3c98291b45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai\n",
    "# %pip install evaluate\n",
    "# %pip install py-readability-metrics\n",
    "# %pip install sacrebleu\n",
    "# %pip install sacremoses\n",
    "# %pip install nltk\n",
    "# %pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f4bda8-35d6-4fe2-97c0-30ea5900c509",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resource'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_llm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\engine\\arg_utils.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CacheConfig, CompilationConfig, ConfigFormat,\n\u001b[0;32m     12\u001b[0m                          DecodingConfig, DeviceConfig, HfOverrides,\n\u001b[0;32m     13\u001b[0m                          KVTransferConfig, LoadConfig, LoadFormat, LoRAConfig,\n\u001b[0;32m     14\u001b[0m                          ModelConfig, ObservabilityConfig, ParallelConfig,\n\u001b[0;32m     15\u001b[0m                          PoolerConfig, PromptAdapterConfig, SchedulerConfig,\n\u001b[0;32m     16\u001b[0m                          SpeculativeConfig, TaskOption, TokenizerPoolConfig,\n\u001b[0;32m     17\u001b[0m                          VllmConfig)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\config.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minductor_pass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallableInductorPass, InductorPass\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (QUANTIZATION_METHODS,\n\u001b[0;32m     23\u001b[0m                                                      get_quantization_config)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform, interface\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\model_executor\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasevLLMParameter,\n\u001b[0;32m      2\u001b[0m                                            PackedvLLMParameter)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling_metadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (SamplingMetadata,\n\u001b[0;32m      4\u001b[0m                                                    SamplingMetadataCache)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\model_executor\\parameter.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tensor_model_parallel_rank\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasevLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackedvLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerTensorScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelWeightParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChannelQuantScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupQuantScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackedColumnParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRowvLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunication_op\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\communication_op.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_model_parallel_all_reduce\u001b[39m(input_: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\parallel_state.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backend, ProcessGroup\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer_agent\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkv_transfer\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatelessProcessGroup\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_transfer_agent.py:15\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_connector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     KVConnectorFactory)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_connector\\factory.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KVConnectorBase\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_connector\\base.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List, Tuple, Union\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\sequence.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmsgspec\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingletonInputs, SingletonInputsAdapter\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoRARequest\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalDataDict, MultiModalPlaceholderDict\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\inputs\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (DecoderOnlyInputs, EncoderDecoderInputs,\n\u001b[0;32m      2\u001b[0m                    ExplicitEncoderDecoderPrompt, ProcessorInputs, PromptType,\n\u001b[0;32m      3\u001b[0m                    SingletonInputs, SingletonInputsAdapter, SingletonPrompt,\n\u001b[0;32m      4\u001b[0m                    TextPrompt, TokenInputs, TokensPrompt,\n\u001b[0;32m      5\u001b[0m                    build_explicit_enc_dec_prompt, to_enc_dec_tuple_list,\n\u001b[0;32m      6\u001b[0m                    token_inputs, zip_enc_dec_prompts)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (DummyData, InputContext, InputProcessingContext,\n\u001b[0;32m      8\u001b[0m                        InputRegistry)\n\u001b[0;32m     10\u001b[0m INPUT_REGISTRY \u001b[38;5;241m=\u001b[39m InputRegistry()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mThe global :class:`~InputRegistry` which is used by :class:`~vllm.LLMEngine`\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mto dispatch data processing according to the target model.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    :ref:`input-processing-pipeline`\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\inputs\\registry.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_get_processor\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyTokenizer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ClassRegistry, get_allowed_kwarg_only_overrides,\n\u001b[0;32m     15\u001b[0m                         print_warning_once, resolve_mm_processor_kwargs)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProcessorInputs, SingletonInputs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\transformers_utils\\tokenizer.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralTokenizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_gguf_file\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_async\n\u001b[0;32m     18\u001b[0m logger \u001b[38;5;241m=\u001b[39m init_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     20\u001b[0m AnyTokenizer \u001b[38;5;241m=\u001b[39m Union[PreTrainedTokenizer, PreTrainedTokenizerFast,\n\u001b[0;32m     21\u001b[0m                      MistralTokenizer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\utils.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mresource\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'resource'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import vllm\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "# NLP packages\n",
    "import evaluate\n",
    "import readability\n",
    "import nltk\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ca716-d9d9-4e42-894c-dcf202a4e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3f645e-5193-4876-9c9a-e545efa5cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"\"\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "vllm_model = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a9e922-02f1-439a-a37f-9bbdd3a338cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIChatBot:\n",
    "    def __init__(self, model, api_key):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.chat_log = []\n",
    "\n",
    "    def add_system_prompt(self, prompt):\n",
    "        self.chat_log.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    def send_prompt(self, prompt):\n",
    "        self.chat_log.append({\"role\": \"user\", \"content\": prompt}) \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.chat_log\n",
    "        )\n",
    "        self.chat_log.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "    def get_last_response(self):\n",
    "        return self.chat_log[-1][\"content\"]\n",
    "    \n",
    "    def print_chat(self):\n",
    "        for message in self.chat_log:\n",
    "            role = message[\"role\"].upper()\n",
    "            print(f\"{role}: {message[\"content\"]}\", end=\"\\n\\n\")\n",
    "\n",
    "    def save_chat(self):\n",
    "        current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "        file_name = f\"./runs/chat-log_{current_datetime}.json\"\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump(self.chat_log, file)\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.chat_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VllmChatBot:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = vllm.LLM(model_name, max_model_len=8192) # Make this nicer !!!\n",
    "        self.chat_log = []\n",
    "\n",
    "    def add_system_prompt(self, prompt):\n",
    "        self.chat_log.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    def send_prompt(self, prompt):\n",
    "        self.chat_log.append({\"role\": \"user\", \"content\": prompt}) \n",
    "        response = self.model.chat(\n",
    "            messages=self.chat_log,\n",
    "            sampling_params=SamplingParams(max_tokens=8192), # Make this nicer !!!\n",
    "        )\n",
    "        self.chat_log.append({\"role\": \"assistant\", \"content\": response[0].outputs[0].text})\n",
    "\n",
    "    def get_last_response(self):\n",
    "        return self.chat_log[-1][\"content\"]\n",
    "    \n",
    "    def print_chat(self):\n",
    "        for message in self.chat_log:\n",
    "            role = message[\"role\"].upper()\n",
    "            print(f\"{role}: {message[\"content\"]}\", end=\"\\n\\n\")\n",
    "\n",
    "    def save_chat(self):\n",
    "        current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "        file_name = f\"./runs/chat-log_{current_datetime}.json\"\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json.dump(self.chat_log, file)\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.chat_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafa9748-50d9-452a-89b2-22c1525e6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chat_bot = ChatBot(\n",
    "# #     model=\"gpt-4o-mini\",\n",
    "# #     api_key=api_key,\n",
    "# # )\n",
    "\n",
    "# temp_bot = ChatBot(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     api_key=api_key,\n",
    "# )\n",
    "# temp_bot.add_system_prompt(\"You solve math equations\")\n",
    "# temp_bot.send_prompt(\"What is 2+3?\")\n",
    "# temp_bot.print_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9bcfd3b-ef55-4c86-9864-b173decaf431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algorithm_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a56e1-1f3d-4f03-8cce-4973ff195e8f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0d8b2-3cb0-4178-8792-91983d861e8a",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201616df-86dd-4a43-b509-22e13eb0cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite to class?\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    dataset[data_type] = {}\n",
    "    for passage_length in [\"abs\", \"snt\"]:\n",
    "        dataset[data_type][passage_length] = {}\n",
    "\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    for passage_length in [\"abs\", \"snt\"]:\n",
    "        for passage_type in [\"source\", \"reference\"]:\n",
    "            if data_type == \"test\" and passage_type == \"reference\":\n",
    "                continue\n",
    "                \n",
    "            folder = f\"./dataset/{data_type}/\"\n",
    "            file_name = f\"simpletext_task3_2024_{data_type}_{passage_length}_{passage_type}.json\"\n",
    "            path = folder + file_name\n",
    "\n",
    "            print(\"Reading: \" + path)\n",
    "            with open(path, \"r\", encoding=\"utf8\") as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            dataset[data_type][passage_length][passage_type] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa29ae-0841-4535-a957-dfbdfbe4a22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3051733c-71a3-45ac-805d-a5470eaa95d7",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8e83a-e35c-40ee-af2a-81ce9a9f02cc",
   "metadata": {},
   "source": [
    "Mssing metrics:\n",
    "- count\n",
    "- Compression ratio\n",
    "- Sentence splits\n",
    "- Levenshtein similarity\n",
    "- Exact copies\n",
    "- Additions proportion\n",
    "- Deletions proportion\n",
    "- Lexical complexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7be020c-a189-4088-b0ec-48c5088306df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "sari = evaluate.load(\"sari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203f2e11-4b20-46d0-a439-8cc6700533bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(sources, predictions, references):\n",
    "    # sources – original passages, predictions – simplified passages, references – target simplifications\n",
    "    sari_nested_references = [[reference] for reference in references]\n",
    "    \n",
    "    results = {}\n",
    "    results[\"SARI\"] = sari.compute(sources=sources, predictions=predictions, references=sari_nested_references)[\"sari\"]\n",
    "    results[\"BLEU\"] = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
    "    # results[\"FKGL\"] = readability.Readability(simplified_passage).flesch_kincaid() # Should I use score or grade.level?\n",
    "    results[\"FKGL\"] = np.mean([textstat.flesch_kincaid_grade(passage) for passage in predictions])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6599f2f0-e60d-42bb-916b-3d857401aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "# >>> references = [\n",
    "# ...     [\"hello there general kenobi\", \"hello there !\"],\n",
    "# ...     [\"foo bar foobar\"]\n",
    "# ... ]\n",
    "# >>> bleu = evaluate.load(\"bleu\")\n",
    "# >>> results = bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba3437f-5d65-415c-a858-bd3eccc290cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources=[\"About 95 species are currently accepted.\"]\n",
    "# predictions=[\"About 95 you now get in.\"]\n",
    "# references=[[\"About 95 species are currently known.\",\"About 95 species are now accepted.\",\"95 species are now accepted.\"]]\n",
    "# sari_score = sari.compute(sources=sources, predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1d62e-530d-4fdb-abbd-50beac9a995b",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac6b94e-b21e-4fbe-8c4f-a5a6d1bc6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_parameters = {\n",
    "    \"DC\": \"University student\",\n",
    "    \"ILT\": \"Medium\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f38724db-b2ba-43d5-bec6-db97db1caa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a reader assistant. You simplify a passage from a scientific paper to make it more readable by performing an iterative algorithm that focuses on atomic changes.\n",
    "\n",
    "You are also given two parameters: DC (Desired Complexity) – a desired complexity of the simplified text, and ILT (Information Loss Tolerance) – a threshold for information loss compared to original passage that specifies whether an atomic change should be accepted or not.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Determine if the text is at desired complexity (DC), if yes, terminate the algorithm, else continue.\n",
    "2. Identify a section of the text whose complexity is above DC.\n",
    "3. Propose a simplification of the identified section.\n",
    "4. Identify information loss and the severity of it – if the severity for any of the information loss questions is higher than information loss tolerance (ILT), reject changes, else update the current state of the passage\n",
    "5. Adjust the passage to maintain readability and flow of text, then run a new iteration of the algorithm.\n",
    "\n",
    "When the algorithm terminates, you print the final simplified passage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa405b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_iterative_system_prompt = \"\"\"\n",
    "You are a reader assistant. You simplify a passage from a scientific paper to make it more readable.\n",
    "\n",
    "You are also given two parameters: DC (Desired Complexity) – a desired complexity of the simplified text, and ILT (Information Loss Tolerance) – a threshold for information loss compared to original passage.\n",
    "\n",
    "Only print the simplified passage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deaeee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiir_mistral_system_prompt = \"\"\"\n",
    "You are a skilled editor, known for your ability to simplify complex text while preserving it. You explain the technical terms, defining what they are (e.g., terms like Blockchain, Cryptojacking, all abbreviations), without removing sentences or summarizing them.\n",
    "\"\"\"\n",
    "\n",
    "aiir_llama_run_1_system_prompt = \"\"\"\n",
    "Simplify this text for English speaking science students in college. Maximize the use of simple words and short sentences, but include keywords from the original text. Optimize the output ROUGE, SARI, and BLEU scores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28a2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ad8c321-6bb2-4ce4-be31-07f34e464ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_passage_iteratively(chat_bot, system_prompt, parameters, passage, max_iter=20):\n",
    "    chat_bot.clear_chat()\n",
    "    \n",
    "    chat_bot.add_system_prompt(system_prompt)\n",
    "    chat_bot.add_system_prompt(f\"The passage:\\n{passage}\")\n",
    "\n",
    "    if parameters is not None and parameters != {}:\n",
    "        chat_bot.add_system_prompt(\"\\n\".join(f\"{parameter}: {value}\" for parameter, value in parameters.items()))\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        chat_bot.send_prompt(\"Identify which parts of the text are the most complex, then the complexity level of the passage\")\n",
    "        chat_bot.send_prompt(f'Is determined complexity higher than DC ({algorithm_parameters[\"DC\"]})? Answer \"Yes\" or \"No\"')\n",
    "        if \"NO\" in chat_bot.get_last_response().upper():\n",
    "            break\n",
    "        chat_bot.send_prompt(f'Identify a single complicated section of the passage. Remember to respect the ILT ({algorithm_parameters[\"ILT\"]}) contraint. Only provide the identified section')\n",
    "        chat_bot.send_prompt(\"Simplify this section. Only provide the proposed simplification\")\n",
    "        chat_bot.send_prompt(\"Reincorporate the simplified section into the passage\")\n",
    "        chat_bot.send_prompt(\"Identify information loss and its severity in the updated passage compared to the original. Comparison must be between the originally provided (the very first) passage and the current simplified version\")\n",
    "        chat_bot.send_prompt(f'What is the highest severity level identified in your last answer? Is it higher than ILT ({algorithm_parameters[\"ILT\"]})? Provide the highest severity level, followed by an answer to the ILT question as \"Yes\" or \"No\"')\n",
    "        if \"YES\" in chat_bot.get_last_response().upper():\n",
    "            chat_bot.send_prompt(\"Revert the last proposed change. In further iterations you may still attempt to simplify this section in other ways\")\n",
    "        # else:\n",
    "        #     chat_bot.send_prompt(\"If needed, adjust the passage to maintain readabily and flow of text\")\n",
    "\n",
    "    chat_bot.send_prompt(\"Print the final version of the simplified passage, include only the text of the passage with no comments or additional punctuation, and do not provide the original passage\")\n",
    "    # chat_bot.print_chat()\n",
    "    chat_bot.save_chat()\n",
    "\n",
    "    return chat_bot.get_last_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "274a6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sources_and_references(passage_type_abbreviation, n):    \n",
    "    sources = [entry[f\"source_{passage_type_abbreviation}\"] for entry in dataset[\"train\"][passage_type_abbreviation][\"source\"][:n] ]\n",
    "    references = [entry[f\"simplified_{passage_type_abbreviation}\"] for entry in dataset[\"train\"][passage_type_abbreviation][\"reference\"][:n] ]\n",
    "\n",
    "    return (sources, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "914823ed-ab95-4456-9788-fd8b99e8385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_passages(algorithm_fn, system_prompt, parameters, passage_type, max_iter, n=None):\n",
    "    if passage_type == \"abstract\":\n",
    "        sources, references = get_sources_and_references(\"abs\", n)\n",
    "    elif passage_type == \"sentence\":\n",
    "        sources, references = get_sources_and_references(\"snt\", n)\n",
    "    else:\n",
    "        raise ValueError('Passage type should be \"abstract\" or \"sentence\"')\n",
    "\n",
    "    predictions = []\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        # chat_bot = OpenAIChatBot(\n",
    "        #     model=openai_model,\n",
    "        #     api_key=api_key,\n",
    "        # )\n",
    "        chat_bot = VllmChatBot(\n",
    "            model=vllm_model,\n",
    "        )\n",
    "\n",
    "        prediction = algorithm_fn(chat_bot, system_prompt, parameters, sources[i], max_iter)\n",
    "        # metrics = compute_metrics([sources[i]], [prediction], [references[i]])\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        results.append({\n",
    "            \"source\": sources[i],\n",
    "            \"prediction\": prediction,\n",
    "            \"reference\": references[i],\n",
    "            # \"metrics\": metrics,\n",
    "        })\n",
    "\n",
    "    overall_metrics = compute_metrics(sources, predictions, references)\n",
    "    return (overall_metrics, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9169a95d-ce53-41d3-802e-5c74f54733ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_to_simplify = 1\n",
    "passage_type_to_simplify = \"sentence\"\n",
    "\n",
    "algorithm_results[\"iterative\"] = simplify_passages(simplify_passage_iteratively, system_prompt, algorithm_parameters, passage_type_to_simplify, 20, passages_to_simplify)\n",
    "# algorithm_results[\"non_iterative\"] = simplify_passages(simplify_passage_iteratively, non_iterative_system_prompt, algorithm_parameters, passage_type_to_simplify, 0, passages_to_simplify)\n",
    "# algorithm_results[\"aiir_mistral_prompt\"] = simplify_passages(simplify_passage_iteratively, aiir_mistral_system_prompt, {}, passage_type_to_simplify, 0, passages_to_simplify)\n",
    "# algorithm_results[\"aiir_llama_run_1_prompt\"] = simplify_passages(simplify_passage_iteratively, aiir_llama_run_1_system_prompt, {}, passage_type_to_simplify, 0, passages_to_simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3940fa-de2d-4449-96e3-99bf32715979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"METRICS:\")\n",
    "for algorithm, results in algorithm_results.items():\n",
    "    print(f\"{algorithm.upper()}: {results[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89804a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"SIMPLIFICATION EXAMPLES:\")\n",
    "# print(200*\"–\")\n",
    "# for i in range(5):\n",
    "#     print(f\"{i}:\")\n",
    "#     print(f\"SOURCE: {algorithm_results[\"iterative\"][1][i][\"source\"]}\")\n",
    "#     print(f\"REFERENCE: {algorithm_results[\"iterative\"][1][i][\"reference\"]}\")\n",
    "#     for algorithm, results in algorithm_results.items():\n",
    "#         print(f\"{algorithm.upper()}:\")\n",
    "#         print(results[1][i][\"prediction\"])\n",
    "#     print(200*\"–\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0290847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "# with open(f\"./evaluations/{passage_type_to_simplify}s_university_medium_max20_{current_datetime}\", \"w\") as file:\n",
    "#     json.dump(algorithm_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10f137-bc09-4d8c-9df9-ed882914ff4d",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3c3a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('evaluations/abstracts_university_medium_max20_2024-12-12_21-37-41.106794') as f:\n",
    "#     old_simplifications = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_fkgl_scores = np.array([(textstat.flesch_kincaid_grade(passage_set[\"source\"]), textstat.flesch_kincaid_grade(passage_set[\"prediction\"])) for passage_set in old_simplifications[\"iterative\"][1]])\n",
    "# # old_fkgl_scores = np.array([(readability.Readability(passage_set[\"source\"]).flesch_kincaid().score, readability.Readability(passage_set[\"prediction\"]).flesch_kincaid().score) for passage_set in old_simplifications[\"iterative\"][1]])\n",
    "# print(old_fkgl_scores)\n",
    "\n",
    "# # for passage_set in old_simplifications[\"iterative\"][1][:20]:\n",
    "# #     print(textstat.flesch_kincaid_grade(passage_set[\"source\"]), end=\"\")\n",
    "# #     print(\" –> \", end=\"\")\n",
    "# #     print(textstat.flesch_kincaid_grade(passage_set[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195077fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = old_fkgl_scores[:, 0] - old_fkgl_scores[:, 1]\n",
    "# dir = diff >= 0\n",
    "\n",
    "# plt.scatter(old_fkgl_scores[:, 0], old_fkgl_scores[:, 1], abs(diff), np.where(dir, \"b\", \"r\"))\n",
    "# plt.title(\"Abstract complexity change\")\n",
    "# plt.xlabel(\"Original passage FKGL\")\n",
    "# plt.ylabel(\"Simplified passage FKGL\")\n",
    "# plt.xscale(\"log\")\n",
    "# plt.yscale(\"log\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b992c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers = []\n",
    "# for passage_set, scores in zip(old_simplifications[\"iterative\"][1], old_fkgl_scores):\n",
    "#     if scores[0] - scores[1] < -20:\n",
    "#         outliers.append((passage_set, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91a739",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_exp(x, k, a):\n",
    "#     return a * (np.exp(-k * (x-0.8)) - 1)\n",
    "\n",
    "# # Define the range of x values\n",
    "# x = np.linspace(0, 1, 500)\n",
    "\n",
    "# # Define the functions\n",
    "# exp_1 = my_exp(x, k=1, a=0.816)\n",
    "# exp_2 = my_exp(x, k=2, a=0.253)\n",
    "# exp_3 = my_exp(x, k=10, a=0.00033559)\n",
    "# linear = 1 - 1.25 * x\n",
    "\n",
    "# nullifier = np.where(x > 0.8, 0, 1)\n",
    "\n",
    "# # Horizontal and vertical constants\n",
    "# horizontal_constant = 0.25\n",
    "# vertical_constant = 0.5\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Functions\n",
    "# plt.plot(x, linear * nullifier, color=\"red\")\n",
    "# plt.plot(x, exp_1 * nullifier, color=\"red\")\n",
    "# plt.plot(x, exp_2 * nullifier, color=\"red\")\n",
    "# plt.plot(x, exp_3 * nullifier, color=\"red\")\n",
    "\n",
    "# # Parameters\n",
    "# plt.axhline(y=horizontal_constant, color=\"blue\", linestyle=\"--\")\n",
    "# plt.axvline(x=vertical_constant, color=\"blue\", linestyle=\"--\")\n",
    "# plt.fill_between(x, horizontal_constant, 1, color=\"blue\", alpha=0.4)\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title(\"Complexity and Information Loss Trade-Off with Parameters\")\n",
    "# plt.xlabel(\"Complexity\")\n",
    "# plt.ylabel(\"Information loss\")\n",
    "# plt.axhline(0, color=\"black\", linewidth=0.5, linestyle=\"-\")  # x-axis\n",
    "# plt.axvline(0, color=\"black\", linewidth=0.5, linestyle=\"-\")  # y-axis\n",
    "# plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "# # plt.savefig(\"graphs/trade_off_parameters.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb418c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbeb84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
